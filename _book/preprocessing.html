<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Preprocessing | Data Analysis in Software Engineering using R</title>
  <meta name="description" content="DASE Data Analysis in Software Engineering" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Preprocessing | Data Analysis in Software Engineering using R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="DASE Data Analysis in Software Engineering" />
  <meta name="github-repo" content="danrodgar/DASE" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Preprocessing | Data Analysis in Software Engineering using R" />
  
  <meta name="twitter:description" content="DASE Data Analysis in Software Engineering" />
  

<meta name="author" content="Daniel Rodriguez and Javier Dolado" />


<meta name="date" content="2021-10-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classical-hypothesis-testing.html"/>
<link rel="next" href="supervised-classification.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Analysis in Software Engineering with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="part"><span><b>I Introduction to the R Language</b></span></li>
<li class="chapter" data-level="1" data-path="r-intro.html"><a href="r-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-intro.html"><a href="r-intro.html#installation"><i class="fa fa-check"></i><b>1.1</b> Installation</a></li>
<li class="chapter" data-level="1.2" data-path="r-intro.html"><a href="r-intro.html#r-and-rstudio"><i class="fa fa-check"></i><b>1.2</b> R and RStudio</a></li>
<li class="chapter" data-level="1.3" data-path="r-intro.html"><a href="r-intro.html#basic-data-types"><i class="fa fa-check"></i><b>1.3</b> Basic Data Types</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="r-intro.html"><a href="r-intro.html#mising-values"><i class="fa fa-check"></i><b>1.3.1</b> Mising values</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="r-intro.html"><a href="r-intro.html#vectors"><i class="fa fa-check"></i><b>1.4</b> Vectors</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="r-intro.html"><a href="r-intro.html#coercion-for-vectors"><i class="fa fa-check"></i><b>1.4.1</b> Coercion for vectors</a></li>
<li class="chapter" data-level="1.4.2" data-path="r-intro.html"><a href="r-intro.html#vector-arithmetic"><i class="fa fa-check"></i><b>1.4.2</b> Vector arithmetic</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="r-intro.html"><a href="r-intro.html#arrays-and-matrices"><i class="fa fa-check"></i><b>1.5</b> Arrays and Matrices</a></li>
<li class="chapter" data-level="1.6" data-path="r-intro.html"><a href="r-intro.html#factors"><i class="fa fa-check"></i><b>1.6</b> Factors</a></li>
<li class="chapter" data-level="1.7" data-path="r-intro.html"><a href="r-intro.html#lists"><i class="fa fa-check"></i><b>1.7</b> Lists</a></li>
<li class="chapter" data-level="1.8" data-path="r-intro.html"><a href="r-intro.html#data-frames"><i class="fa fa-check"></i><b>1.8</b> Data frames</a></li>
<li class="chapter" data-level="1.9" data-path="r-intro.html"><a href="r-intro.html#r---functions-apply-lapply-sapply-tapply-mapply-vapply"><i class="fa fa-check"></i><b>1.9</b> R - Functions <code>apply()</code>, <code>lapply()</code>, <code>sapply()</code>, <code>tapply()</code>, <code>mapply()</code>, <code>vapply()</code></a></li>
<li class="chapter" data-level="1.10" data-path="r-intro.html"><a href="r-intro.html#environments"><i class="fa fa-check"></i><b>1.10</b> Environments</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="r-intro.html"><a href="r-intro.html#global-variables-local-variables-and-programming-scope"><i class="fa fa-check"></i><b>1.10.1</b> Global variables, local variables and programming scope</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="r-intro.html"><a href="r-intro.html#reading-data"><i class="fa fa-check"></i><b>1.11</b> Reading Data</a></li>
<li class="chapter" data-level="1.12" data-path="r-intro.html"><a href="r-intro.html#plots"><i class="fa fa-check"></i><b>1.12</b> Plots</a></li>
<li class="chapter" data-level="1.13" data-path="r-intro.html"><a href="r-intro.html#flow-of-control"><i class="fa fa-check"></i><b>1.13</b> Flow of Control</a></li>
<li class="chapter" data-level="1.14" data-path="r-intro.html"><a href="r-intro.html#rattle"><i class="fa fa-check"></i><b>1.14</b> Rattle</a></li>
<li class="chapter" data-level="1.15" data-path="r-intro.html"><a href="r-intro.html#datasets"><i class="fa fa-check"></i><b>1.15</b> Datasets</a></li>
</ul></li>
<li class="part"><span><b>II Introduction to Data Mining</b></span></li>
<li class="chapter" data-level="2" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><i class="fa fa-check"></i><b>2</b> What is Data Mining / Knowledge Discovery in Databases (KDD)</a>
<ul>
<li class="chapter" data-level="2.1" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#the-aim-of-data-analysis-and-statistical-learning"><i class="fa fa-check"></i><b>2.1</b> The Aim of Data Analysis and Statistical Learning</a></li>
<li class="chapter" data-level="2.2" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#data-science"><i class="fa fa-check"></i><b>2.2</b> Data Science</a></li>
<li class="chapter" data-level="2.3" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#some-references"><i class="fa fa-check"></i><b>2.3</b> Some References</a></li>
<li class="chapter" data-level="2.4" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#data-mining-and-data-science-with-r"><i class="fa fa-check"></i><b>2.4</b> Data Mining and Data Science with R</a></li>
<li class="chapter" data-level="2.5" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#data-mining-with-weka"><i class="fa fa-check"></i><b>2.5</b> Data Mining with Weka</a></li>
</ul></li>
<li class="part"><span><b>III Data Sources and Metrics and Standards in Software Engineering Defect Prediction</b></span></li>
<li class="chapter" data-level="3" data-path="data-sources-in-software-engineering.html"><a href="data-sources-in-software-engineering.html"><i class="fa fa-check"></i><b>3</b> Data Sources in Software Engineering</a></li>
<li class="chapter" data-level="4" data-path="repositories.html"><a href="repositories.html"><i class="fa fa-check"></i><b>4</b> Repositories</a></li>
<li class="chapter" data-level="5" data-path="open-toolsdashboards-to-extract-data.html"><a href="open-toolsdashboards-to-extract-data.html"><i class="fa fa-check"></i><b>5</b> Open Tools/Dashboards to extract data</a>
<ul>
<li class="chapter" data-level="5.1" data-path="open-toolsdashboards-to-extract-data.html"><a href="open-toolsdashboards-to-extract-data.html#issues"><i class="fa fa-check"></i><b>5.1</b> Issues</a></li>
<li class="chapter" data-level="5.2" data-path="open-toolsdashboards-to-extract-data.html"><a href="open-toolsdashboards-to-extract-data.html#effort-estimation-data-in-software-engineering"><i class="fa fa-check"></i><b>5.2</b> Effort Estimation Data in Software Engineering</a></li>
</ul></li>
<li class="part"><span><b>IV Exploratory and Descriptive Data analysis</b></span></li>
<li class="chapter" data-level="6" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html"><i class="fa fa-check"></i><b>6</b> Exploratory Data Analysis</a>
<ul>
<li class="chapter" data-level="6.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#descriptive-statistics"><i class="fa fa-check"></i><b>6.1</b> Descriptive statistics</a></li>
<li class="chapter" data-level="6.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#basic-plots"><i class="fa fa-check"></i><b>6.2</b> Basic Plots</a></li>
<li class="chapter" data-level="6.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#normality"><i class="fa fa-check"></i><b>6.3</b> Normality</a></li>
<li class="chapter" data-level="6.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#using-a-running-example-to-visualise-the-different-plots"><i class="fa fa-check"></i><b>6.4</b> Using a running Example to visualise the different plots</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#example-with-the-china-dataset"><i class="fa fa-check"></i><b>6.4.1</b> Example with the China dataset</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#correlation"><i class="fa fa-check"></i><b>6.5</b> Correlation</a></li>
<li class="chapter" data-level="6.6" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#confidence-intervals.-bootstrap"><i class="fa fa-check"></i><b>6.6</b> Confidence Intervals. Bootstrap</a></li>
<li class="chapter" data-level="6.7" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#nonparametric-bootstrap"><i class="fa fa-check"></i><b>6.7</b> Nonparametric Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classical-hypothesis-testing.html"><a href="classical-hypothesis-testing.html"><i class="fa fa-check"></i><b>7</b> Classical Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="7.1" data-path="classical-hypothesis-testing.html"><a href="classical-hypothesis-testing.html#p-values"><i class="fa fa-check"></i><b>7.1</b> p-values</a></li>
</ul></li>
<li class="part"><span><b>V Preprocessing</b></span></li>
<li class="chapter" data-level="8" data-path="preprocessing.html"><a href="preprocessing.html"><i class="fa fa-check"></i><b>8</b> Preprocessing</a>
<ul>
<li class="chapter" data-level="8.1" data-path="preprocessing.html"><a href="preprocessing.html#data"><i class="fa fa-check"></i><b>8.1</b> Data</a></li>
<li class="chapter" data-level="8.2" data-path="preprocessing.html"><a href="preprocessing.html#missing-values"><i class="fa fa-check"></i><b>8.2</b> Missing values</a></li>
<li class="chapter" data-level="8.3" data-path="preprocessing.html"><a href="preprocessing.html#noise"><i class="fa fa-check"></i><b>8.3</b> Noise</a></li>
<li class="chapter" data-level="8.4" data-path="preprocessing.html"><a href="preprocessing.html#outliers"><i class="fa fa-check"></i><b>8.4</b> Outliers</a></li>
<li class="chapter" data-level="8.5" data-path="preprocessing.html"><a href="preprocessing.html#feature-selection"><i class="fa fa-check"></i><b>8.5</b> Feature selection</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="preprocessing.html"><a href="preprocessing.html#fselector-package-in-r"><i class="fa fa-check"></i><b>8.5.1</b> FSelector package in R</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="preprocessing.html"><a href="preprocessing.html#instance-selection"><i class="fa fa-check"></i><b>8.6</b> Instance selection</a></li>
<li class="chapter" data-level="8.7" data-path="preprocessing.html"><a href="preprocessing.html#discretization"><i class="fa fa-check"></i><b>8.7</b> Discretization</a></li>
<li class="chapter" data-level="8.8" data-path="preprocessing.html"><a href="preprocessing.html#correlation-coefficient-and-covariance-for-numeric-data"><i class="fa fa-check"></i><b>8.8</b> Correlation Coefficient and Covariance for Numeric Data</a></li>
<li class="chapter" data-level="8.9" data-path="preprocessing.html"><a href="preprocessing.html#normalization-1"><i class="fa fa-check"></i><b>8.9</b> Normalization</a>
<ul>
<li class="chapter" data-level="8.9.1" data-path="preprocessing.html"><a href="preprocessing.html#min-max-normalization"><i class="fa fa-check"></i><b>8.9.1</b> Min-Max Normalization</a></li>
<li class="chapter" data-level="8.9.2" data-path="preprocessing.html"><a href="preprocessing.html#z-score-normalization"><i class="fa fa-check"></i><b>8.9.2</b> Z-score normalization</a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="preprocessing.html"><a href="preprocessing.html#transformations"><i class="fa fa-check"></i><b>8.10</b> Transformations</a>
<ul>
<li class="chapter" data-level="8.10.1" data-path="preprocessing.html"><a href="preprocessing.html#linear-transformations-and-quadratic-trans-formations"><i class="fa fa-check"></i><b>8.10.1</b> Linear Transformations and Quadratic Trans formations</a></li>
<li class="chapter" data-level="8.10.2" data-path="preprocessing.html"><a href="preprocessing.html#box-cox-transformation"><i class="fa fa-check"></i><b>8.10.2</b> Box-cox transformation</a></li>
<li class="chapter" data-level="8.10.3" data-path="preprocessing.html"><a href="preprocessing.html#nominal-to-binary-tranformations"><i class="fa fa-check"></i><b>8.10.3</b> Nominal to Binary tranformations</a></li>
</ul></li>
<li class="chapter" data-level="8.11" data-path="preprocessing.html"><a href="preprocessing.html#preprocessing-in-r"><i class="fa fa-check"></i><b>8.11</b> Preprocessing in R</a>
<ul>
<li class="chapter" data-level="8.11.1" data-path="preprocessing.html"><a href="preprocessing.html#the-dplyr-package"><i class="fa fa-check"></i><b>8.11.1</b> The <code>dplyr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="8.12" data-path="preprocessing.html"><a href="preprocessing.html#other-libraries-and-tricks"><i class="fa fa-check"></i><b>8.12</b> Other libraries and tricks</a></li>
</ul></li>
<li class="part"><span><b>VI Supervised Models</b></span></li>
<li class="chapter" data-level="9" data-path="supervised-classification.html"><a href="supervised-classification.html"><i class="fa fa-check"></i><b>9</b> Supervised Classification</a>
<ul>
<li class="chapter" data-level="9.1" data-path="supervised-classification.html"><a href="supervised-classification.html#classification-trees"><i class="fa fa-check"></i><b>9.1</b> Classification Trees</a></li>
<li class="chapter" data-level="9.2" data-path="supervised-classification.html"><a href="supervised-classification.html#rules"><i class="fa fa-check"></i><b>9.2</b> Rules</a></li>
<li class="chapter" data-level="9.3" data-path="supervised-classification.html"><a href="supervised-classification.html#distanced-based-methods"><i class="fa fa-check"></i><b>9.3</b> Distanced-based Methods</a></li>
<li class="chapter" data-level="9.4" data-path="supervised-classification.html"><a href="supervised-classification.html#neural-networks"><i class="fa fa-check"></i><b>9.4</b> Neural Networks</a></li>
<li class="chapter" data-level="9.5" data-path="supervised-classification.html"><a href="supervised-classification.html#support-vector-machine"><i class="fa fa-check"></i><b>9.5</b> Support Vector Machine</a></li>
<li class="chapter" data-level="9.6" data-path="supervised-classification.html"><a href="supervised-classification.html#probabilistic-methods"><i class="fa fa-check"></i><b>9.6</b> Probabilistic Methods</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="supervised-classification.html"><a href="supervised-classification.html#naive-bayes"><i class="fa fa-check"></i><b>9.6.1</b> Naive Bayes</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="supervised-classification.html"><a href="supervised-classification.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.7</b> Linear Discriminant Analysis (LDA)</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="supervised-classification.html"><a href="supervised-classification.html#predicting-the-number-of-defects-numerical-class"><i class="fa fa-check"></i><b>9.7.1</b> Predicting the number of defects (numerical class)</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="supervised-classification.html"><a href="supervised-classification.html#binary-logistic-regression-blr"><i class="fa fa-check"></i><b>9.8</b> Binary Logistic Regression (BLR)</a></li>
<li class="chapter" data-level="9.9" data-path="supervised-classification.html"><a href="supervised-classification.html#the-caret-package"><i class="fa fa-check"></i><b>9.9</b> The caret package</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>10</b> Regression</a>
<ul>
<li class="chapter" data-level="10.1" data-path="regression.html"><a href="regression.html#linear-regression-modeling"><i class="fa fa-check"></i><b>10.1</b> Linear Regression modeling</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="regression.html"><a href="regression.html#regression-galton-data"><i class="fa fa-check"></i><b>10.1.1</b> Regression: Galton Data</a></li>
<li class="chapter" data-level="10.1.2" data-path="regression.html"><a href="regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>10.1.2</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="10.1.3" data-path="regression.html"><a href="regression.html#least-squares"><i class="fa fa-check"></i><b>10.1.3</b> Least Squares</a></li>
<li class="chapter" data-level="10.1.4" data-path="regression.html"><a href="regression.html#linear-regression-in-r"><i class="fa fa-check"></i><b>10.1.4</b> Linear regression in R</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="regression.html"><a href="regression.html#linear-regression-diagnostics"><i class="fa fa-check"></i><b>10.2</b> Linear Regression Diagnostics</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="regression.html"><a href="regression.html#simulation-example"><i class="fa fa-check"></i><b>10.2.1</b> Simulation example</a></li>
<li class="chapter" data-level="10.2.2" data-path="regression.html"><a href="regression.html#diagnostics-fro-assessing-the-regression-line"><i class="fa fa-check"></i><b>10.2.2</b> Diagnostics fro assessing the regression line</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="regression.html"><a href="regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>10.3</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="regression.html"><a href="regression.html#partial-least-squares"><i class="fa fa-check"></i><b>10.3.1</b> Partial Least Squares</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="regression.html"><a href="regression.html#linear-regression-in-software-effort-estimation"><i class="fa fa-check"></i><b>10.4</b> Linear regression in Software Effort estimation</a></li>
<li class="chapter" data-level="10.5" data-path="regression.html"><a href="regression.html#references"><i class="fa fa-check"></i><b>10.5</b> References</a></li>
</ul></li>
<li class="part"><span><b>VII Unsupervised Models</b></span></li>
<li class="chapter" data-level="11" data-path="unsupervised-or-descriptive-modeling.html"><a href="unsupervised-or-descriptive-modeling.html"><i class="fa fa-check"></i><b>11</b> Unsupervised or Descriptive modeling</a>
<ul>
<li class="chapter" data-level="11.1" data-path="unsupervised-or-descriptive-modeling.html"><a href="unsupervised-or-descriptive-modeling.html#clustering"><i class="fa fa-check"></i><b>11.1</b> Clustering</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="unsupervised-or-descriptive-modeling.html"><a href="unsupervised-or-descriptive-modeling.html#k-means"><i class="fa fa-check"></i><b>11.1.1</b> k-Means</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="unsupervised-or-descriptive-modeling.html"><a href="unsupervised-or-descriptive-modeling.html#association-rules"><i class="fa fa-check"></i><b>11.2</b> Association rules</a></li>
</ul></li>
<li class="part"><span><b>VIII Evaluation</b></span></li>
<li class="chapter" data-level="12" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html"><i class="fa fa-check"></i><b>12</b> Evaluation of Models</a>
<ul>
<li class="chapter" data-level="12.1" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#building-and-validating-a-model"><i class="fa fa-check"></i><b>12.1</b> Building and Validating a Model</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#holdout-approach"><i class="fa fa-check"></i><b>12.1.1</b> Holdout approach</a></li>
<li class="chapter" data-level="12.1.2" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#cross-validation-cv"><i class="fa fa-check"></i><b>12.1.2</b> Cross Validation (CV)</a></li>
<li class="chapter" data-level="12.1.3" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#leave-one-out-cross-validation-loo-cv"><i class="fa fa-check"></i><b>12.1.3</b> Leave-One-Out Cross-Validation (LOO-CV)</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#evaluation-of-classification-models"><i class="fa fa-check"></i><b>12.2</b> Evaluation of Classification Models</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#prediction-in-probabilistic-classifiers"><i class="fa fa-check"></i><b>12.2.1</b> Prediction in probabilistic classifiers</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#other-metrics-used-in-software-engineering-with-classification"><i class="fa fa-check"></i><b>12.3</b> Other Metrics used in Software Engineering with Classification</a></li>
<li class="chapter" data-level="12.4" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#graphical-evaluation"><i class="fa fa-check"></i><b>12.4</b> Graphical Evaluation</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#receiver-operating-characteristic-roc"><i class="fa fa-check"></i><b>12.4.1</b> Receiver Operating Characteristic (ROC)</a></li>
<li class="chapter" data-level="12.4.2" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#precision-recall-curve-prc"><i class="fa fa-check"></i><b>12.4.2</b> Precision-Recall Curve (PRC)</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#numeric-prediction-evaluation"><i class="fa fa-check"></i><b>12.5</b> Numeric Prediction Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="evaluationSE.html"><a href="evaluationSE.html"><i class="fa fa-check"></i><b>13</b> Measures of Evaluation in Software Engineering</a>
<ul>
<li class="chapter" data-level="13.1" data-path="evaluationSE.html"><a href="evaluationSE.html#effort-estimation-evaluation-metrics"><i class="fa fa-check"></i><b>13.1</b> Effort estimation evaluation metrics</a></li>
<li class="chapter" data-level="13.2" data-path="evaluationSE.html"><a href="evaluationSE.html#evaluation-of-the-model-in-the-testing-data"><i class="fa fa-check"></i><b>13.2</b> Evaluation of the Model in the Testing data</a></li>
<li class="chapter" data-level="13.3" data-path="evaluationSE.html"><a href="evaluationSE.html#building-a-linear-model-on-the-telecom1-dataset"><i class="fa fa-check"></i><b>13.3</b> Building a Linear Model on the Telecom1 dataset</a></li>
<li class="chapter" data-level="13.4" data-path="evaluationSE.html"><a href="evaluationSE.html#building-a-linear-model-on-the-telecom1-dataset-with-all-observations"><i class="fa fa-check"></i><b>13.4</b> Building a Linear Model on the Telecom1 dataset with all observations</a></li>
<li class="chapter" data-level="13.5" data-path="evaluationSE.html"><a href="evaluationSE.html#standardised-accuracy.-marp0-using-the-china-test-dataset"><i class="fa fa-check"></i><b>13.5</b> Standardised Accuracy. MARP0 using the China Test dataset</a></li>
<li class="chapter" data-level="13.6" data-path="evaluationSE.html"><a href="evaluationSE.html#standardised-accuracy.-marp0-using-the-telecom1-dataset"><i class="fa fa-check"></i><b>13.6</b> Standardised Accuracy. MARP0 using the Telecom1 dataset</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="evaluationSE.html"><a href="evaluationSE.html#marp0-using-the-atkinson-dataset"><i class="fa fa-check"></i><b>13.6.1</b> MARP0 using the Atkinson dataset</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="evaluationSE.html"><a href="evaluationSE.html#exact-marp0"><i class="fa fa-check"></i><b>13.7</b> Exact MARP0</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="wbl-simple-r-code-to-calculate-shepperd-and-macdonells-marp0-exactly.html"><a href="wbl-simple-r-code-to-calculate-shepperd-and-macdonells-marp0-exactly.html"><i class="fa fa-check"></i><b>14</b> WBL simple R code to calculate Shepperd and MacDonell’s MARP0 exactly</a>
<ul>
<li class="chapter" data-level="14.1" data-path="wbl-simple-r-code-to-calculate-shepperd-and-macdonells-marp0-exactly.html"><a href="wbl-simple-r-code-to-calculate-shepperd-and-macdonells-marp0-exactly.html#computing-the-bootstraped-confidence-interval-of-the-mean-for-the-test-observations-of-the-china-dataset"><i class="fa fa-check"></i><b>14.1</b> Computing the bootstraped confidence interval of the mean for the Test observations of the China dataset:</a></li>
<li class="chapter" data-level="14.2" data-path="wbl-simple-r-code-to-calculate-shepperd-and-macdonells-marp0-exactly.html"><a href="wbl-simple-r-code-to-calculate-shepperd-and-macdonells-marp0-exactly.html#defect-prediction-evaluation-metrics"><i class="fa fa-check"></i><b>14.2</b> Defect prediction evaluation metrics</a></li>
</ul></li>
<li class="part"><span><b>IX Advanced Topics</b></span></li>
<li class="chapter" data-level="15" data-path="feature-selection-1.html"><a href="feature-selection-1.html"><i class="fa fa-check"></i><b>15</b> Feature Selection</a>
<ul>
<li class="chapter" data-level="15.1" data-path="feature-selection-1.html"><a href="feature-selection-1.html#instance-selection-1"><i class="fa fa-check"></i><b>15.1</b> Instance Selection</a></li>
<li class="chapter" data-level="15.2" data-path="feature-selection-1.html"><a href="feature-selection-1.html#missing-data-imputation"><i class="fa fa-check"></i><b>15.2</b> Missing Data Imputation</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="feature-selection-example.html"><a href="feature-selection-example.html"><i class="fa fa-check"></i><b>16</b> Feature Selection Example</a></li>
<li class="chapter" data-level="17" data-path="advanced-models.html"><a href="advanced-models.html"><i class="fa fa-check"></i><b>17</b> Advanced Models</a>
<ul>
<li class="chapter" data-level="17.1" data-path="advanced-models.html"><a href="advanced-models.html#genetic-programming-for-symbolic-regression"><i class="fa fa-check"></i><b>17.1</b> Genetic Programming for Symbolic Regression</a></li>
<li class="chapter" data-level="17.2" data-path="advanced-models.html"><a href="advanced-models.html#genetic-programming-example"><i class="fa fa-check"></i><b>17.2</b> Genetic Programming Example</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="advanced-models.html"><a href="advanced-models.html#load-data"><i class="fa fa-check"></i><b>17.2.1</b> Load Data</a></li>
<li class="chapter" data-level="17.2.2" data-path="advanced-models.html"><a href="advanced-models.html#genetic-programming-for-symbolic-regression-china-dataset."><i class="fa fa-check"></i><b>17.2.2</b> Genetic Programming for Symbolic Regression: China dataset.</a></li>
<li class="chapter" data-level="17.2.3" data-path="advanced-models.html"><a href="advanced-models.html#genetic-programming-for-symbolic-regression.-telecom1-dataset."><i class="fa fa-check"></i><b>17.2.3</b> Genetic Programming for Symbolic Regression. Telecom1 dataset.</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="advanced-models.html"><a href="advanced-models.html#neural-networks-1"><i class="fa fa-check"></i><b>17.3</b> Neural Networks</a></li>
<li class="chapter" data-level="17.4" data-path="advanced-models.html"><a href="advanced-models.html#support-vector-machines"><i class="fa fa-check"></i><b>17.4</b> Support Vector Machines</a></li>
<li class="chapter" data-level="17.5" data-path="advanced-models.html"><a href="advanced-models.html#ensembles"><i class="fa fa-check"></i><b>17.5</b> Ensembles</a>
<ul>
<li class="chapter" data-level="17.5.1" data-path="advanced-models.html"><a href="advanced-models.html#bagging"><i class="fa fa-check"></i><b>17.5.1</b> Bagging</a></li>
<li class="chapter" data-level="17.5.2" data-path="advanced-models.html"><a href="advanced-models.html#boosting"><i class="fa fa-check"></i><b>17.5.2</b> Boosting</a></li>
<li class="chapter" data-level="17.5.3" data-path="advanced-models.html"><a href="advanced-models.html#rotation-forests"><i class="fa fa-check"></i><b>17.5.3</b> Rotation Forests</a></li>
<li class="chapter" data-level="17.5.4" data-path="advanced-models.html"><a href="advanced-models.html#boosting-in-r"><i class="fa fa-check"></i><b>17.5.4</b> Boosting in R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="further-classification-models.html"><a href="further-classification-models.html"><i class="fa fa-check"></i><b>18</b> Further Classification Models</a>
<ul>
<li class="chapter" data-level="18.1" data-path="further-classification-models.html"><a href="further-classification-models.html#multilabel-classification"><i class="fa fa-check"></i><b>18.1</b> Multilabel classification</a></li>
<li class="chapter" data-level="18.2" data-path="further-classification-models.html"><a href="further-classification-models.html#semi-supervised-learning"><i class="fa fa-check"></i><b>18.2</b> Semi-supervised Learning</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="social-network-analysis-in-se.html"><a href="social-network-analysis-in-se.html"><i class="fa fa-check"></i><b>19</b> Social Network Analysis in SE</a></li>
<li class="chapter" data-level="20" data-path="text-mining-software-engineering-data.html"><a href="text-mining-software-engineering-data.html"><i class="fa fa-check"></i><b>20</b> Text Mining Software Engineering Data</a>
<ul>
<li class="chapter" data-level="20.1" data-path="text-mining-software-engineering-data.html"><a href="text-mining-software-engineering-data.html#terminology"><i class="fa fa-check"></i><b>20.1</b> Terminology</a></li>
<li class="chapter" data-level="20.2" data-path="text-mining-software-engineering-data.html"><a href="text-mining-software-engineering-data.html#example-of-classifying-bugs-from-bugzilla"><i class="fa fa-check"></i><b>20.2</b> Example of classifying bugs from Bugzilla</a></li>
<li class="chapter" data-level="20.3" data-path="text-mining-software-engineering-data.html"><a href="text-mining-software-engineering-data.html#extracting-data-from-twitter"><i class="fa fa-check"></i><b>20.3</b> Extracting data from Twitter</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>21</b> Time Series</a>
<ul>
<li class="chapter" data-level="21.1" data-path="time-series.html"><a href="time-series.html#web-tutorials-about-time-series"><i class="fa fa-check"></i><b>21.1</b> Web tutorials about Time Series:</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis in Software Engineering using R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="preprocessing" class="section level1" number="8">
<h1><span class="header-section-number">Chapter 8</span> Preprocessing</h1>
<p>Following the data mining process, we describe what is meant by preprocessing, classical supervised models, unsupervised models and evaluation in the context of software engineering with examples</p>
<p>This task is probably the hardest and where most of effort is spend in the data mining process. It is quite typical to transform the data, for example, finding inconsistencies, normalising, imputing missing values, tranforming input data, merging variables, etc.</p>
<p>Typically, preprocessing consist of the following tasks (subprocesses):</p>
<ul>
<li>Data cleaning (consistency, noise detection, outliers)</li>
<li>Data integration</li>
<li>Data transformation (normalisation, discretisation) and derivation of new attributes from existing ones (e.g., population density from population and area)</li>
<li>Missing data imputation</li>
<li>Data reduction (feature selection and instace selection)</li>
</ul>
<div id="data" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Data</h2>
<p><em>Consistent</em> data are semantically correct based on real-world knowledge of the problem, i.e., no constrains are violated and data that can be used for inducing models and analysis. For example, the LoC or effort is constrained to non-negative values. We can also consider that to multiple attributes are consistent among them, and even datasets (e.g., same metrics but collected by different tools)</p>
</div>
<div id="missing-values" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Missing values</h2>
<p><em>Missing values</em> will have a negative effect when analysing the data or learning models. The results can be biased when compared with the models induced from the complete data, the results can be harder to analyse, it may be needed to discard records with missing values depending on the algorithm and this can be an important problems with small datasets such as the effort estimation ones.</p>
<p>Missing data is typically classified into:
* MCAR (Missing Completely at Random) or MAR (Missing At Random) where there is no reason for those missing values and we can assume that the distribution could follow the attribute’s distribution.
* MNAR (Missing Not At Random) where there is a pattern for those missing values and it may may be advisable to check the data gathering process to try to understand why such information is missing.</p>
<p><em>Imputation</em> consists in replacing missing values for estimates of those missing values. Many algorithms do cannot handle missing values and therefore, imputation methods are needed. We can use simple approaches such as the replacing the missing values with the mean or mode of the attribute. More elaborated approaches include:</p>
<ul>
<li>EM (Expectation-Maximisation)</li>
<li>Distance-based
<ul>
<li>kNN (k Nearest Neighbours)</li>
<li>Clustering</li>
</ul></li>
</ul>
<p>In R, a missing value is represented with <code>NA</code> and the analyst must decide what to do with missing data. The simplest approach is to leave out instances (ignore missing -IM-) with with missing data. This functionality is supported by many base functions through the <code>na.rm</code> option.</p>
<p>The <code>mice</code> R package. MICE (Multivariate Imputation via Chained Equations) assumes that data are missing at random. Other packages include <code>Amelia</code>, <code>missForest</code>, <code>Hmisc</code> and <code>mi</code>.</p>
</div>
<div id="noise" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> Noise</h2>
<p>Imperfections of the real-world data that influences negatively in the induced machine learning models. Approaches to deal with noisy data include:
* Robust learners capable of handling noisy data (e.g., C4.5 through pruning strategies)
* Data polishing methods which aim to correct noisy instances prior training
* Noise filters which are used to identify and eliminate noisy instances from the training data.</p>
<p>Types of noise data:
* Class Noise (aka label noise).
+ There can be contradictory cases (all attributes have the same value except the class)
+ Misclassifications. The class attribute is not labeled with the true label (golden truth)
* Attribute Noise. Values of attributes that are noise, missing or unknown.</p>
</div>
<div id="outliers" class="section level2" number="8.4">
<h2><span class="header-section-number">8.4</span> Outliers</h2>
<p>There is a large amount of literature related to outlier detection, and furthermore several definitions of outlier exist.</p>
<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb430-1"><a href="preprocessing.html#cb430-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DMwR2)</span>
<span id="cb430-2"><a href="preprocessing.html#cb430-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(foreign)</span>
<span id="cb430-3"><a href="preprocessing.html#cb430-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb430-4"><a href="preprocessing.html#cb430-4" aria-hidden="true" tabindex="-1"></a>kc1 <span class="ot">&lt;-</span> <span class="fu">read.arff</span>(<span class="st">&quot;./datasets/defectPred/D1/KC1.arff&quot;</span>)</span></code></pre></div>
<p>The LOF algorithm (<code>lofactor</code>), given a data set it produces a vector of local outlier factors for each case.</p>
<div class="sourceCode" id="cb431"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb431-1"><a href="preprocessing.html#cb431-1" aria-hidden="true" tabindex="-1"></a>kc1num <span class="ot">&lt;-</span> kc1[,<span class="dv">1</span><span class="sc">:</span><span class="dv">21</span>]</span>
<span id="cb431-2"><a href="preprocessing.html#cb431-2" aria-hidden="true" tabindex="-1"></a>outlier.scores <span class="ot">&lt;-</span> <span class="fu">lofactor</span>(kc1num, <span class="at">k=</span><span class="dv">5</span>)</span>
<span id="cb431-3"><a href="preprocessing.html#cb431-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">density</span>(<span class="fu">na.omit</span>(outlier.scores)))</span></code></pre></div>
<p><img src="DASE_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<div class="sourceCode" id="cb432"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb432-1"><a href="preprocessing.html#cb432-1" aria-hidden="true" tabindex="-1"></a>outliers <span class="ot">&lt;-</span> <span class="fu">order</span>(outlier.scores, <span class="at">decreasing=</span>T)[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span>
<span id="cb432-2"><a href="preprocessing.html#cb432-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(outliers)</span></code></pre></div>
<pre><code>## [1]  1  6 14 31 33</code></pre>
<p>Another simple method of Hiridoglou and Berthelot for positive observations.</p>
</div>
<div id="feature-selection" class="section level2" number="8.5">
<h2><span class="header-section-number">8.5</span> Feature selection</h2>
<p>Feature Selection (FS) aims at identifying the most relevant attributes from a dataset. It is important in different ways:</p>
<ul>
<li><p>A reduced volume of data allows different data mining or searching techniques to be applied.</p></li>
<li><p>Irrelevant and redundant attributes can generate less accurate and more complex models. Furthermore, data mining algorithms can be executed faster.</p></li>
<li><p>It avoids the collection of data for those irrelevant and redundant attributes in the future.</p></li>
</ul>
<p>The problem of FS received a thorough treatment in pattern recognition and machine learning. Most of the FS algorithms tackle the task as a <em>search</em> problem, where each
state in the search specifies a distinct subset of the possible attributes <span class="citation">(<a href="#ref-BL97" role="doc-biblioref">Blum and Langley 1997</a>)</span>. The search procedure is combined with a criterion to evaluate the merit of each candidate subset of attributes. There are a multiple possible combinations between each procedure search and each attribute measure <span class="citation">(<a href="#ref-LY05" role="doc-biblioref">Liu and Yu 2005</a>)</span>.</p>
<p>There are two major approaches in FS from the method’s output point of view:</p>
<ul>
<li><p><em>Feature subset selection</em> (FSS)</p></li>
<li><p><em>Feature ranking</em> in which attributes are ranked as a list of features which are ordered according to evaluation measures (a subset of features is often selected from the top of the ranking list).</p></li>
</ul>
<p>FFS algorithms designed with different evaluation criteria broadly fall into two categories:</p>
<ul>
<li><p>The <em>filter</em> model relies on general characteristics of the data to evaluate and select feature subsets without involving any data mining algorithm.</p></li>
<li><p>The <em>wrapper</em> model requires one predetermined mining algorithm and uses its performance as the evaluation criterion. It searches for features better suited to the mining algorithm aiming to improve mining performance, but it also tends to be more computationally expensive than filter model <span class="citation"><a href="#ref-Lan94" role="doc-biblioref">Langley</a> (<a href="#ref-Lan94" role="doc-biblioref">1994</a>)</span>.</p></li>
</ul>
<p>Feature subset algorithms search through candidate feature subsets guide by a certain evaluation measure <span class="citation">(<a href="#ref-LM98" role="doc-biblioref">Liu and Motoda 1998</a>)</span> which captures the goodness of each subset. An optimal (or near optimal) subset is selected when the search stops.</p>
<p>Some existing evaluation measures that have been shown effective in removing both irrelevant and redundant features include the consistency measure <span class="citation">(<a href="#ref-DLM00" role="doc-biblioref">Dash, Liu, and Motoda 2000</a>)</span>, the correlation measure <span class="citation">(<a href="#ref-Hal99" role="doc-biblioref">Hall 1999</a>)</span> and the estimated accuracy of a learning algorithm <span class="citation">(<a href="#ref-KJ97" role="doc-biblioref">Kohavi and John 1997</a>)</span>.</p>
<ul>
<li><p><em>Consistency</em> measure attempts to find a minimum number of features that separate classes as consistently as the full set of features can. An inconsistency is defined as to instances having the same
feature values but different class labels.</p></li>
<li><p><em>Correlation</em> measure evaluates the goodness of feature subsets based on the hypothesis that good feature subsets contain features highly correlated to the class, yet uncorrelated to each other.</p></li>
<li><p><em>Wrapper-based</em> attribute selection uses the target learning algorithm to estimate the worth of attribute subsets. The feature subset selection algorithm conducts a search for a good subset using
the induction algorithm itself as part of the evaluation function.</p></li>
</ul>
<p>Langley <span class="citation">(<a href="#ref-Lan94" role="doc-biblioref">1994</a>)</span> notes that feature selection algorithms that search through the space of feature subsets must address four main issues: (i) the starting point of the search, (ii) the organization of the search, (iii) the evaluation of features subsets and (iv) the criterion used to terminate the search. Different algorithms address theses issues differently.</p>
<p>It is impractical to look at all possible feature subsets, even with a small number of attributes. Feature selection algorithms usually proceed greedily and are be classified into those that add features to an initially empty set (<em>forward selection</em>) and those that remove features from an initially complete set (<em>backwards elimination</em>). Hybrids both add and remove features as the algorithm progresses. Forward selection is much faster than backward elimination and therefore scales better to large data sets. A wide range of search strategies can be used: best-first, branch-and-bound, simulated annealing, genetic algorithms (see Kohavi and John <span class="citation">(<a href="#ref-KJ97" role="doc-biblioref">1997</a>)</span> for a review).</p>
<div id="fselector-package-in-r" class="section level3" number="8.5.1">
<h3><span class="header-section-number">8.5.1</span> FSelector package in R</h3>
<p>The FSelector package in R implements many algorithms available in Weka</p>
<div class="sourceCode" id="cb434"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb434-1"><a href="preprocessing.html#cb434-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(FSelector)</span>
<span id="cb434-2"><a href="preprocessing.html#cb434-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(foreign)</span>
<span id="cb434-3"><a href="preprocessing.html#cb434-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb434-4"><a href="preprocessing.html#cb434-4" aria-hidden="true" tabindex="-1"></a>cm1 <span class="ot">&lt;-</span> <span class="fu">read.arff</span>(<span class="st">&quot;./datasets/defectPred/D1/CM1.arff&quot;</span>)</span>
<span id="cb434-5"><a href="preprocessing.html#cb434-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb434-6"><a href="preprocessing.html#cb434-6" aria-hidden="true" tabindex="-1"></a>cm1RFWeigths <span class="ot">&lt;-</span> <span class="fu">random.forest.importance</span>(Defective <span class="sc">~</span> ., cm1)</span>
<span id="cb434-7"><a href="preprocessing.html#cb434-7" aria-hidden="true" tabindex="-1"></a><span class="fu">cutoff.biggest.diff</span>(cm1RFWeigths)</span></code></pre></div>
<pre><code>## [1] &quot;LOC_COMMENTS&quot;         &quot;NUM_UNIQUE_OPERATORS&quot;</code></pre>
<p>Using the Information Gain meaure as ranking:</p>
<div class="sourceCode" id="cb436"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb436-1"><a href="preprocessing.html#cb436-1" aria-hidden="true" tabindex="-1"></a>cm1GRWeights <span class="ot">&lt;-</span> <span class="fu">gain.ratio</span>(Defective <span class="sc">~</span> ., cm1)</span>
<span id="cb436-2"><a href="preprocessing.html#cb436-2" aria-hidden="true" tabindex="-1"></a>cm1GRWeights</span></code></pre></div>
<pre><code>##                                 attr_importance
## LOC_BLANK                                0.0000
## BRANCH_COUNT                             0.0000
## CALL_PAIRS                               0.0000
## LOC_CODE_AND_COMMENT                     0.0000
## LOC_COMMENTS                             0.0754
## CONDITION_COUNT                          0.0000
## CYCLOMATIC_COMPLEXITY                    0.0000
## CYCLOMATIC_DENSITY                       0.0000
## DECISION_COUNT                           0.0000
## DECISION_DENSITY                         0.0000
## DESIGN_COMPLEXITY                        0.0000
## DESIGN_DENSITY                           0.0000
## EDGE_COUNT                               0.0000
## ESSENTIAL_COMPLEXITY                     0.0000
## ESSENTIAL_DENSITY                        0.0000
## LOC_EXECUTABLE                           0.0888
## PARAMETER_COUNT                          0.0000
## HALSTEAD_CONTENT                         0.0701
## HALSTEAD_DIFFICULTY                      0.0000
## HALSTEAD_EFFORT                          0.0375
## HALSTEAD_ERROR_EST                       0.0448
## HALSTEAD_LENGTH                          0.0425
## HALSTEAD_LEVEL                           0.0000
## HALSTEAD_PROG_TIME                       0.0375
## HALSTEAD_VOLUME                          0.0471
## MAINTENANCE_SEVERITY                     0.0000
## MODIFIED_CONDITION_COUNT                 0.0000
## MULTIPLE_CONDITION_COUNT                 0.0000
## NODE_COUNT                               0.0000
## NORMALIZED_CYLOMATIC_COMPLEXITY          0.0000
## NUM_OPERANDS                             0.0000
## NUM_OPERATORS                            0.0471
## NUM_UNIQUE_OPERANDS                      0.0589
## NUM_UNIQUE_OPERATORS                     0.0616
## NUMBER_OF_LINES                          0.0573
## PERCENT_COMMENTS                         0.0663
## LOC_TOTAL                                0.0763</code></pre>
<div class="sourceCode" id="cb438"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb438-1"><a href="preprocessing.html#cb438-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cutoff.biggest.diff</span>(cm1GRWeights)</span></code></pre></div>
<pre><code>##  [1] &quot;LOC_EXECUTABLE&quot;       &quot;LOC_TOTAL&quot;            &quot;LOC_COMMENTS&quot;        
##  [4] &quot;HALSTEAD_CONTENT&quot;     &quot;PERCENT_COMMENTS&quot;     &quot;NUM_UNIQUE_OPERATORS&quot;
##  [7] &quot;NUM_UNIQUE_OPERANDS&quot;  &quot;NUMBER_OF_LINES&quot;      &quot;HALSTEAD_VOLUME&quot;     
## [10] &quot;NUM_OPERATORS&quot;        &quot;HALSTEAD_ERROR_EST&quot;   &quot;HALSTEAD_LENGTH&quot;     
## [13] &quot;HALSTEAD_EFFORT&quot;      &quot;HALSTEAD_PROG_TIME&quot;</code></pre>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb440-1"><a href="preprocessing.html#cb440-1" aria-hidden="true" tabindex="-1"></a><span class="co"># After assigning weights, we can select the statistaclly significant ones</span></span>
<span id="cb440-2"><a href="preprocessing.html#cb440-2" aria-hidden="true" tabindex="-1"></a>cm1X2Weights <span class="ot">&lt;-</span> <span class="fu">chi.squared</span>(Defective <span class="sc">~</span> ., cm1)</span>
<span id="cb440-3"><a href="preprocessing.html#cb440-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cutoff.biggest.diff</span>(cm1X2Weights)</span></code></pre></div>
<pre><code>##  [1] &quot;LOC_EXECUTABLE&quot;       &quot;LOC_COMMENTS&quot;         &quot;LOC_TOTAL&quot;           
##  [4] &quot;NUM_UNIQUE_OPERATORS&quot; &quot;NUM_UNIQUE_OPERANDS&quot;  &quot;NUMBER_OF_LINES&quot;     
##  [7] &quot;HALSTEAD_VOLUME&quot;      &quot;NUM_OPERATORS&quot;        &quot;HALSTEAD_ERROR_EST&quot;  
## [10] &quot;HALSTEAD_CONTENT&quot;     &quot;HALSTEAD_EFFORT&quot;      &quot;HALSTEAD_PROG_TIME&quot;  
## [13] &quot;HALSTEAD_LENGTH&quot;      &quot;PERCENT_COMMENTS&quot;</code></pre>
<p>Using CFS attribute selection</p>
<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb442-1"><a href="preprocessing.html#cb442-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(FSelector)</span>
<span id="cb442-2"><a href="preprocessing.html#cb442-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(foreign)</span>
<span id="cb442-3"><a href="preprocessing.html#cb442-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb442-4"><a href="preprocessing.html#cb442-4" aria-hidden="true" tabindex="-1"></a>cm1 <span class="ot">&lt;-</span> <span class="fu">read.arff</span>(<span class="st">&quot;./datasets/defectPred/D1/CM1.arff&quot;</span>)</span>
<span id="cb442-5"><a href="preprocessing.html#cb442-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb442-6"><a href="preprocessing.html#cb442-6" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">cfs</span>(Defective <span class="sc">~</span> ., cm1)</span>
<span id="cb442-7"><a href="preprocessing.html#cb442-7" aria-hidden="true" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="fu">as.simple.formula</span>(result, <span class="st">&quot;Defective&quot;</span>)</span>
<span id="cb442-8"><a href="preprocessing.html#cb442-8" aria-hidden="true" tabindex="-1"></a>f</span></code></pre></div>
<pre><code>## Defective ~ LOC_COMMENTS + LOC_EXECUTABLE + HALSTEAD_CONTENT + 
##     NUM_UNIQUE_OPERATORS + PERCENT_COMMENTS
## &lt;environment: 0x5654ac86e2d0&gt;</code></pre>
<p>Other packages for Feature selection in R include <code>FSelectorRccp</code> which re-implments the FSlector without WEKA dependencies.</p>
<p>Another popular package is <code>Boruta</code>, which is based on selection based on Random Forest.</p>
</div>
</div>
<div id="instance-selection" class="section level2" number="8.6">
<h2><span class="header-section-number">8.6</span> Instance selection</h2>
<p>Removal of samples (complementary to the removal of attributes) in order to scale down the dataset prior to learning a model so that there is (almost) no performance loss.</p>
<p>There are two types of processes:</p>
<ul>
<li><p><em>Prototype Selection</em> (PS) <span class="citation">(<a href="#ref-GDCH12" role="doc-biblioref">Garcia et al. 2012</a>)</span> when the subset is used with a distance based method (kNN)</p></li>
<li><p><em>Training Set Selection</em> (TSS) <span class="citation">(<a href="#ref-CanoHL07" role="doc-biblioref">Cano, Herrera, and Lozano 2007</a>)</span> in which an actual model is learned.</p></li>
</ul>
<p>It is also a search problem as with <em>feature selection</em>. Garcia et al. <span class="citation">(<a href="#ref-GDCH12" role="doc-biblioref">2012</a>)</span> provide a comprehensive overview of the topic.</p>
</div>
<div id="discretization" class="section level2" number="8.7">
<h2><span class="header-section-number">8.7</span> Discretization</h2>
<p>This process transforms continuous attributes into discrete ones, by associating categorical values to intervals and thus transforming quantitative data into qualitative data.</p>
</div>
<div id="correlation-coefficient-and-covariance-for-numeric-data" class="section level2" number="8.8">
<h2><span class="header-section-number">8.8</span> Correlation Coefficient and Covariance for Numeric Data</h2>
<p>Two random variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are called independent if the probability distribution of one variable is not affected by the presence of another.</p>
<p><span class="math inline">\(\tilde{\chi}^2=\frac{1}{d}\sum_{k=1}^{n} \frac{(O_k - E_k)^2}{E_k}\)</span></p>
<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb444-1"><a href="preprocessing.html#cb444-1" aria-hidden="true" tabindex="-1"></a><span class="fu">chisq.test</span>(kc1<span class="sc">$</span>LOC_BLANK,kc1<span class="sc">$</span>BRANCH_TOTAL)</span></code></pre></div>
<pre><code>## 
##  Chi-squared test for given probabilities
## 
## data:  kc1$LOC_BLANK
## X-squared = 17705, df = 2095, p-value &lt;2e-16</code></pre>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb446-1"><a href="preprocessing.html#cb446-1" aria-hidden="true" tabindex="-1"></a><span class="fu">chisq.test</span>(kc1<span class="sc">$</span>DESIGN_COMPLEXITY,kc1<span class="sc">$</span>CYCLOMATIC_COMPLEXITY)</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  kc1$DESIGN_COMPLEXITY and kc1$CYCLOMATIC_COMPLEXITY
## X-squared = 25101, df = 696, p-value &lt;2e-16</code></pre>
</div>
<div id="normalization-1" class="section level2" number="8.9">
<h2><span class="header-section-number">8.9</span> Normalization</h2>
<div id="min-max-normalization" class="section level3" number="8.9.1">
<h3><span class="header-section-number">8.9.1</span> Min-Max Normalization</h3>
<p><span class="math inline">\(z_i=\frac{x_i-\min(x)}{\max(x)-\min(x)}\)</span></p>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb448-1"><a href="preprocessing.html#cb448-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb448-2"><a href="preprocessing.html#cb448-2" aria-hidden="true" tabindex="-1"></a>preObj <span class="ot">&lt;-</span> <span class="fu">preProcess</span>(kc1[, <span class="sc">-</span><span class="dv">22</span>], <span class="at">method=</span><span class="fu">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>))</span></code></pre></div>
</div>
<div id="z-score-normalization" class="section level3" number="8.9.2">
<h3><span class="header-section-number">8.9.2</span> Z-score normalization</h3>
<p>TBD</p>
</div>
</div>
<div id="transformations" class="section level2" number="8.10">
<h2><span class="header-section-number">8.10</span> Transformations</h2>
<div id="linear-transformations-and-quadratic-trans-formations" class="section level3" number="8.10.1">
<h3><span class="header-section-number">8.10.1</span> Linear Transformations and Quadratic Trans formations</h3>
<p>TBD</p>
</div>
<div id="box-cox-transformation" class="section level3" number="8.10.2">
<h3><span class="header-section-number">8.10.2</span> Box-cox transformation</h3>
<p>TBD</p>
</div>
<div id="nominal-to-binary-tranformations" class="section level3" number="8.10.3">
<h3><span class="header-section-number">8.10.3</span> Nominal to Binary tranformations</h3>
<p>TBD</p>
</div>
</div>
<div id="preprocessing-in-r" class="section level2" number="8.11">
<h2><span class="header-section-number">8.11</span> Preprocessing in R</h2>
<div id="the-dplyr-package" class="section level3" number="8.11.1">
<h3><span class="header-section-number">8.11.1</span> The <code>dplyr</code> package</h3>
<p>The <em><a href="https://cran.r-project.org/web/packages/dplyr/index.html">dplyr</a></em> package created by Hadley Wickham. Some functions are similar to SQL syntax. key functions in dplyr include:</p>
<ul>
<li>select: select columns from a dataframe</li>
<li>filter: select rows from a dataframe</li>
<li>summarize: allows us to do summary stats based upon the grouped variable</li>
<li>group_by: group by a factor variable</li>
<li>arrange: order the dataset</li>
<li>joins: as in sql left join</li>
</ul>
<p>Tutorial:
<a href="https://github.com/justmarkham/dplyr-tutorial">https://github.com/justmarkham/dplyr-tutorial</a></p>
<p>Examples</p>
<div class="sourceCode" id="cb449"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb449-1"><a href="preprocessing.html#cb449-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span></code></pre></div>
<p>Describe the dataframe:</p>
<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb450-1"><a href="preprocessing.html#cb450-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(kc1)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    2096 obs. of  22 variables:
##  $ LOC_BLANK            : num  0 0 0 0 2 0 0 0 0 2 ...
##  $ BRANCH_COUNT         : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ LOC_CODE_AND_COMMENT : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ LOC_COMMENTS         : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ CYCLOMATIC_COMPLEXITY: num  1 1 1 1 1 1 1 1 1 1 ...
##  $ DESIGN_COMPLEXITY    : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ ESSENTIAL_COMPLEXITY : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ LOC_EXECUTABLE       : num  3 1 1 1 8 3 1 1 1 9 ...
##  $ HALSTEAD_CONTENT     : num  11.6 0 0 0 18 ...
##  $ HALSTEAD_DIFFICULTY  : num  2.67 0 0 0 3.5 2.67 0 0 0 3.75 ...
##  $ HALSTEAD_EFFORT      : num  82.3 0 0 0 220.9 ...
##  $ HALSTEAD_ERROR_EST   : num  0.01 0 0 0 0.02 0.01 0 0 0 0.04 ...
##  $ HALSTEAD_LENGTH      : num  11 1 1 1 19 11 1 1 1 29 ...
##  $ HALSTEAD_LEVEL       : num  0.38 0 0 0 0.29 0.38 0 0 0 0.27 ...
##  $ HALSTEAD_PROG_TIME   : num  4.57 0 0 0 12.27 ...
##  $ HALSTEAD_VOLUME      : num  30.9 0 0 0 63.1 ...
##  $ NUM_OPERANDS         : num  4 0 0 0 7 4 0 0 0 10 ...
##  $ NUM_OPERATORS        : num  7 1 1 1 12 7 1 1 1 19 ...
##  $ NUM_UNIQUE_OPERANDS  : num  3 0 0 0 5 3 0 0 0 8 ...
##  $ NUM_UNIQUE_OPERATORS : num  4 1 1 1 5 4 1 1 1 6 ...
##  $ LOC_TOTAL            : num  5 3 3 3 12 5 3 3 3 13 ...
##  $ Defective            : Factor w/ 2 levels &quot;N&quot;,&quot;Y&quot;: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<p><code>tbl_df</code> creates a “local data frame” as a wrapper for better printing</p>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb452-1"><a href="preprocessing.html#cb452-1" aria-hidden="true" tabindex="-1"></a>kc1_tbl <span class="ot">&lt;-</span> <span class="fu">tbl_df</span>(kc1) <span class="co">#deprecated</span></span></code></pre></div>
<pre><code>## Warning: `tbl_df()` was deprecated in dplyr 1.0.0.
## Please use `tibble::as_tibble()` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.</code></pre>
<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb454-1"><a href="preprocessing.html#cb454-1" aria-hidden="true" tabindex="-1"></a>kc1_tbl <span class="ot">&lt;-</span> <span class="fu">tibble</span>(kc1)</span></code></pre></div>
<p>Filter:</p>
<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb455-1"><a href="preprocessing.html#cb455-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter rows: use comma or &amp; to represent AND condition</span></span>
<span id="cb455-2"><a href="preprocessing.html#cb455-2" aria-hidden="true" tabindex="-1"></a><span class="fu">filter</span>(kc1_tbl, Defective <span class="sc">==</span> <span class="st">&quot;Y&quot;</span> <span class="sc">&amp;</span> LOC_BLANK <span class="sc">!=</span> <span class="dv">0</span>)</span></code></pre></div>
<pre><code>## # A tibble: 251 × 22
##    LOC_BLANK BRANCH_COUNT LOC_CODE_AND_COMMENT LOC_COMMENTS CYCLOMATIC_COMPLEXI…
##        &lt;dbl&gt;        &lt;dbl&gt;                &lt;dbl&gt;        &lt;dbl&gt;                &lt;dbl&gt;
##  1         6           21                    0           10                   11
##  2         5           15                    0            2                    8
##  3         2            5                    0            0                    3
##  4         4            5                    0            2                    3
##  5         2           11                    0            2                    6
##  6         2           23                    0            3                   12
##  7         1           11                    0            2                    6
##  8         1           13                    0            2                    7
##  9         2           17                    0            2                    9
## 10         3            1                    0            0                    1
## # … with 241 more rows, and 17 more variables: DESIGN_COMPLEXITY &lt;dbl&gt;,
## #   ESSENTIAL_COMPLEXITY &lt;dbl&gt;, LOC_EXECUTABLE &lt;dbl&gt;, HALSTEAD_CONTENT &lt;dbl&gt;,
## #   HALSTEAD_DIFFICULTY &lt;dbl&gt;, HALSTEAD_EFFORT &lt;dbl&gt;, HALSTEAD_ERROR_EST &lt;dbl&gt;,
## #   HALSTEAD_LENGTH &lt;dbl&gt;, HALSTEAD_LEVEL &lt;dbl&gt;, HALSTEAD_PROG_TIME &lt;dbl&gt;,
## #   HALSTEAD_VOLUME &lt;dbl&gt;, NUM_OPERANDS &lt;dbl&gt;, NUM_OPERATORS &lt;dbl&gt;,
## #   NUM_UNIQUE_OPERANDS &lt;dbl&gt;, NUM_UNIQUE_OPERATORS &lt;dbl&gt;, LOC_TOTAL &lt;dbl&gt;,
## #   Defective &lt;fct&gt;</code></pre>
<p>Another operator is <code>%in%</code>.</p>
<p>Select:</p>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb457-1"><a href="preprocessing.html#cb457-1" aria-hidden="true" tabindex="-1"></a><span class="fu">select</span>(kc1_tbl, <span class="fu">contains</span>(<span class="st">&quot;LOC&quot;</span>), Defective)</span></code></pre></div>
<pre><code>## # A tibble: 2,096 × 6
##    LOC_BLANK LOC_CODE_AND_COMME… LOC_COMMENTS LOC_EXECUTABLE LOC_TOTAL Defective
##        &lt;dbl&gt;               &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;    
##  1         0                   0            0              3         5 N        
##  2         0                   0            0              1         3 N        
##  3         0                   0            0              1         3 N        
##  4         0                   0            0              1         3 N        
##  5         2                   0            0              8        12 N        
##  6         0                   0            0              3         5 N        
##  7         0                   0            0              1         3 N        
##  8         0                   0            0              1         3 N        
##  9         0                   0            0              1         3 N        
## 10         2                   0            0              9        13 N        
## # … with 2,086 more rows</code></pre>
<p>Now, <code>kc1_tbl</code> contains(“LOC”), Defective</p>
<p>Filter and Select together:</p>
<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb459-1"><a href="preprocessing.html#cb459-1" aria-hidden="true" tabindex="-1"></a><span class="co"># nesting method</span></span>
<span id="cb459-2"><a href="preprocessing.html#cb459-2" aria-hidden="true" tabindex="-1"></a><span class="fu">filter</span>(<span class="fu">select</span>(kc1_tbl, <span class="fu">contains</span>(<span class="st">&quot;LOC&quot;</span>), Defective), Defective <span class="sc">!=</span><span class="dv">0</span>)</span></code></pre></div>
<pre><code>## # A tibble: 2,096 × 6
##    LOC_BLANK LOC_CODE_AND_COMME… LOC_COMMENTS LOC_EXECUTABLE LOC_TOTAL Defective
##        &lt;dbl&gt;               &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;    
##  1         0                   0            0              3         5 N        
##  2         0                   0            0              1         3 N        
##  3         0                   0            0              1         3 N        
##  4         0                   0            0              1         3 N        
##  5         2                   0            0              8        12 N        
##  6         0                   0            0              3         5 N        
##  7         0                   0            0              1         3 N        
##  8         0                   0            0              1         3 N        
##  9         0                   0            0              1         3 N        
## 10         2                   0            0              9        13 N        
## # … with 2,086 more rows</code></pre>
<p>It is easier usign the chaining method:</p>
<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb461-1"><a href="preprocessing.html#cb461-1" aria-hidden="true" tabindex="-1"></a><span class="co"># chaining method</span></span>
<span id="cb461-2"><a href="preprocessing.html#cb461-2" aria-hidden="true" tabindex="-1"></a>kc1_tbl <span class="sc">%&gt;%</span></span>
<span id="cb461-3"><a href="preprocessing.html#cb461-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(<span class="fu">contains</span>(<span class="st">&quot;LOC&quot;</span>), Defective) <span class="sc">%&gt;%</span></span>
<span id="cb461-4"><a href="preprocessing.html#cb461-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(Defective <span class="sc">!=</span><span class="dv">0</span>)</span></code></pre></div>
<pre><code>## # A tibble: 2,096 × 6
##    LOC_BLANK LOC_CODE_AND_COMME… LOC_COMMENTS LOC_EXECUTABLE LOC_TOTAL Defective
##        &lt;dbl&gt;               &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;    
##  1         0                   0            0              3         5 N        
##  2         0                   0            0              1         3 N        
##  3         0                   0            0              1         3 N        
##  4         0                   0            0              1         3 N        
##  5         2                   0            0              8        12 N        
##  6         0                   0            0              3         5 N        
##  7         0                   0            0              1         3 N        
##  8         0                   0            0              1         3 N        
##  9         0                   0            0              1         3 N        
## 10         2                   0            0              9        13 N        
## # … with 2,086 more rows</code></pre>
<p>Arrange ascending</p>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb463-1"><a href="preprocessing.html#cb463-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb463-2"><a href="preprocessing.html#cb463-2" aria-hidden="true" tabindex="-1"></a>kc1_tbl <span class="sc">%&gt;%</span></span>
<span id="cb463-3"><a href="preprocessing.html#cb463-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(LOC_TOTAL, Defective) <span class="sc">%&gt;%</span></span>
<span id="cb463-4"><a href="preprocessing.html#cb463-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">arrange</span>(LOC_TOTAL)</span></code></pre></div>
<pre><code>## # A tibble: 2,096 × 2
##    LOC_TOTAL Defective
##        &lt;dbl&gt; &lt;fct&gt;    
##  1         1 N        
##  2         1 N        
##  3         1 N        
##  4         1 N        
##  5         1 N        
##  6         1 N        
##  7         1 N        
##  8         1 N        
##  9         1 N        
## 10         1 N        
## # … with 2,086 more rows</code></pre>
<p>Arrange descending:</p>
<div class="sourceCode" id="cb465"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb465-1"><a href="preprocessing.html#cb465-1" aria-hidden="true" tabindex="-1"></a>kc1_tbl <span class="sc">%&gt;%</span></span>
<span id="cb465-2"><a href="preprocessing.html#cb465-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(LOC_TOTAL, Defective) <span class="sc">%&gt;%</span></span>
<span id="cb465-3"><a href="preprocessing.html#cb465-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">arrange</span>(<span class="fu">desc</span>(LOC_TOTAL))</span></code></pre></div>
<pre><code>## # A tibble: 2,096 × 2
##    LOC_TOTAL Defective
##        &lt;dbl&gt; &lt;fct&gt;    
##  1       288 Y        
##  2       286 Y        
##  3       283 N        
##  4       220 Y        
##  5       217 Y        
##  6       210 N        
##  7       205 Y        
##  8       184 Y        
##  9       179 Y        
## 10       176 Y        
## # … with 2,086 more rows</code></pre>
<p>Mutate:</p>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb467-1"><a href="preprocessing.html#cb467-1" aria-hidden="true" tabindex="-1"></a>kc1_tbl <span class="sc">%&gt;%</span></span>
<span id="cb467-2"><a href="preprocessing.html#cb467-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(Defective <span class="sc">==</span> <span class="st">&quot;Y&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb467-3"><a href="preprocessing.html#cb467-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(NUM_OPERANDS, NUM_OPERATORS, Defective) <span class="sc">%&gt;%</span></span>
<span id="cb467-4"><a href="preprocessing.html#cb467-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">HalsteadLength =</span> NUM_OPERANDS <span class="sc">+</span> NUM_OPERATORS)</span></code></pre></div>
<pre><code>## # A tibble: 325 × 4
##    NUM_OPERANDS NUM_OPERATORS Defective HalsteadLength
##           &lt;dbl&gt;         &lt;dbl&gt; &lt;fct&gt;              &lt;dbl&gt;
##  1           64           107 Y                    171
##  2           52            89 Y                    141
##  3           17            41 Y                     58
##  4           41            74 Y                    115
##  5           54            95 Y                    149
##  6           75           156 Y                    231
##  7           54            95 Y                    149
##  8           56            99 Y                    155
##  9           69           124 Y                    193
## 10           44            60 Y                    104
## # … with 315 more rows</code></pre>
<p><code>summarise</code>: Reduce variables to values</p>
<div class="sourceCode" id="cb469"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb469-1"><a href="preprocessing.html#cb469-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a table grouped by Defective, and then summarise each group by taking the mean of loc</span></span>
<span id="cb469-2"><a href="preprocessing.html#cb469-2" aria-hidden="true" tabindex="-1"></a>kc1_tbl <span class="sc">%&gt;%</span></span>
<span id="cb469-3"><a href="preprocessing.html#cb469-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">group_by</span>(Defective) <span class="sc">%&gt;%</span></span>
<span id="cb469-4"><a href="preprocessing.html#cb469-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarise</span>(<span class="at">avg_loc =</span> <span class="fu">mean</span>(LOC_TOTAL, <span class="at">na.rm=</span><span class="cn">TRUE</span>))</span></code></pre></div>
<pre><code>## # A tibble: 2 × 2
##   Defective avg_loc
##   &lt;fct&gt;       &lt;dbl&gt;
## 1 N            15.9
## 2 Y            44.7</code></pre>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb471-1"><a href="preprocessing.html#cb471-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a table grouped by Defective, and then summarise each group by taking the mean of loc</span></span>
<span id="cb471-2"><a href="preprocessing.html#cb471-2" aria-hidden="true" tabindex="-1"></a>kc1_tbl <span class="sc">%&gt;%</span></span>
<span id="cb471-3"><a href="preprocessing.html#cb471-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">group_by</span>(Defective) <span class="sc">%&gt;%</span></span>
<span id="cb471-4"><a href="preprocessing.html#cb471-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarise_each</span>(<span class="fu">funs</span>(mean, min, max), BRANCH_COUNT, LOC_TOTAL)</span></code></pre></div>
<pre><code>## Warning: `summarise_each_()` was deprecated in dplyr 0.7.0.
## Please use `across()` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.</code></pre>
<pre><code>## Warning: `funs()` was deprecated in dplyr 0.8.0.
## Please use a list of either functions or lambdas: 
## 
##   # Simple named list: 
##   list(mean = mean, median = median)
## 
##   # Auto named with `tibble::lst()`: 
##   tibble::lst(mean, median)
## 
##   # Using lambdas
##   list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.</code></pre>
<pre><code>## # A tibble: 2 × 7
##   Defective BRANCH_COUNT_mean LOC_TOTAL_mean BRANCH_COUNT_min LOC_TOTAL_min
##   &lt;fct&gt;                 &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;
## 1 N                      3.68           15.9                1             1
## 2 Y                     10.1            44.7                1             2
## # … with 2 more variables: BRANCH_COUNT_max &lt;dbl&gt;, LOC_TOTAL_max &lt;dbl&gt;</code></pre>
<p>It seems than the number of <em>Defective</em> modules is larger than the <em>Non-Defective</em> ones. We can count them with:</p>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb475-1"><a href="preprocessing.html#cb475-1" aria-hidden="true" tabindex="-1"></a><span class="co"># n() or tally</span></span>
<span id="cb475-2"><a href="preprocessing.html#cb475-2" aria-hidden="true" tabindex="-1"></a>kc1_tbl <span class="sc">%&gt;%</span></span>
<span id="cb475-3"><a href="preprocessing.html#cb475-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">group_by</span>(Defective) <span class="sc">%&gt;%</span></span>
<span id="cb475-4"><a href="preprocessing.html#cb475-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tally</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 × 2
##   Defective     n
##   &lt;fct&gt;     &lt;int&gt;
## 1 N          1771
## 2 Y           325</code></pre>
<p>It seems that it’s an imbalanced dataset…</p>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb477-1"><a href="preprocessing.html#cb477-1" aria-hidden="true" tabindex="-1"></a><span class="co"># randomly sample a fixed number of rows, without replacement</span></span>
<span id="cb477-2"><a href="preprocessing.html#cb477-2" aria-hidden="true" tabindex="-1"></a>kc1_tbl <span class="sc">%&gt;%</span> <span class="fu">sample_n</span>(<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## # A tibble: 2 × 22
##   LOC_BLANK BRANCH_COUNT LOC_CODE_AND_COMMENT LOC_COMMENTS CYCLOMATIC_COMPLEXITY
##       &lt;dbl&gt;        &lt;dbl&gt;                &lt;dbl&gt;        &lt;dbl&gt;                 &lt;dbl&gt;
## 1         0            3                    0            0                     2
## 2         0            1                    0            0                     1
## # … with 17 more variables: DESIGN_COMPLEXITY &lt;dbl&gt;,
## #   ESSENTIAL_COMPLEXITY &lt;dbl&gt;, LOC_EXECUTABLE &lt;dbl&gt;, HALSTEAD_CONTENT &lt;dbl&gt;,
## #   HALSTEAD_DIFFICULTY &lt;dbl&gt;, HALSTEAD_EFFORT &lt;dbl&gt;, HALSTEAD_ERROR_EST &lt;dbl&gt;,
## #   HALSTEAD_LENGTH &lt;dbl&gt;, HALSTEAD_LEVEL &lt;dbl&gt;, HALSTEAD_PROG_TIME &lt;dbl&gt;,
## #   HALSTEAD_VOLUME &lt;dbl&gt;, NUM_OPERANDS &lt;dbl&gt;, NUM_OPERATORS &lt;dbl&gt;,
## #   NUM_UNIQUE_OPERANDS &lt;dbl&gt;, NUM_UNIQUE_OPERATORS &lt;dbl&gt;, LOC_TOTAL &lt;dbl&gt;,
## #   Defective &lt;fct&gt;</code></pre>
<div class="sourceCode" id="cb479"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb479-1"><a href="preprocessing.html#cb479-1" aria-hidden="true" tabindex="-1"></a><span class="co"># randomly sample a fraction of rows, with replacement</span></span>
<span id="cb479-2"><a href="preprocessing.html#cb479-2" aria-hidden="true" tabindex="-1"></a>kc1_tbl <span class="sc">%&gt;%</span> <span class="fu">sample_frac</span>(<span class="fl">0.05</span>, <span class="at">replace=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## # A tibble: 105 × 22
##    LOC_BLANK BRANCH_COUNT LOC_CODE_AND_COMMENT LOC_COMMENTS CYCLOMATIC_COMPLEXI…
##        &lt;dbl&gt;        &lt;dbl&gt;                &lt;dbl&gt;        &lt;dbl&gt;                &lt;dbl&gt;
##  1         1            3                    0            0                    2
##  2         0            1                    0            0                    1
##  3         0            1                    0            0                    1
##  4         2            5                    0            0                    3
##  5         2            7                    0            0                    4
##  6         0            1                    0            0                    1
##  7         0            1                    0            0                    1
##  8         0            1                    0            0                    1
##  9         0            1                    0            0                    1
## 10         0            1                    0            1                    1
## # … with 95 more rows, and 17 more variables: DESIGN_COMPLEXITY &lt;dbl&gt;,
## #   ESSENTIAL_COMPLEXITY &lt;dbl&gt;, LOC_EXECUTABLE &lt;dbl&gt;, HALSTEAD_CONTENT &lt;dbl&gt;,
## #   HALSTEAD_DIFFICULTY &lt;dbl&gt;, HALSTEAD_EFFORT &lt;dbl&gt;, HALSTEAD_ERROR_EST &lt;dbl&gt;,
## #   HALSTEAD_LENGTH &lt;dbl&gt;, HALSTEAD_LEVEL &lt;dbl&gt;, HALSTEAD_PROG_TIME &lt;dbl&gt;,
## #   HALSTEAD_VOLUME &lt;dbl&gt;, NUM_OPERANDS &lt;dbl&gt;, NUM_OPERATORS &lt;dbl&gt;,
## #   NUM_UNIQUE_OPERANDS &lt;dbl&gt;, NUM_UNIQUE_OPERATORS &lt;dbl&gt;, LOC_TOTAL &lt;dbl&gt;,
## #   Defective &lt;fct&gt;</code></pre>
<div class="sourceCode" id="cb481"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb481-1"><a href="preprocessing.html#cb481-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Better formatting adapted to the screen width</span></span>
<span id="cb481-2"><a href="preprocessing.html#cb481-2" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(kc1_tbl)</span></code></pre></div>
<pre><code>## Rows: 2,096
## Columns: 22
## $ LOC_BLANK             &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 2, 1, 2, 2, …
## $ BRANCH_COUNT          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, …
## $ LOC_CODE_AND_COMMENT  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ LOC_COMMENTS          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ CYCLOMATIC_COMPLEXITY &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, …
## $ DESIGN_COMPLEXITY     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, …
## $ ESSENTIAL_COMPLEXITY  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, …
## $ LOC_EXECUTABLE        &lt;dbl&gt; 3, 1, 1, 1, 8, 3, 1, 1, 1, 9, 8, 1, 8, 1, 8, 12,…
## $ HALSTEAD_CONTENT      &lt;dbl&gt; 11.6, 0.0, 0.0, 0.0, 18.0, 11.6, 0.0, 0.0, 0.0, …
## $ HALSTEAD_DIFFICULTY   &lt;dbl&gt; 2.67, 0.00, 0.00, 0.00, 3.50, 2.67, 0.00, 0.00, …
## $ HALSTEAD_EFFORT       &lt;dbl&gt; 82.3, 0.0, 0.0, 0.0, 220.9, 82.3, 0.0, 0.0, 0.0,…
## $ HALSTEAD_ERROR_EST    &lt;dbl&gt; 0.01, 0.00, 0.00, 0.00, 0.02, 0.01, 0.00, 0.00, …
## $ HALSTEAD_LENGTH       &lt;dbl&gt; 11, 1, 1, 1, 19, 11, 1, 1, 1, 29, 19, 1, 19, 1, …
## $ HALSTEAD_LEVEL        &lt;dbl&gt; 0.38, 0.00, 0.00, 0.00, 0.29, 0.38, 0.00, 0.00, …
## $ HALSTEAD_PROG_TIME    &lt;dbl&gt; 4.57, 0.00, 0.00, 0.00, 12.27, 4.57, 0.00, 0.00,…
## $ HALSTEAD_VOLUME       &lt;dbl&gt; 30.9, 0.0, 0.0, 0.0, 63.1, 30.9, 0.0, 0.0, 0.0, …
## $ NUM_OPERANDS          &lt;dbl&gt; 4, 0, 0, 0, 7, 4, 0, 0, 0, 10, 7, 0, 7, 0, 11, 1…
## $ NUM_OPERATORS         &lt;dbl&gt; 7, 1, 1, 1, 12, 7, 1, 1, 1, 19, 12, 1, 12, 1, 16…
## $ NUM_UNIQUE_OPERANDS   &lt;dbl&gt; 3, 0, 0, 0, 5, 3, 0, 0, 0, 8, 5, 0, 5, 0, 6, 9, …
## $ NUM_UNIQUE_OPERATORS  &lt;dbl&gt; 4, 1, 1, 1, 5, 4, 1, 1, 1, 6, 5, 1, 5, 1, 8, 12,…
## $ LOC_TOTAL             &lt;dbl&gt; 5, 3, 3, 3, 12, 5, 3, 3, 3, 13, 12, 3, 12, 4, 13…
## $ Defective             &lt;fct&gt; N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, …</code></pre>
</div>
</div>
<div id="other-libraries-and-tricks" class="section level2" number="8.12">
<h2><span class="header-section-number">8.12</span> Other libraries and tricks</h2>
<p>The <code>lubridate</code> package contains a number of functions facilitating the conversion of text to
POSIX dates. As an example, consider the following code. We may use this, for example, with time series.</p>
<p>For example <a href="https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf">https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf</a></p>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb483-1"><a href="preprocessing.html#cb483-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lubridate)</span>
<span id="cb483-2"><a href="preprocessing.html#cb483-2" aria-hidden="true" tabindex="-1"></a>dates <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;15/02/2013&quot;</span>, <span class="st">&quot;15 Feb 13&quot;</span>, <span class="st">&quot;It happened on 15 02 &#39;13&quot;</span>)</span>
<span id="cb483-3"><a href="preprocessing.html#cb483-3" aria-hidden="true" tabindex="-1"></a><span class="fu">dmy</span>(dates)</span></code></pre></div>
<pre><code>## [1] &quot;2013-02-15&quot; &quot;2013-02-15&quot; &quot;2013-02-15&quot;</code></pre>

</div>
</div>



<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-BL97" class="csl-entry">
Blum, A. L., and P. Langley. 1997. <span>“Selection of Relevant Features and Examples in Machine Learning.”</span> Edited by R. Greiner and D. Subramanian. <em>Artificial Intelligence</em> 97 (1-2): 245–71.
</div>
<div id="ref-CanoHL07" class="csl-entry">
Cano, José Ramón, Francisco Herrera, and Manuel Lozano. 2007. <span>“Evolutionary Stratified Training Set Selection for Extracting Classification Rules with Trade Off Precision-Interpretability.”</span> <em>Data &amp; Knowledge Engineering</em> 60 (1): 90–108. <a href="https://doi.org/10.1016/j.datak.2006.01.008">https://doi.org/10.1016/j.datak.2006.01.008</a>.
</div>
<div id="ref-DLM00" class="csl-entry">
Dash, M., H. Liu, and H. Motoda. 2000. <span>“Consistency Based Feature Selection.”</span> In <em>Pacific-Asia Conf. On Knowledge Discovery and Data Mining</em>, 98–109.
</div>
<div id="ref-GDCH12" class="csl-entry">
Garcia, S., J. Derrac, J. Cano, and F. Herrera. 2012. <span>“Prototype Selection for Nearest Neighbor Classification: Taxonomy and Empirical Study.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 34 (3): 417–35. <a href="https://doi.org/10.1109/TPAMI.2011.142">https://doi.org/10.1109/TPAMI.2011.142</a>.
</div>
<div id="ref-Hal99" class="csl-entry">
Hall, M. A. 1999. <span>“Correlation-Based Feature Selection for Machine Learning.”</span> PhD thesis, Hamilton, New Zealand: University of Waikato, Department of Computer Science.
</div>
<div id="ref-KJ97" class="csl-entry">
Kohavi, R., and G. H. John. 1997. <span>“Wrappers for Feature Subset Selection.”</span> <em>Artificial Intelligence</em> 1-2: 273–324.
</div>
<div id="ref-Lan94" class="csl-entry">
Langley, P. 1994. <span>“Selection of Relevant Features in Machine Learning.”</span> In <em>Procs. Of the AAAI Fall Symposium on Relevance</em>, 140–44.
</div>
<div id="ref-LM98" class="csl-entry">
Liu, H., and H. Motoda. 1998. <em>Feature Selection for Knowlegde Discovery and Data Mining</em>. London, UK: Kluwer Academic Publishers.
</div>
<div id="ref-LY05" class="csl-entry">
Liu, H., and L. Yu. 2005. <span>“Toward Integrating Feature Selection Algorithms for Classification and Clustering.”</span> <em>IEEE Trans. On Knowledge and Data Eng.</em> 17 (3): 1–12.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classical-hypothesis-testing.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="supervised-classification.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/danrodgar/dasedown/edit/master/300_basicPreprocessing.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DASE.pdf", "DASE.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
