<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 What is Data Mining / Knowledge Discovery in Databases (KDD) | Data Analysis in Software Engineering using R</title>
  <meta name="description" content="DASE Data Analysis in Software Engineering" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 What is Data Mining / Knowledge Discovery in Databases (KDD) | Data Analysis in Software Engineering using R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="DASE Data Analysis in Software Engineering" />
  <meta name="github-repo" content="danrodgar/DASE" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 What is Data Mining / Knowledge Discovery in Databases (KDD) | Data Analysis in Software Engineering using R" />
  
  <meta name="twitter:description" content="DASE Data Analysis in Software Engineering" />
  

<meta name="author" content="Daniel Rodriguez and Javier Dolado" />


<meta name="date" content="2021-10-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="r-intro.html"/>
<link rel="next" href="evaluationSE.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Analysis in Software Engineering with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="part"><span><b>I Introduction to the R Language</b></span></li>
<li class="chapter" data-level="1" data-path="r-intro.html"><a href="r-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-intro.html"><a href="r-intro.html#installation"><i class="fa fa-check"></i><b>1.1</b> Installation</a></li>
<li class="chapter" data-level="1.2" data-path="r-intro.html"><a href="r-intro.html#r-and-rstudio"><i class="fa fa-check"></i><b>1.2</b> R and RStudio</a></li>
<li class="chapter" data-level="1.3" data-path="r-intro.html"><a href="r-intro.html#basic-data-types"><i class="fa fa-check"></i><b>1.3</b> Basic Data Types</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="r-intro.html"><a href="r-intro.html#mising-values"><i class="fa fa-check"></i><b>1.3.1</b> Mising values</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="r-intro.html"><a href="r-intro.html#vectors"><i class="fa fa-check"></i><b>1.4</b> Vectors</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="r-intro.html"><a href="r-intro.html#coercion-for-vectors"><i class="fa fa-check"></i><b>1.4.1</b> Coercion for vectors</a></li>
<li class="chapter" data-level="1.4.2" data-path="r-intro.html"><a href="r-intro.html#vector-arithmetic"><i class="fa fa-check"></i><b>1.4.2</b> Vector arithmetic</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="r-intro.html"><a href="r-intro.html#arrays-and-matrices"><i class="fa fa-check"></i><b>1.5</b> Arrays and Matrices</a></li>
<li class="chapter" data-level="1.6" data-path="r-intro.html"><a href="r-intro.html#factors"><i class="fa fa-check"></i><b>1.6</b> Factors</a></li>
<li class="chapter" data-level="1.7" data-path="r-intro.html"><a href="r-intro.html#lists"><i class="fa fa-check"></i><b>1.7</b> Lists</a></li>
<li class="chapter" data-level="1.8" data-path="r-intro.html"><a href="r-intro.html#data-frames"><i class="fa fa-check"></i><b>1.8</b> Data frames</a></li>
<li class="chapter" data-level="1.9" data-path="r-intro.html"><a href="r-intro.html#r-functional-functions"><i class="fa fa-check"></i><b>1.9</b> R Functional Functions</a></li>
<li class="chapter" data-level="1.10" data-path="r-intro.html"><a href="r-intro.html#environments"><i class="fa fa-check"></i><b>1.10</b> Environments</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="r-intro.html"><a href="r-intro.html#global-variables-local-variables-and-programming-scope"><i class="fa fa-check"></i><b>1.10.1</b> Global variables, local variables and programming scope</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="r-intro.html"><a href="r-intro.html#reading-data"><i class="fa fa-check"></i><b>1.11</b> Reading Data</a></li>
<li class="chapter" data-level="1.12" data-path="r-intro.html"><a href="r-intro.html#plots"><i class="fa fa-check"></i><b>1.12</b> Plots</a></li>
<li class="chapter" data-level="1.13" data-path="r-intro.html"><a href="r-intro.html#control-flow-in-r"><i class="fa fa-check"></i><b>1.13</b> Control flow in R</a></li>
<li class="chapter" data-level="1.14" data-path="r-intro.html"><a href="r-intro.html#built-in-datasets"><i class="fa fa-check"></i><b>1.14</b> Built-in Datasets</a></li>
<li class="chapter" data-level="1.15" data-path="r-intro.html"><a href="r-intro.html#other-tools-with-r"><i class="fa fa-check"></i><b>1.15</b> Other tools with R</a>
<ul>
<li class="chapter" data-level="1.15.1" data-path="r-intro.html"><a href="r-intro.html#rattle"><i class="fa fa-check"></i><b>1.15.1</b> Rattle</a></li>
<li class="chapter" data-level="1.15.2" data-path="r-intro.html"><a href="r-intro.html#jamovi"><i class="fa fa-check"></i><b>1.15.2</b> Jamovi</a></li>
<li class="chapter" data-level="1.15.3" data-path="r-intro.html"><a href="r-intro.html#jasp"><i class="fa fa-check"></i><b>1.15.3</b> JASP</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Introduction to Data Mining</b></span></li>
<li class="chapter" data-level="2" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><i class="fa fa-check"></i><b>2</b> What is Data Mining / Knowledge Discovery in Databases (KDD)</a>
<ul>
<li class="chapter" data-level="2.1" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#the-aim-of-data-analysis-and-statistical-learning"><i class="fa fa-check"></i><b>2.1</b> The Aim of Data Analysis and Statistical Learning</a></li>
<li class="chapter" data-level="2.2" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#data-science"><i class="fa fa-check"></i><b>2.2</b> Data Science</a></li>
<li class="chapter" data-level="2.3" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#some-references"><i class="fa fa-check"></i><b>2.3</b> Some References</a></li>
<li class="chapter" data-level="2.4" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#data-mining-and-data-science-with-r"><i class="fa fa-check"></i><b>2.4</b> Data Mining and Data Science with R</a></li>
<li class="chapter" data-level="2.5" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#data-mining-with-weka"><i class="fa fa-check"></i><b>2.5</b> Data Mining with Weka</a></li>
<li class="chapter" data-level="2.6" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#r-markdown"><i class="fa fa-check"></i><b>2.6</b> R Markdown</a></li>
<li class="chapter" data-level="2.7" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#including-plots"><i class="fa fa-check"></i><b>2.7</b> Including Plots</a></li>
<li class="chapter" data-level="2.8" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#references"><i class="fa fa-check"></i><b>2.8</b> References</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#prediction-in-probabilistic-classifiers"><i class="fa fa-check"></i><b>2.8.1</b> Prediction in probabilistic classifiers</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#other-metrics-used-in-software-engineering-with-classification"><i class="fa fa-check"></i><b>2.9</b> Other Metrics used in Software Engineering with Classification</a></li>
<li class="chapter" data-level="2.10" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#graphical-evaluation"><i class="fa fa-check"></i><b>2.10</b> Graphical Evaluation</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#receiver-operating-characteristic-roc"><i class="fa fa-check"></i><b>2.10.1</b> Receiver Operating Characteristic (ROC)</a></li>
<li class="chapter" data-level="2.10.2" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#precision-recall-curve-prc"><i class="fa fa-check"></i><b>2.10.2</b> Precision-Recall Curve (PRC)</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#numeric-prediction-evaluation"><i class="fa fa-check"></i><b>2.11</b> Numeric Prediction Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="evaluationSE.html"><a href="evaluationSE.html"><i class="fa fa-check"></i><b>3</b> Measures of Evaluation in Software Engineering</a>
<ul>
<li class="chapter" data-level="3.1" data-path="evaluationSE.html"><a href="evaluationSE.html#effort-estimation-evaluation-metrics"><i class="fa fa-check"></i><b>3.1</b> Effort estimation evaluation metrics</a></li>
<li class="chapter" data-level="3.2" data-path="evaluationSE.html"><a href="evaluationSE.html#evaluation-of-the-model-in-the-testing-data"><i class="fa fa-check"></i><b>3.2</b> Evaluation of the Model in the Testing data</a></li>
<li class="chapter" data-level="3.3" data-path="evaluationSE.html"><a href="evaluationSE.html#building-a-linear-model-on-the-telecom1-dataset"><i class="fa fa-check"></i><b>3.3</b> Building a Linear Model on the Telecom1 dataset</a></li>
<li class="chapter" data-level="3.4" data-path="evaluationSE.html"><a href="evaluationSE.html#building-a-linear-model-on-the-telecom1-dataset-with-all-observations"><i class="fa fa-check"></i><b>3.4</b> Building a Linear Model on the Telecom1 dataset with all observations</a></li>
<li class="chapter" data-level="3.5" data-path="evaluationSE.html"><a href="evaluationSE.html#standardised-accuracy-examples"><i class="fa fa-check"></i><b>3.5</b> Standardised Accuracy Examples</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="evaluationSE.html"><a href="evaluationSE.html#standardised-accuracy-marp0-using-the-china-test-dataset"><i class="fa fa-check"></i><b>3.5.1</b> Standardised Accuracy MARP0 using the China Test dataset</a></li>
<li class="chapter" data-level="3.5.2" data-path="evaluationSE.html"><a href="evaluationSE.html#standardised-accuracy.-marp0-using-the-telecom1-dataset"><i class="fa fa-check"></i><b>3.5.2</b> Standardised Accuracy. MARP0 using the Telecom1 dataset</a></li>
<li class="chapter" data-level="3.5.3" data-path="evaluationSE.html"><a href="evaluationSE.html#standard-accuracy-marp0-using-the-atkinson-dataset"><i class="fa fa-check"></i><b>3.5.3</b> Standard Accuracy MARP0 using the Atkinson Dataset</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="evaluationSE.html"><a href="evaluationSE.html#exact-marp0"><i class="fa fa-check"></i><b>3.6</b> Exact MARP0</a></li>
<li class="chapter" data-level="3.7" data-path="evaluationSE.html"><a href="evaluationSE.html#computing-the-bootstraped-confidence-interval-of-the-mean-for-the-test-observations-of-the-china-dataset"><i class="fa fa-check"></i><b>3.7</b> Computing the bootstraped confidence interval of the mean for the Test observations of the China dataset:</a></li>
<li class="chapter" data-level="3.8" data-path="evaluationSE.html"><a href="evaluationSE.html#defect-prediction-evaluation-metrics"><i class="fa fa-check"></i><b>3.8</b> Defect prediction evaluation metrics</a></li>
</ul></li>
<li class="part"><span><b>III Advanced Topics</b></span></li>
<li class="chapter" data-level="4" data-path="feature-selection.html"><a href="feature-selection.html"><i class="fa fa-check"></i><b>4</b> Feature Selection</a>
<ul>
<li class="chapter" data-level="4.1" data-path="feature-selection.html"><a href="feature-selection.html#instance-selection"><i class="fa fa-check"></i><b>4.1</b> Instance Selection</a></li>
<li class="chapter" data-level="4.2" data-path="feature-selection.html"><a href="feature-selection.html#missing-data-imputation"><i class="fa fa-check"></i><b>4.2</b> Missing Data Imputation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="feature-selection-example.html"><a href="feature-selection-example.html"><i class="fa fa-check"></i><b>5</b> Feature Selection Example</a></li>
<li class="chapter" data-level="6" data-path="advanced-models.html"><a href="advanced-models.html"><i class="fa fa-check"></i><b>6</b> Advanced Models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="advanced-models.html"><a href="advanced-models.html#genetic-programming-for-symbolic-regression"><i class="fa fa-check"></i><b>6.1</b> Genetic Programming for Symbolic Regression</a></li>
<li class="chapter" data-level="6.2" data-path="advanced-models.html"><a href="advanced-models.html#genetic-programming-example"><i class="fa fa-check"></i><b>6.2</b> Genetic Programming Example</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="advanced-models.html"><a href="advanced-models.html#load-data"><i class="fa fa-check"></i><b>6.2.1</b> Load Data</a></li>
<li class="chapter" data-level="6.2.2" data-path="advanced-models.html"><a href="advanced-models.html#genetic-programming-for-symbolic-regression-china-dataset."><i class="fa fa-check"></i><b>6.2.2</b> Genetic Programming for Symbolic Regression: China dataset.</a></li>
<li class="chapter" data-level="6.2.3" data-path="advanced-models.html"><a href="advanced-models.html#genetic-programming-for-symbolic-regression.-telecom1-dataset."><i class="fa fa-check"></i><b>6.2.3</b> Genetic Programming for Symbolic Regression. Telecom1 dataset.</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="advanced-models.html"><a href="advanced-models.html#neural-networks"><i class="fa fa-check"></i><b>6.3</b> Neural Networks</a></li>
<li class="chapter" data-level="6.4" data-path="advanced-models.html"><a href="advanced-models.html#support-vector-machines"><i class="fa fa-check"></i><b>6.4</b> Support Vector Machines</a></li>
<li class="chapter" data-level="6.5" data-path="advanced-models.html"><a href="advanced-models.html#ensembles"><i class="fa fa-check"></i><b>6.5</b> Ensembles</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="advanced-models.html"><a href="advanced-models.html#bagging"><i class="fa fa-check"></i><b>6.5.1</b> Bagging</a></li>
<li class="chapter" data-level="6.5.2" data-path="advanced-models.html"><a href="advanced-models.html#boosting"><i class="fa fa-check"></i><b>6.5.2</b> Boosting</a></li>
<li class="chapter" data-level="6.5.3" data-path="advanced-models.html"><a href="advanced-models.html#rotation-forests"><i class="fa fa-check"></i><b>6.5.3</b> Rotation Forests</a></li>
<li class="chapter" data-level="6.5.4" data-path="advanced-models.html"><a href="advanced-models.html#boosting-in-r"><i class="fa fa-check"></i><b>6.5.4</b> Boosting in R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="further-classification-models.html"><a href="further-classification-models.html"><i class="fa fa-check"></i><b>7</b> Further Classification Models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="further-classification-models.html"><a href="further-classification-models.html#multilabel-classification"><i class="fa fa-check"></i><b>7.1</b> Multilabel classification</a></li>
<li class="chapter" data-level="7.2" data-path="further-classification-models.html"><a href="further-classification-models.html#semi-supervised-learning"><i class="fa fa-check"></i><b>7.2</b> Semi-supervised Learning</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="social-network-analysis-in-se.html"><a href="social-network-analysis-in-se.html"><i class="fa fa-check"></i><b>8</b> Social Network Analysis in SE</a></li>
<li class="chapter" data-level="9" data-path="text-mining-software-engineering-data.html"><a href="text-mining-software-engineering-data.html"><i class="fa fa-check"></i><b>9</b> Text Mining Software Engineering Data</a>
<ul>
<li class="chapter" data-level="9.1" data-path="text-mining-software-engineering-data.html"><a href="text-mining-software-engineering-data.html#terminology"><i class="fa fa-check"></i><b>9.1</b> Terminology</a></li>
<li class="chapter" data-level="9.2" data-path="text-mining-software-engineering-data.html"><a href="text-mining-software-engineering-data.html#example-of-classifying-bugs-from-bugzilla"><i class="fa fa-check"></i><b>9.2</b> Example of classifying bugs from Bugzilla</a></li>
<li class="chapter" data-level="9.3" data-path="text-mining-software-engineering-data.html"><a href="text-mining-software-engineering-data.html#extracting-data-from-twitter"><i class="fa fa-check"></i><b>9.3</b> Extracting data from Twitter</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>10</b> Time Series</a>
<ul>
<li class="chapter" data-level="10.1" data-path="time-series.html"><a href="time-series.html#web-tutorials-about-time-series"><i class="fa fa-check"></i><b>10.1</b> Web tutorials about Time Series:</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis in Software Engineering using R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="what-is-data-mining-knowledge-discovery-in-databases-kdd" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> What is Data Mining / Knowledge Discovery in Databases (KDD)</h1>
<p>The non-trivial process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data <span class="citation">(<a href="#ref-FayyadPS1996" role="doc-biblioref">Fayyad, Piatetsky-Shapiro, and Smyth 1996</a>)</span></p>
<div class="figure">
<img src="figures/Fayyad96kdd-process.png" alt="" />
<p class="caption">KDD Process</p>
</div>
<p>The Cross Industry Process for Data Mining (CRISP-DM) also provides a common and well-developed framework for delivering data mining projects identifying six steps <span class="citation">(<a href="#ref-shearer00crisp" role="doc-biblioref">Shearer 2000</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li>Problem Understanding</li>
<li>Data Understanding</li>
<li>Data Preparation</li>
<li>Modeling</li>
<li>Evaluation</li>
<li>Deployment</li>
</ol>
<div class="figure">
<img src="figures/CRISP-DM_Process_Diagram.png" alt="" />
<p class="caption">CRISP-DM (Wikipedia)</p>
</div>
<div id="the-aim-of-data-analysis-and-statistical-learning" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> The Aim of Data Analysis and Statistical Learning</h2>
<ul>
<li>The aim of any data analysis is to <strong>understand the data</strong></li>
<li>and to build models for making predictions and estimating future events based on past data</li>
<li>and to make statistical inferences from our data.</li>
<li>We may want to test different hypothesis on the data</li>
<li>We want to generate conclusions about the population where our sample data comes from</li>
<li>Most probably we are interested in building a model for quality, time, defects or effort prediction</li>
</ul>
<p><img src="figures/prediction.png" /></p>
<ul>
<li>We want to find a function <span class="math inline">\(f()\)</span>, that given <span class="math inline">\(X1, X2, ...\)</span> computes <span class="math inline">\(Y=f(X1, X2, ..., Xn)\)</span></li>
</ul>
</div>
<div id="data-science" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Data Science</h2>
<p>Data science (DS) is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structured and unstructured data. Data science is related to data mining, machine learning and big data.</p>
<p>We may say that the term DS embraces all terms related to data analysis that previously were under different disciplines.</p>
<div class="figure">
<img src="figures/Data_science.png" alt="" />
<p class="caption">Wikipedia Data Science</p>
</div>
</div>
<div id="some-references" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Some References</h2>
<ul>
<li><a href="https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf">W.N. Venables, D.M. Smith and the R Core Team, An Introduction to R</a></li>
</ul>
<p>Generic books about statistics:</p>
<ul>
<li><p><a href="https://cran.r-project.org/doc/contrib/Verzani-SimpleR.pdf">John Verzani, <em>simpleR - Using R for Introductory Statistics</em></a></p></li>
<li><p><a href="https://www.springer.com/gp/book/9780387790534">Peter Dalgaard, <em>Introductory Statistics with R</em>, 2nd Edt., Springer, 2008</a></p></li>
<li><p><a href="http://www.springer.com/it/book/9781461471370">Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, <em>An Introduction to Statistical Learning with Applications in R</em>, Springer, 2013</a></p></li>
<li><p><a href="https://www.routledge.com/products/9780415879682">Geoff Cumming, <em>Understanding the New Statistics: Effect Sizes, Confidence Intervals, and Meta-Analysis</em>, Routledge, New York, 2012</a></p></li>
</ul>
</div>
<div id="data-mining-and-data-science-with-r" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Data Mining and Data Science with R</h2>
<ul>
<li><p><a href="https://r4ds.had.co.nz/">R for Data Science</a></p></li>
<li><p><a href="https://www.manning.com/books/practical-data-science-with-r-second-edition">Practical Data Science with R</a>
*<a href="https://www.jaredlander.com/r-for-everyone/">R for Everyone: Advanced Analytics and Graphics</a></p>
<ul>
<li><p><a href="http://www.springer.com/gp/book/9781441998897">Graham Williams, <em>Data Mining with Rattle and R: The Art of Excavating Data for Knowledge Discovery</em>, Springer 2011</a></p>
<p>Also the author maintains a Web site:
<a href="http://rattle.togaware.com/">http://rattle.togaware.com/</a></p></li>
<li><p><a href="https://www.crcpress.com/Data-Mining-with-R-Learning-with-Case-Studies/Torgo/9781439810187">Luis Torgo, <em>Data Mining with R: Learning with Case Studies</em>, Chapman and Hall/CRC, 2010</a></p></li>
<li><p><a href="http://www.rdatamining.com/">http://www.rdatamining.com/</a></p></li>
</ul></li>
</ul>
</div>
<div id="data-mining-with-weka" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Data Mining with Weka</h2>
<p>Weka is another popular framework written in Java that can be used and extended with other languages and frameworks. The authors of Weka also have a popular book:</p>
<ul>
<li>Ian Witten, Eibe Frank, Mark Hall, Christopher J. Pal, Data Mining: Practical Machine Learning Tools and Techniques (4th Edt), Morgan Kaufmann, 2016, ISBN: 978-0128042915</li>
</ul>

</div>
<div id="r-markdown" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> R Markdown</h2>
<p>Cheatsheet link for the markdown documents
<a href="https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet" class="uri">https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet</a></p>
<p>This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <a href="http://rmarkdown.rstudio.com" class="uri">http://rmarkdown.rstudio.com</a>.</p>
<p>When you click the <strong>Knit</strong> button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:</p>
<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb394-1"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#cb394-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(cars)</span></code></pre></div>
<pre><code>##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00</code></pre>
</div>
<div id="including-plots" class="section level2" number="2.7">
<h2><span class="header-section-number">2.7</span> Including Plots</h2>
<p>You can also embed plots, for example:</p>
<p><img src="DASE_files/figure-html/pressure-1.png" width="672" /></p>
<p>Note that the <code>echo = FALSE</code> parameter was added to the code chunk to prevent printing of the R code that generated the plot.</p>
</div>
<div id="references" class="section level2" number="2.8">
<h2><span class="header-section-number">2.8</span> References</h2>
<p><a href="https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet" class="uri">https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet</a>
<a href="https://rmarkdown.rstudio.com/lesson-15.html" class="uri">https://rmarkdown.rstudio.com/lesson-15.html</a></p>

<table style="width:6%;">
<colgroup>
<col width="5%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">output: html_document
pdf_document: default</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">## R and Python</td>
</tr>
<tr class="even">
<td align="right">R and Python can interact together via the <em>reticulate</em> package.</td>
</tr>
<tr class="odd">
<td align="right">The documentation for the <code>reticulate</code> package can be found here:
<a href="https://rstudio.github.io/reticulate/">https://rstudio.github.io/reticulate/</a></td>
</tr>
<tr class="even">
<td align="right"><img src="figures/reticulated_python.png" alt="reticulate" /></td>
</tr>
<tr class="odd">
<td align="right">Instructions for configuring the system can be found at RStudio site: <a href="https://support.rstudio.com/hc/en-us/articles/360023654474-Installing-and-Configuring-Python-with-RStudio" class="uri">https://support.rstudio.com/hc/en-us/articles/360023654474-Installing-and-Configuring-Python-with-RStudio</a>.</td>
</tr>
<tr class="even">
<td align="right">Or we can create our environment following - [R and Python – a happy union with reticulate webinar]<a href="https://www.youtube.com/watch?v=8WE-EU5k97Q&amp;t=27s" class="uri">https://www.youtube.com/watch?v=8WE-EU5k97Q&amp;t=27s</a></td>
</tr>
<tr class="odd">
<td align="right"><code>r install.packages("reticulate")</code></td>
</tr>
<tr class="even">
<td align="right">Note that the <code>reticulate</code> package needs Python &gt;= 2.7 and for <code>NumPy</code> requires NumPy &gt;= 1.6.</td>
</tr>
<tr class="odd">
<td align="right">## Using Python with RMarkdown and RStudio</td>
</tr>
<tr class="even">
<td align="right">The R build-in dataset will be used later</td>
</tr>
<tr class="odd">
<td align="right"><code>r library("reticulate") # use_virtualenv("myenv") data("mtcars")</code></td>
</tr>
<tr class="even">
<td align="right"><code>python print("Hello Python!")</code></td>
</tr>
<tr class="odd">
<td align="right"><code>## Hello Python!</code></td>
</tr>
<tr class="even">
<td align="right">```python
datactrs = {
‘CHN’: {‘COUNTRY’: ‘China,’ ‘POP’: 1_398.72, ‘AREA’: 9_596.96,
‘GDP’: 12_234.78, ‘CONT’: ‘Asia’},
‘IND’: {‘COUNTRY’: ‘India,’ ‘POP’: 1_351.16, ‘AREA’: 3_287.26,
‘GDP’: 2_575.67, ‘CONT’: ‘Asia,’ ‘IND_DAY’: ‘1947-08-15’},
‘USA’: {‘COUNTRY’: ‘US,’ ‘POP’: 329.74, ‘AREA’: 9_833.52,
‘GDP’: 19_485.39, ‘CONT’: ‘N.America,’
‘IND_DAY’: ‘1776-07-04’},
‘IDN’: {‘COUNTRY’: ‘Indonesia,’ ‘POP’: 268.07, ‘AREA’: 1_910.93,
‘GDP’: 1_015.54, ‘CONT’: ‘Asia,’ ‘IND_DAY’: ‘1945-08-17’},
‘BRA’: {‘COUNTRY’: ‘Brazil,’ ‘POP’: 210.32, ‘AREA’: 8_515.77,
‘GDP’: 2_055.51, ‘CONT’: ‘S.America,’ ‘IND_DAY’: ‘1822-09-07’},
‘PAK’: {‘COUNTRY’: ‘Pakistan,’ ‘POP’: 205.71, ‘AREA’: 881.91,
‘GDP’: 302.14, ‘CONT’: ‘Asia,’ ‘IND_DAY’: ‘1947-08-14’},
‘NGA’: {‘COUNTRY’: ‘Nigeria,’ ‘POP’: 200.96, ‘AREA’: 923.77,
‘GDP’: 375.77, ‘CONT’: ‘Africa,’ ‘IND_DAY’: ‘1960-10-01’},
‘BGD’: {‘COUNTRY’: ‘Bangladesh,’ ‘POP’: 167.09, ‘AREA’: 147.57,
‘GDP’: 245.63, ‘CONT’: ‘Asia,’ ‘IND_DAY’: ‘1971-03-26’},
‘RUS’: {‘COUNTRY’: ‘Russia,’ ‘POP’: 146.79, ‘AREA’: 17_098.25,
‘GDP’: 1_530.75, ‘IND_DAY’: ‘1992-06-12’},
‘MEX’: {‘COUNTRY’: ‘Mexico,’ ‘POP’: 126.58, ‘AREA’: 1_964.38,
‘GDP’: 1_158.23, ‘CONT’: ‘N.America,’ ‘IND_DAY’: ‘1810-09-16’},
‘JPN’: {‘COUNTRY’: ‘Japan,’ ‘POP’: 126.22, ‘AREA’: 377.97,
‘GDP’: 4_872.42, ‘CONT’: ‘Asia’},
‘DEU’: {‘COUNTRY’: ‘Germany,’ ‘POP’: 83.02, ‘AREA’: 357.11,
‘GDP’: 3_693.20, ‘CONT’: ‘Europe’},
‘FRA’: {‘COUNTRY’: ‘France,’ ‘POP’: 67.02, ‘AREA’: 640.68,
‘GDP’: 2_582.49, ‘CONT’: ‘Europe,’ ‘IND_DAY’: ‘1789-07-14’},
‘GBR’: {‘COUNTRY’: ‘UK,’ ‘POP’: 66.44, ‘AREA’: 242.50,
‘GDP’: 2_631.23, ‘CONT’: ‘Europe’},
‘ITA’: {‘COUNTRY’: ‘Italy,’ ‘POP’: 60.36, ‘AREA’: 301.34,
‘GDP’: 1_943.84, ‘CONT’: ‘Europe’},
‘ARG’: {‘COUNTRY’: ‘Argentina,’ ‘POP’: 44.94, ‘AREA’: 2_780.40,
‘GDP’: 637.49, ‘CONT’: ‘S.America,’ ‘IND_DAY’: ‘1816-07-09’},
‘DZA’: {‘COUNTRY’: ‘Algeria,’ ‘POP’: 43.38, ‘AREA’: 2_381.74,
‘GDP’: 167.56, ‘CONT’: ‘Africa,’ ‘IND_DAY’: ‘1962-07-05’},
‘CAN’: {‘COUNTRY’: ‘Canada,’ ‘POP’: 37.59, ‘AREA’: 9_984.67,
‘GDP’: 1_647.12, ‘CONT’: ‘N.America,’ ‘IND_DAY’: ‘1867-07-01’},
‘AUS’: {‘COUNTRY’: ‘Australia,’ ‘POP’: 25.47, ‘AREA’: 7_692.02,
‘GDP’: 1_408.68, ‘CONT’: ‘Oceania’},
‘KAZ’: {‘COUNTRY’: ‘Kazakhstan,’ ‘POP’: 18.53, ‘AREA’: 2_724.90,
‘GDP’: 159.41, ‘CONT’: ‘Asia,’ ‘IND_DAY’: ‘1991-12-16’}
}</td>
</tr>
<tr class="odd">
<td align="right">columns = (‘COUNTRY,’ ‘POP,’ ‘AREA,’ ‘GDP,’ ‘CONT,’ ‘IND_DAY’)
```</td>
</tr>
<tr class="even">
<td align="right">```python
import pandas as pd
import seaborn as sns #ubuntu #sudo apt-get install -y python3-seaborn
import matplotlib.pyplot as plt</td>
</tr>
<tr class="odd">
<td align="right">tips = sns.load_dataset(“tips”)
mylist = [“youtube,” ‘linkedin,’ ‘1littlecoder’]
sns.scatterplot(x=tips[‘total_bill’], y = tips[‘tip’], hue=tips[‘day’])
plt.show()
```</td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/impor_tips_fmri-1.png" width="672" /></td>
</tr>
<tr class="odd">
<td align="right"><code>python fmri = sns.load_dataset("fmri")</code></td>
</tr>
<tr class="even">
<td align="right"><code>r f1 &lt;- subset(py$fmri, region == "parietal")</code></td>
</tr>
<tr class="odd">
<td align="right"><code>python import matplotlib as mpl sns.lmplot("timepoint","signal", data=r.f1)</code></td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/accesing_r-1.png" width="245" /></td>
</tr>
<tr class="odd">
<td align="right"><code>python mpl.pyplot.show()</code></td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/accesing_r-2.png" width="480" /></td>
</tr>
<tr class="odd">
<td align="right"><code>python sns.lmplot("mpg", "cyl", data=r.mtcars)</code></td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/accesing_r-3.png" width="244" /></td>
</tr>
<tr class="odd">
<td align="right"><code>python mpl.pyplot.show()</code></td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/accesing_r-4.png" width="480" /></td>
</tr>
<tr class="odd">
<td align="right"><code>python import pandas as pd df = pd.DataFrame(data=datactrs, index=columns).T df</code></td>
</tr>
<tr class="even">
<td align="right"><code>##         COUNTRY      POP      AREA       GDP       CONT     IND_DAY ## CHN       China  1398.72   9596.96  12234.78       Asia         NaN ## IND       India  1351.16   3287.26   2575.67       Asia  1947-08-15 ## USA          US   329.74   9833.52  19485.39  N.America  1776-07-04 ## IDN   Indonesia   268.07   1910.93   1015.54       Asia  1945-08-17 ## BRA      Brazil   210.32   8515.77   2055.51  S.America  1822-09-07 ## PAK    Pakistan   205.71    881.91    302.14       Asia  1947-08-14 ## NGA     Nigeria   200.96    923.77    375.77     Africa  1960-10-01 ## BGD  Bangladesh   167.09    147.57    245.63       Asia  1971-03-26 ## RUS      Russia   146.79  17098.25   1530.75        NaN  1992-06-12 ## MEX      Mexico   126.58   1964.38   1158.23  N.America  1810-09-16 ## JPN       Japan   126.22    377.97   4872.42       Asia         NaN ## DEU     Germany    83.02    357.11    3693.2     Europe         NaN ## FRA      France    67.02    640.68   2582.49     Europe  1789-07-14 ## GBR          UK    66.44     242.5   2631.23     Europe         NaN ## ITA       Italy    60.36    301.34   1943.84     Europe         NaN ## ARG   Argentina    44.94    2780.4    637.49  S.America  1816-07-09 ## DZA     Algeria    43.38   2381.74    167.56     Africa  1962-07-05 ## CAN      Canada    37.59   9984.67   1647.12  N.America  1867-07-01 ## AUS   Australia    25.47   7692.02   1408.68    Oceania         NaN ## KAZ  Kazakhstan    18.53    2724.9    159.41       Asia  1991-12-16</code></td>
</tr>
<tr class="odd">
<td align="right"><code>python df.to_csv('datasets/data_countries.csv')</code>
We can read the dataset from python</td>
</tr>
<tr class="even">
<td align="right"><code>python df1 = pd.read_csv("datasets/other/data_countries.csv", index_col=0)</code></td>
</tr>
<tr class="odd">
<td align="right">Use R to read and write data from a package</td>
</tr>
<tr class="even">
<td align="right"><code>r library("nycflights13") write.csv(flights, "datasets/other/flights.csv")</code></td>
</tr>
<tr class="odd">
<td align="right">Use python to read the dataset and process the data</td>
</tr>
<tr class="even">
<td align="right"><code>python import pandas flights = pandas.read_csv("datasets/other/flights.csv") flights = flights[flights['dest'] =="ORD"] flights = flights[['carrier', 'dep_delay', 'arr_delay']] flights = flights.dropna() print(flights.head())</code></td>
</tr>
<tr class="odd">
<td align="right"><code>##    carrier  dep_delay  arr_delay ## 5       UA       -4.0       12.0 ## 9       AA       -2.0        8.0 ## 25      MQ        8.0       32.0 ## 38      AA       -1.0       14.0 ## 57      AA       -4.0        4.0</code>
Use Python for plotting</td>
</tr>
<tr class="even">
<td align="right"><code>python import matplotlib.pyplot as plt import numpy as np t = np.arange(0.0, 2.0, 0.01) s = 1 + np.sin(2*np.pi*t) plt.plot(t,s) plt.xlabel('time (s)') plt.ylabel('voltage (mV)') plt.grid(True) plt.savefig("test.png") plt.show()</code></td>
</tr>
<tr class="odd">
<td align="right"><img src="DASE_files/figure-html/matplotlib-1.png" width="480" /></td>
</tr>
<tr class="even">
<td align="right">Use R for plotting Python objects:
<img src="DASE_files/figure-html/unnamed-chunk-28-3.png" width="672" /></td>
</tr>
<tr class="odd">
<td align="right">Note that the <code>echo = FALSE</code> parameter was added to the code chunk to prevent printing of the R code that generated the plot.</td>
</tr>
<tr class="even">
<td align="right">## References</td>
</tr>
<tr class="odd">
<td align="right">- <a href="https://info.rstudio.com/WN07MCXqS1A2NYS0040LW00">https://rstudio.com/resources/webinars/rstudio-a-single-home-for-r-and-python/</a></td>
</tr>
<tr class="even">
<td align="right">- <a href="https://rstudio.github.io/reticulate/">R interface to Python</a></td>
</tr>
<tr class="odd">
<td align="right">- <a href="https://blog.rstudio.com/2020/07/28/practical-interoperability/">3 Wild-Caught R and Python Applications</a></td>
</tr>
<tr class="even">
<td align="right">- <a href="https://www.r-bloggers.com/2021/01/rstudio-python-visual-markdown-editor-rstudio-latest-update/">RStudio + Python, Visual Markdown Editor – RStudio Latest Update</a></td>
</tr>
<tr class="odd">
<td align="right">- [Arrays in R and in Python]<a href="https://rstudio.github.io/reticulate/articles/arrays.html" class="uri">https://rstudio.github.io/reticulate/articles/arrays.html</a></td>
</tr>
<tr class="even">
<td align="right">- <a href="https://www.r-bloggers.com/2021/02/pythons-pandas-vs-rs-dplyr-which-is-the-best-data-analysis-library/?utm_source=feedburner&amp;utm_medium=email&amp;utm_campaign=Feed%3A+RBloggers+%28R+bloggers%29" class="uri">https://www.r-bloggers.com/2021/02/pythons-pandas-vs-rs-dplyr-which-is-the-best-data-analysis-library/?utm_source=feedburner&amp;utm_medium=email&amp;utm_campaign=Feed%3A+RBloggers+%28R+bloggers%29</a></td>
</tr>
<tr class="odd">
<td align="right"><!--chapter:end:102_rnpython.Rmd--></td>
</tr>
<tr class="even">
<td align="right"># (PART) Data Sources and Metrics and Standards in Software Engineering Defect Prediction {-}</td>
</tr>
<tr class="odd">
<td align="right"># Data Sources in Software Engineering</td>
</tr>
<tr class="even">
<td align="right">We classify this trail in the following categories:</td>
</tr>
<tr class="odd">
<td align="right">* <em>Source code</em> can be studied to measure its properties, such as size or complexity.</td>
</tr>
<tr class="even">
<td align="right">* <em>Source Code Management Systems</em> (SCM) make it possible to store all the changes that the different source code files undergo during the project. Also, SCM systems allow for work to be done in parallel by different developers over the same source code tree. Every change recorded in the system is accompanied with meta-information (author, date, reason for the change, etc) that can be used for research purposes.</td>
</tr>
<tr class="odd">
<td align="right">* <em>Issue or Bug tracking systems</em> (ITS). Bugs, defects and user requests are managed in ISTs, where users and developers can fill tickets with a description of a defect found, or a desired new functionality. All the changes to the ticket are recorded in the system, and most of the systems also record the comments and communications among all the users and developers implied in the task.</td>
</tr>
<tr class="even">
<td align="right">* <em>Messages</em> between developers and users. In the case of free/open source software, the projects are open to the world, and the messages are archived in the form of mailing lists and social networks which can also be mined for research purposes. There are also some other open message systems, such as IRC or forums.</td>
</tr>
<tr class="odd">
<td align="right">* <em>Meta-data about the projects</em>. As well as the low level information of the software processes, we can also find meta-data about the software projects which can be useful for research. This meta-data may include intended-audience, programming language, domain of application, license (in the case of open source), etc.</td>
</tr>
<tr class="even">
<td align="right"><img src="figures/artifactsMeta.png" alt="Metadata (source: Israel Herraiz)" /></td>
</tr>
<tr class="odd">
<td align="right">* <em>Usage data</em>. There are statistics about software downloads, logs from servers, software reviews, etc.</td>
</tr>
<tr class="even">
<td align="right">Types of information stored in the repositories:</td>
</tr>
<tr class="odd">
<td align="right">* Meta-information about the project itself and the
people that participated.</td>
</tr>
<tr class="even">
<td align="right">+ Low-level information</td>
</tr>
<tr class="odd">
<td align="right">* Mailing Lists (ML)</td>
</tr>
<tr class="even">
<td align="right">* Bug Tracking Systems (BTS) or Project Tracker System (PTS)</td>
</tr>
<tr class="odd">
<td align="right">* Software Configuration Management Systems (SCM)</td>
</tr>
<tr class="even">
<td align="right">+ Processed information. For example project management information about the effort estimation and cost of the project.</td>
</tr>
<tr class="odd">
<td align="right">* Whether the repository is public or not</td>
</tr>
<tr class="even">
<td align="right">* Single project vs. multiprojects. Whether the repository contains information of a single project with multiples versions or multiples projects and/or versions.</td>
</tr>
<tr class="odd">
<td align="right">* Type of content, open source or industrial projects</td>
</tr>
<tr class="even">
<td align="right">* Format in which the information is stored and formats or technologies for accessing the information:</td>
</tr>
<tr class="odd">
<td align="right">+ Text. It can be just plain text, CSV (Comma Separated Values) files, Attribute-Relation File Format
(ARFF) or its variants</td>
</tr>
<tr class="even">
<td align="right">+ Through databases. Downloading dumps of the database.</td>
</tr>
<tr class="odd">
<td align="right">+ Remote access such as APIs of Web services or REST</td>
</tr>
<tr class="even">
<td align="right"># Repositories</td>
</tr>
<tr class="odd">
<td align="right">There is a number of open research repositories in Software Engineering. Among them:</td>
</tr>
<tr class="even">
<td align="right">+ Zenodo. It is becoming a popular site for publishing datasets associated with papers. It provides DOIs for referencing data and code:
<a href="https://zenodo.org/">https://zenodo.org/</a></td>
</tr>
<tr class="odd">
<td align="right">+ Spinellis maintais a curated repository on Github:
<a href="https://github.com/dspinellis/awesome-msr">https://github.com/dspinellis/awesome-msr</a></td>
</tr>
<tr class="even">
<td align="right">+ PROMISE (PRedictOr Models In Software Engineering). There is a conference with this name (<a href="https://promiseconf.github.io/">Promise Conference</a>)</td>
</tr>
<tr class="odd">
<td align="right">Some popular datasets used as benchmarking in may paper can still be found on:
<a href="http://promise.site.uottawa.ca/SERepository/datasets-page.html">http://promise.site.uottawa.ca/SERepository/datasets-page.html</a>
The is some well-known issues wit the NASA datasets and the source code is not available.</td>
</tr>
<tr class="even">
<td align="right"><!--  + Finding Faults using Ensemble Learners (ELFF) [@Shippey2016Esem]
[http://www.elff.org.uk/](http://www.elff.org.uk/)--></td>
</tr>
<tr class="odd">
<td align="right">+ FLOSSMole <span class="citation">(<a href="#ref-HCC06" role="doc-biblioref">Howison, Conklin, and Crowston 2006</a>)</span>
<a href="http://flossmole.org/">http://flossmole.org/</a></td>
</tr>
<tr class="even">
<td align="right">+ FLOSSMetrics <span class="citation">(<a href="#ref-herraiz2009flossmetrics" role="doc-biblioref">Herraiz et al. 2009</a>)</span>:
<a href="http://flossmetrics.org/">http://flossmetrics.org/</a></td>
</tr>
<tr class="odd">
<td align="right">+ Qualitas Corpus (QC) <span class="citation">(<a href="#ref-QualitasCorpus2010" role="doc-biblioref">Tempero et al. 2010</a>)</span>:
<a href="http://qualitascorpus.com/">http://qualitascorpus.com/</a></td>
</tr>
<tr class="even">
<td align="right">+ Sourcerer Project <span class="citation">(<a href="#ref-LBNRB09" role="doc-biblioref">Linstead et al. 2009</a>)</span>:
<a href="http://sourcerer.ics.uci.edu/">http://sourcerer.ics.uci.edu/</a></td>
</tr>
<tr class="odd">
<td align="right">+ Ultimate Debian Database (UDD) <span class="citation">(<a href="#ref-NZ10" role="doc-biblioref">Nussbaum and Zacchiroli 2010</a>)</span>
<a href="http://udd.debian.org/">http://udd.debian.org/</a></td>
</tr>
<tr class="even">
<td align="right">+ SourceForge Research Data Archive (SRDA) <span class="citation">(<a href="#ref-VanAntwerpM2008" role="doc-biblioref">Van Antwerp and Madey 2008</a>)</span>
<a href="http://zerlot.cse.nd.edu/">http://zerlot.cse.nd.edu/</a></td>
</tr>
<tr class="odd">
<td align="right"><!--  + SECOLD (Source code ECOsystem Linked Data):
[http://www.secold.org/](http://www.secold.org/)
--></td>
</tr>
<tr class="even">
<td align="right">+ Software-artifact Infrastructure Repository (SIR)
[<a href="http://sir.unl.edu" class="uri">http://sir.unl.edu</a>]</td>
</tr>
<tr class="odd">
<td align="right">+ OpenHub:
<a href="https://www.openhub.net/">https://www.openhub.net/</a></td>
</tr>
<tr class="even">
<td align="right">Not openly available (and mainly for effort estimation):</td>
</tr>
<tr class="odd">
<td align="right">+ The International Software Benchmarking Standards Group (ISBSG)
<a href="http://www.isbsg.org/">http://www.isbsg.org/</a></td>
</tr>
<tr class="even">
<td align="right"><!--  + TukuTuku
[http://www.metriq.biz/tukutuku/](http://www.metriq.biz/tukutuku/)
--></td>
</tr>
<tr class="odd">
<td align="right">Some papers and publications/theses that have been used in the literature:</td>
</tr>
<tr class="even">
<td align="right">+ Helix Data Set <span class="citation">(<a href="#ref-Vasa2010" role="doc-biblioref">Vasa 2010</a>)</span>:
<a href="http://www.ict.swin.edu.au/research/projects/helix/">http://www.ict.swin.edu.au/research/projects/helix/</a></td>
</tr>
<tr class="odd">
<td align="right">+ Bug Prediction Dataset (BPD) <span class="citation"><a href="#ref-ALR11" role="doc-biblioref">D’Ambros, Lanza, and Robbes</a> (<a href="#ref-ALR11" role="doc-biblioref">2011</a>)</span>:
<a href="http://bug.inf.usi.ch/">http://bug.inf.usi.ch/</a></td>
</tr>
<tr class="even">
<td align="right">+ Eclipse Bug Data (EBD) <span class="citation"><a href="#ref-NZZH12" role="doc-biblioref">Nagappan et al.</a> (<a href="#ref-NZZH12" role="doc-biblioref">2012</a>)</span>:
<a href="http://www.st.cs.uni-saarland.de/softevo/bug-data/eclipse/">http://www.st.cs.uni-saarland.de/softevo/bug-data/eclipse/</a></td>
</tr>
<tr class="odd">
<td align="right"># Open Tools/Dashboards to extract data</td>
</tr>
<tr class="even">
<td align="right">Process to extract data:</td>
</tr>
<tr class="odd">
<td align="right"><img src="figures/process.png" alt="Process" /></td>
</tr>
<tr class="even">
<td align="right">Within the open source community, several toolkits allow us to extract data that can be used to explore projects:</td>
</tr>
<tr class="odd">
<td align="right">Metrics Grimoire
<a href="http://metricsgrimoire.github.io/">http://metricsgrimoire.github.io/</a></td>
</tr>
<tr class="even">
<td align="right"><img src="figures/Grimoire.png" alt="Grimoire" /></td>
</tr>
<tr class="odd">
<td align="right">SonarQube
<a href="http://www.sonarqube.org/">http://www.sonarqube.org/</a></td>
</tr>
<tr class="even">
<td align="right"><img src="figures/sonarQube.png" alt="SonarQube" /></td>
</tr>
<tr class="odd">
<td align="right">CKJM (OO Metrics tool)
<a href="http://gromit.iiar.pwr.wroc.pl/p_inf/ckjm/">http://gromit.iiar.pwr.wroc.pl/p_inf/ckjm/</a></td>
</tr>
<tr class="even">
<td align="right">Collects a large number of object-oriented metrics from code.</td>
</tr>
<tr class="odd">
<td align="right">## Issues</td>
</tr>
<tr class="even">
<td align="right">There are problems such as different tools report different values for the same metric <span class="citation">(<a href="#ref-Lincke2008" role="doc-biblioref">Lincke, Lundberg, and Löwe 2008</a>)</span></td>
</tr>
<tr class="odd">
<td align="right">It is well-know that the NASA datasets have some problems:</td>
</tr>
<tr class="even">
<td align="right">+ <span class="citation">(<a href="#ref-Gray2011" role="doc-biblioref">Gray et al. 2011</a>)</span> The misuse of the NASA metrics data program data sets for automated software defect prediction</td>
</tr>
<tr class="odd">
<td align="right">+ <span class="citation">(<a href="#ref-Shepperd2013" role="doc-biblioref">Shepperd et al. 2013</a>)</span> Data Quality: Some Comments on the NASA Software Defect Datasets</td>
</tr>
<tr class="even">
<td align="right"><!--chapter:end:110_dataSources.Rmd--></td>
</tr>
<tr class="odd">
<td align="right">## Effort Estimation Data in Software Engineering</td>
</tr>
<tr class="even">
<td align="right">It is worth highlighting the case of software effort estimation datasets with their peculiarities. First, most effort estimation datasets used in the literature are scattered through research papers with the exception of a few kept in the PROMISE repository. Mair et al <span class="citation">(<a href="#ref-MairSJ05" role="doc-biblioref">2005</a>)</span> also have analysed available datasets in the field of cost estimation identifying 65 different datasets in 50 papers.</td>
</tr>
<tr class="odd">
<td align="right">Second, their size is very small with the exception of ISBSG repository discussed previously which a small sample is available through PROMISE and the China dataset with 499 instances.</td>
</tr>
<tr class="even">
<td align="right">Third, some can be quite old in a context and time that is not applicable to current development environments. The authors noted that the oldest datasets (COCOMO, Desharnais, Kemerer and Albrecht and Gaffney) tend to be the most studied ones and a subset of the most relevant ones. Also, from the artificial intelligence or data mining point of view effort estimation has been mainly tackled with different types of regression techniques and more recently with techniques which are also typically considered under the umbrella of data mining techniques. However, as the number of examples per dataset is increasing, other machine learning techniques are also being studied (e.g.: Dejaeger et al <span class="citation">(<a href="#ref-Dejaeger_TSE12_EffEst" role="doc-biblioref">2012</a>)</span> report on a comparison of several machine learning techniques to effort estimation with only 5 out the 9 used datasets publicly available). From the data mining point of view, the small number of instances hinders the application of machine learning techniques.</td>
</tr>
<tr class="odd">
<td align="right">However, software effort and cost estimation still remain one of the main challenges in software engineering and have attracted a great deal of interest by many researchers <span class="citation">(<a href="#ref-Jorgensen07" role="doc-biblioref">2007</a>)</span>. For example, there are continuous analyses of whether software development follows economies or diseconomies of scale (see <span class="citation"><a href="#ref-Kitchenham2002" role="doc-biblioref">B. A. Kitchenham</a> (<a href="#ref-Kitchenham2002" role="doc-biblioref">2002</a>)</span>).</td>
</tr>
<tr class="even">
<td align="right">Next Table <a href="#tab:effEstimation">2.1</a> (following Mair et al <span class="citation">(<a href="#ref-MairSJ05" role="doc-biblioref">2005</a>)</span> ) shows the most open cost/effort datasets available in the literature with their main reference.</td>
</tr>
<tr class="odd">
<td align="right">Table: (#tab:effEstimation) Effort Estimation Dataset from articles</td>
</tr>
<tr class="even">
<td align="right">| Reference | Instances | Attributes |
| ———————————-| ————: |————:|
|Abran and Robillard <span class="citation">(<a href="#ref-Abran_TSE96_FP" role="doc-biblioref">1996</a>)</span> | 21 | 31|
|Albrecht-Gaffney <span class="citation">(<a href="#ref-AlbrechtG83" role="doc-biblioref">1983</a>)</span> | 24 | 7 |
|Bailey and Basili <span class="citation">(<a href="#ref-Bailey81" role="doc-biblioref">1981</a>)</span> | 18 | 9 |
|Belady and Lehman <span class="citation">(<a href="#ref-Belady79" role="doc-biblioref">1979</a>)</span> | 33 | |
|Boehm (aka COCOMO Dataset) <span class="citation">(<a href="#ref-Boehm81" role="doc-biblioref">1981</a>)</span> | 63 | 43 |
|China dataset[^1] | 499 | 18 |
|Desharnais <span class="citation">(<a href="#ref-Desharnais88" role="doc-biblioref">1988</a>)</span> | 61 | 10 |
|Dolado <span class="citation">(<a href="#ref-Dolado97" role="doc-biblioref">1997</a>)</span> | 24 | 7 |
|Hastings and Sajeev <span class="citation">(<a href="#ref-Hastings01" role="doc-biblioref">2001</a>)</span> | 8 | 14 |
|Heiat and Heiat <span class="citation">(<a href="#ref-Heiat97" role="doc-biblioref">Heiat and Heiat 1997</a>)</span> | 35 | 4 |
|Jeffery and Stathis <span class="citation">(<a href="#ref-Jeffery_ESE96" role="doc-biblioref">1996</a>)</span> | 17 | 7 |
|Jorgensen <span class="citation">(<a href="#ref-Jorgensen04" role="doc-biblioref">2004</a>)</span> | 47 | 4 |
|Jorgensen et al. <span class="citation">(<a href="#ref-Jorgensen2003" role="doc-biblioref">2003</a>)</span> | 20 | 4 |
|Kemerer <span class="citation">(<a href="#ref-Kemerer87" role="doc-biblioref">1987</a>)</span> | 15 | 5 |
|Kitchenham (Mermaid 2) <span class="citation">(<a href="#ref-Kitchenham2002" role="doc-biblioref">2002</a>)</span> | 30 | 5 |
|Kitchenham et al. (CSC) <span class="citation">(<a href="#ref-Kitchenham02_CSC" role="doc-biblioref">2002</a>)</span> | 145 | 9 |
|Kitchenham and Taylor (ICL) <span class="citation">(<a href="#ref-Kitchenham85" role="doc-biblioref">1985</a>)</span> | 10 | 6 |
|Kitchenham and Taylor (BT System X) <span class="citation">(<a href="#ref-Kitchenham85" role="doc-biblioref">1985</a>)</span> | 10 | 3 |
|Kitchenham and Taylor (BT Software Houses) <span class="citation">(<a href="#ref-Kitchenham85" role="doc-biblioref">1985</a>)</span> | 12 | 6 |
|Li et al.(USP05) <span class="citation">(<a href="#ref-LiRAR07" role="doc-biblioref">2007</a>)</span>[^2] | 202 | 16 |
|Mišić and Tevsić <span class="citation">(<a href="#ref-Misic19981" role="doc-biblioref">1998</a>)</span> | 6 | 16 |
|Maxwell (Dev Effort) <span class="citation">(<a href="#ref-Maxwell02" role="doc-biblioref">2002</a>)</span> | 63 | 32 |
|Maxwell (Maintenance Eff) <span class="citation">(<a href="#ref-Maxwell02" role="doc-biblioref">2002</a>)</span> | 67 | 28 |
|Miyazaki et al. <span class="citation">(<a href="#ref-Miyazaki94" role="doc-biblioref">1994</a>)</span> | 47 | 9 |
|Moser et al. <span class="citation">(<a href="#ref-Moser1999" role="doc-biblioref">1999</a>)</span> | 37 | 4 |
|Shepperd and Cartwright <span class="citation">(<a href="#ref-Shepperd_TSE01" role="doc-biblioref">Shepperd and Cartwright 2001</a>)</span> | 39 | 3 |
|Shepperd and Schofield (Telecom 1) <span class="citation">(<a href="#ref-Shepperd97_Analogy" role="doc-biblioref">1997</a>)</span> | 18 | 5 |
|Schofield (real-time 1) <span class="citation"><a href="#ref-Shepperd97_Analogy" role="doc-biblioref">Shepperd and Schofield</a> (<a href="#ref-Shepperd97_Analogy" role="doc-biblioref">1997</a>)</span> | 21 | 4 |
|Schofield (Mermaid) <span class="citation">(<a href="#ref-Schofield98PhD" role="doc-biblioref">1998</a>)</span> | 30 | 18 |
|Schofield (Finnish) <span class="citation">(<a href="#ref-Schofield98PhD" role="doc-biblioref">1998</a>)</span> | 39 | 30 |
|Schofield (Hughes) <span class="citation">(<a href="#ref-Schofield98PhD" role="doc-biblioref">1998</a>)</span> | 33 | 14|
|Woodfield et al. <span class="citation">(<a href="#ref-Woodfield81" role="doc-biblioref">1981</a>)</span> | 63 | 8 |</td>
</tr>
<tr class="odd">
<td align="right">[^1]: Donated through PROMISE.
[^2]: Only a subset of the data in the paper, the complete dataset is donated through PROMISE</td>
</tr>
<tr class="even">
<td align="right"><!--chapter:end:120_dataEffortEstimation.Rmd--></td>
</tr>
<tr class="odd">
<td align="right"># (PART) Exploratory and Descriptive Data analysis {-}</td>
</tr>
<tr class="even">
<td align="right"># Exploratory Data Analysis</td>
</tr>
<tr class="odd">
<td align="right">## Descriptive statistics</td>
</tr>
<tr class="even">
<td align="right">The first task to do with any dataset is to characterize it in terms of summary statistics and graphics.</td>
</tr>
<tr class="odd">
<td align="right">Displaying information graphically will help us to identify the main characteristics of the data. To describe a distribution we often want to know where it is centered and and what the spread is (mean, median, quantiles)</td>
</tr>
<tr class="even">
<td align="right">## Basic Plots</td>
</tr>
<tr class="odd">
<td align="right">* <em>Histogram</em> defines a sequence of breaks and then counts the number of observations in the bins formed by the breaks.</td>
</tr>
<tr class="even">
<td align="right">* <em>Boxplot</em> used to summarize data succinctly, quickly displaying if the data is symmetric or has suspected outliers.</td>
</tr>
<tr class="odd">
<td align="right"><img src="figures/boxplotexp.png" alt="Boxplot description" /></td>
</tr>
<tr class="even">
<td align="right">* <em>Q-Q plot</em> is used to determine if the data is close to being normally distributed. The quantiles of the standard normal distribution is represented by a straight line. The normality of the data can be evaluated by observing the extent in which the points appear on the line. When the data is normally distributed around the mean, then the mean and the median should be equal. Quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way.</td>
</tr>
<tr class="odd">
<td align="right"><img src="figures/Iqr_with_quantile.png" alt="Quartiles in a normal distribution – wikimedia id 14702157" /></td>
</tr>
<tr class="even">
<td align="right">* <em>Scatterplot</em> provides a graphical view of the relationship between two sets of numbers: one numerical variable against another.</td>
</tr>
<tr class="odd">
<td align="right">* <em>Kernel Density</em> plot visualizes the underlying distribution of a variable. Kernel density estimation is a non-parametric method of estimating the probability density function of continuous random variable. It helps to identify the distribution of the variable.</td>
</tr>
<tr class="even">
<td align="right">* <em>Violin plot</em> is a combination of a boxplot and a kernel density plot.</td>
</tr>
<tr class="odd">
<td align="right">## Normality</td>
</tr>
<tr class="even">
<td align="right">* A normal distribution is an arrangement of a data set in which most values cluster in the middle of the range
* A graphical representation of a normal distribution is sometimes called a <em>bell curve</em> because of its shape.
* Many procedures in statistics are based on this property. <em>Parametric</em> procedures require the normality property.
* In a normal distribution about 95% of the probability lies within 2 Standard Deviations of the mean.
* Two examples: one population with mean 60 and the standard deviation of 1, and the other with mean 60 and <span class="math inline">\(sd=4\)</span> (means shifted to 0)</td>
</tr>
<tr class="odd">
<td align="right"><code>r # Area within 2SD of the mean par(mfrow=c(1,2)) plot(function(x) dnorm(x, mean = 0, sd = 1), xlim = c(-3, 3), main = "SD 1", xlab = "x", ylab = "", cex = 2) segments(-2, 0, -2, 0.4) segments(2, 0, 2, 0.4) # Area within 4SD of the mean plot(function(x) dnorm(x, mean = 0, sd = 4), xlim = c(-12, 12), main = "SD 4", xlab = "x", ylab = "", cex = 2) segments(-8, 0, -8, 0.1) segments(8, 0, 8, 0.1)</code></td>
</tr>
<tr class="even">
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">- if we sample from this population we get “another population”:</td>
</tr>
<tr class="even">
<td align="right">```r
#tidy uses the package formatR to format the code</td>
</tr>
<tr class="odd">
<td align="right">sample.means &lt;- rep(NA, 1000)
for (i in 1:1000) {
sample.40 &lt;- rnorm(40, mean = 60, sd = 4)
#rnorm generates random numbers from normal distribution
sample.means[i] &lt;- mean(sample.40)
}
means40 &lt;- mean(sample.means)
sd40 &lt;- sd(sample.means)
means40
```</td>
</tr>
<tr class="even">
<td align="right"><code>## [1] 59.99136</code></td>
</tr>
<tr class="odd">
<td align="right"><code>r sd40</code></td>
</tr>
<tr class="even">
<td align="right"><code>## [1] 0.6292996</code></td>
</tr>
<tr class="odd">
<td align="right">- These sample means are another “population.” The sampling distribution of the sample mean is normally distributed meaning that the “mean of a representative sample provides an estimate of the unknown population mean.” This is shown in Figure <a href="#fig:sampleMeansExample"><strong>??</strong></a></td>
</tr>
<tr class="even">
<td align="right"><code>r hist(sample.means)</code></td>
</tr>
<tr class="odd">
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">## Using a running Example to visualise the different plots</td>
</tr>
<tr class="odd">
<td align="right">As a running example we do next:</td>
</tr>
<tr class="even">
<td align="right">1. Set the path to to the file</td>
</tr>
<tr class="odd">
<td align="right">2. Read the <em>Telecom1</em> dataset and print out the summary statistics with the command <code>summary</code></td>
</tr>
<tr class="even">
<td align="right"><code>r options(digits=3) telecom1 &lt;- read.table("./datasets/effortEstimation/Telecom1.csv", sep=",",header=TRUE, stringsAsFactors=FALSE, dec = ".") #read data summary(telecom1)</code></td>
</tr>
<tr class="odd">
<td align="right"><code>##       size           effort        EstTotal ##  Min.   :  3.0   Min.   :  24   Min.   : 30 ##  1st Qu.: 37.2   1st Qu.: 119   1st Qu.:142 ##  Median : 68.5   Median : 222   Median :289 ##  Mean   :100.3   Mean   : 284   Mean   :320 ##  3rd Qu.:164.0   3rd Qu.: 352   3rd Qu.:472 ##  Max.   :284.0   Max.   :1116   Max.   :777</code></td>
</tr>
<tr class="even">
<td align="right">* We see that this dataset has three variables (or parameters) and few data points (18)
+ <em>size</em>: the independent variable
+ <em>effort</em>: the dependent variable
+ <em>EstTotal</em>: the estimates coming from an estimation method
* Basic Plots</td>
</tr>
<tr class="odd">
<td align="right">```r
par(mfrow=c(1,2)) #n figures per row
size_telecom1 &lt;- telecom1<span class="math inline">\(size effort_telecom1 &lt;- telecom1\)</span>effort</td>
</tr>
<tr class="even">
<td align="right">hist(size_telecom1, col=“blue,” xlab=‘size,’ ylab = ‘Probability,’ main = ‘Histogram of project Size’)
lines(density(size_telecom1, na.rm = T, from = 0, to = max(size_telecom1)))
plot(density(size_telecom1))
```</td>
</tr>
<tr class="odd">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-30-1.png" width="672" /></td>
</tr>
<tr class="even">
<td align="right"><code>r hist(effort_telecom1, col="blue") plot(density(effort_telecom1))</code></td>
</tr>
<tr class="odd">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-30-2.png" width="672" /></td>
</tr>
<tr class="even">
<td align="right"><code>r boxplot(size_telecom1) boxplot(effort_telecom1)</code></td>
</tr>
<tr class="odd">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-30-3.png" width="672" /></td>
</tr>
<tr class="even">
<td align="right"><code>r # violin plots for those two variables library(vioplot) vioplot(size_telecom1, names = '') title("Violin Plot of Project Size") vioplot(effort_telecom1, names = '') title("Violin Plot of Project Effort")</code></td>
</tr>
<tr class="odd">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-30-4.png" width="672" /></td>
</tr>
<tr class="even">
<td align="right"><code>r par(mfrow=c(1,1)) qqnorm(size_telecom1, main="Q-Q Plot of 'size'") qqline(size_telecom1, col=2, lwd=2, lty=2) #draws a line through the first and third quartiles</code></td>
</tr>
<tr class="odd">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-30-5.png" width="672" /></td>
</tr>
<tr class="even">
<td align="right"><code>r qqnorm(effort_telecom1,  main="Q-Q Plot of 'effort'") qqline(effort_telecom1)</code></td>
</tr>
<tr class="odd">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-30-6.png" width="672" /></td>
</tr>
<tr class="even">
<td align="right">* We can observe the non-normality of the data.</td>
</tr>
<tr class="odd">
<td align="right">* We may look the possible relationship between size and effort with a scatter plot</td>
</tr>
<tr class="even">
<td align="right"><code>r plot(size_telecom1, effort_telecom1)</code></td>
</tr>
<tr class="odd">
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">### Example with the China dataset</td>
</tr>
<tr class="odd">
<td align="right"><code>r library(foreign) china &lt;- read.arff("./datasets/effortEstimation/china.arff") china_size &lt;- china$AFP summary(china_size)</code></td>
</tr>
<tr class="even">
<td align="right"><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. ##       9     100     215     487     438   17518</code></td>
</tr>
<tr class="odd">
<td align="right"><code>r china_effort &lt;- china$Effort summary(china_effort)</code></td>
</tr>
<tr class="even">
<td align="right"><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. ##      26     704    1829    3921    3826   54620</code></td>
</tr>
<tr class="odd">
<td align="right"><code>r par(mfrow=c(1,2)) hist(china_size, col="blue", xlab="Adjusted Function Points", main="Distribution of AFP") hist(china_effort, col="blue",xlab="Effort", main="Distribution of Effort")</code></td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-31-1.png" width="672" /></td>
</tr>
<tr class="odd">
<td align="right"><code>r boxplot(china_size) boxplot(china_effort)</code></td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-31-2.png" width="672" /></td>
</tr>
<tr class="odd">
<td align="right"><code>r qqnorm(china_size) qqline(china_size) qqnorm(china_effort) qqline(china_effort)</code></td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-31-3.png" width="672" />
* We observe the non-normality of the data.</td>
</tr>
<tr class="odd">
<td align="right">#### Normality. Galton data</td>
</tr>
<tr class="even">
<td align="right">It is the data based on the famous 1885 Francis Galton’s study about the relationship between the heights of adult children and the heights of their parents.</td>
</tr>
<tr class="odd">
<td align="right"><img src="DASE_files/figure-html/galtonData-1.png" width="672" /></td>
</tr>
<tr class="even">
<td align="right">#### Normalization</td>
</tr>
<tr class="odd">
<td align="right">Take <span class="math inline">\(log\)</span>s in both independent variables. For example, with the <em>China</em> dataset.</td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/logExample-1.png" width="672" /><img src="DASE_files/figure-html/logExample-2.png" width="672" /></td>
</tr>
<tr class="odd">
<td align="right">* If the <span class="math inline">\(log\)</span> transformation is used, then the estimation equation is:
<span class="math display">\[y= e^{b_0 + b_1 log(x)} \]</span></td>
</tr>
<tr class="even">
<td align="right">## Correlation</td>
</tr>
<tr class="odd">
<td align="right"><em>Correlation</em> is a statistical relationship between two sets of data. With the whole dataset we may check for the linear Correlation of the variables we are interested in.</td>
</tr>
<tr class="even">
<td align="right">As an example with the China dataset</td>
</tr>
<tr class="odd">
<td align="right"><code>r par(mfrow=c(1,1)) plot(china_size,china_effort)</code></td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/correlationChinaDataset-1.png" width="672" /></td>
</tr>
<tr class="odd">
<td align="right"><code>r cor(china_size,china_effort)</code></td>
</tr>
<tr class="even">
<td align="right"><code>## [1] 0.685</code></td>
</tr>
<tr class="odd">
<td align="right"><code>r cor.test(china_size,china_effort)</code></td>
</tr>
<tr class="even">
<td align="right"><code>## ##  Pearson's product-moment correlation ## ## data:  china_size and china_effort ## t = 21, df = 497, p-value &lt;2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ##  0.635 0.729 ## sample estimates: ##   cor ## 0.685</code></td>
</tr>
<tr class="odd">
<td align="right"><code>r cor(china_size,china_effort, method="spearman")</code></td>
</tr>
<tr class="even">
<td align="right"><code>## [1] 0.649</code></td>
</tr>
<tr class="odd">
<td align="right"><code>r cor(china_size,china_effort, method="kendall")</code></td>
</tr>
<tr class="even">
<td align="right"><code>## [1] 0.468</code></td>
</tr>
<tr class="odd">
<td align="right">## Confidence Intervals. Bootstrap
* Until now we have generated point estimates
* A <em>confidence interval</em> (CI) is an interval estimate of a population parameter. The parameter can be the mean, the median or other. The frequentist CI is an observed interval that is different from sample to sample. It frequently includes the value of the unobservable parameter of interest if the experiment is repeated. The <em>confidence level</em> is the value that measures the frequency that the constructed intervals contain the true value of the parameter.
* The construction of a confidence interval with an exact value of confidence level for a distribution requires some statistical properties. Usually, <em>normality</em> is one of the properties required for computing confidence intervals.
+ Not all confidence intervals contain the true value of the parameter.
+ Simulation of confidence intervals</td>
</tr>
<tr class="even">
<td align="right">An example from Ugarte et al. <span class="citation">(<a href="#ref-ugarte2015probability" role="doc-biblioref">Ugarte, Militino, and Arnholt 2015</a>)</span></td>
</tr>
<tr class="odd">
<td align="right"><code>r set.seed(10) norsim(sims = 100, n = 36, mu = 100, sigma = 18, conf.level = 0.95)</code></td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-32-1.png" width="672" /></td>
</tr>
<tr class="odd">
<td align="right">* The range defined by the confidence interval will vary with each sample, because the sample size will vary each time and the standard deviation will vary too.
* 95% confidence interval: it is the probability that the hypothetical confidence intervals (that would be computed from the hypothetical repeated samples) will contain the population mean.
* the particular interval that we compute on one sample does not mean that the population mean lies within that interval with a probability of 95%.
* Recommended reading: <span class="citation">(<a href="#ref-Hoekstra2014" role="doc-biblioref">Hoekstra et al. 2014</a>)</span> <em>Robust misinterpretation of confidence intervals</em></td>
</tr>
<tr class="even">
<td align="right">## Nonparametric Bootstrap
* For computing CIs the important thing is to know the assumptions that are made to “know” the
distribution of the statistic.
* There is a way to compute confidence intervals without meeting the requirements of parametric methods.
* <strong>Resampling</strong> or <strong>bootstraping</strong> is a method to calculate estimates of a parameter taking samples from the original data and using those <em>resamples</em> to calculate statistics. Using the resamples usually gives more accurate results than using the original single sample to calculate an estimate of a parameter.</td>
</tr>
<tr class="odd">
<td align="right"><img src="figures/bootstrap.png" alt="Bootstrap" />
- An example of bootstrap CI can be found in Chapter <a href="evaluationSE.html#evaluationSE">3</a>, “Evaluation in Software Engineering”</td>
</tr>
<tr class="even">
<td align="right"><!--chapter:end:200_exploratoryDataAnalysis.Rmd--></td>
</tr>
<tr class="odd">
<td align="right"># Classical Hypothesis Testing</td>
</tr>
<tr class="even">
<td align="right">- By “classical” we mean the standard “frequentist” approach to hypothesis testing. The “frequentist” approach to probability sees it as the frequency of events in the long run. We repeat experiments over and over and we count the times that our object of interest appears in the sequence.</td>
</tr>
<tr class="odd">
<td align="right">- The classical approach is usually called <strong>null hypothesis significance testing</strong> (NHST) because the process starts by setting a null hypothesis <span class="math inline">\(H_0\)</span> which is the opposite about what we think is true.</td>
</tr>
<tr class="even">
<td align="right">- The rationale of the process is that the statistical hypothesis should be <em>falsifiable</em>, that is, we can find evidence that the hypothesis is not true. We try to find evidence against the null hypothesis in order to support our alternative hypothesis <span class="math inline">\(H_A\)</span></td>
</tr>
<tr class="odd">
<td align="right">- Usually, the null hypothesis is described as the situation of “no effect” and the alternative hypothesis describes the effect that we are looking for.</td>
</tr>
<tr class="even">
<td align="right">- After collecting data, taking an actual sample, we measure the distance of our parameter of interest from the hypothesized population parameter, and use the facts of the sampling distribution to determine the probability of obtaining such a sample <em>assuming the hypothesis is true</em>. This is amounts to a test of the hypothesis.</td>
</tr>
<tr class="odd">
<td align="right">- If the probability of our sample, given the null hypothesis is high, this provides evidence that the null hypothesis is true. Conversely, if the probability of the sample is low (given the hypothesis), this is evidence against the null hypothesis. The hypothesis being tested in this way is named the <em>null hypothesis</em>.</td>
</tr>
<tr class="even">
<td align="right">- The goal of the test is to determine if the null hypothesis can be rejected. A statistical test can either reject or fail to reject a null hypothesis, but never prove it true.</td>
</tr>
<tr class="odd">
<td align="right">- We can make two types of errors: false positive (Type I) and false negative (Type II)</td>
</tr>
<tr class="even">
<td align="right">- Type I and Type II errors</td>
</tr>
<tr class="odd">
<td align="right"><img src="figures/typeIandIIwiki.png" /></td>
</tr>
<tr class="even">
<td align="right">- Two-tailed NHST</td>
</tr>
<tr class="odd">
<td align="right"><img src="figures/stat_power_ggplot.png" /></td>
</tr>
<tr class="even">
<td align="right">- One-tailed NHST</td>
</tr>
<tr class="odd">
<td align="right"><img src="figures/One-tailedNHST.png" /></td>
</tr>
<tr class="even">
<td align="right">- elementary example</td>
</tr>
<tr class="odd">
<td align="right"><code>r data = c(52.7, 53.9, 41.7, 71.5, 47.6, 55.1, 62.2, 56.5, 33.4, 61.8, 54.3, 50.0, 45.3, 63.4, 53.9, 65.5, 66.6, 70.0, 52.4, 38.6, 46.1, 44.4, 60.7, 56.4); t.test(data, mu=50, alternative = 'greater')</code></td>
</tr>
<tr class="even">
<td align="right"><code>## ##  One Sample t-test ## ## data:  data ## t = 2, df = 23, p-value = 0.02 ## alternative hypothesis: true mean is greater than 50 ## 95 percent confidence interval: ##  50.9  Inf ## sample estimates: ## mean of x ##      54.3</code></td>
</tr>
<tr class="odd">
<td align="right">- Keeping this simple, we could start hypothesis testing about one sample median with the wilcoxon test for non-normal distributions.</td>
</tr>
<tr class="even">
<td align="right">- “ae” is the absolute error in the China Test data</td>
</tr>
<tr class="odd">
<td align="right"><code>r median(ae)</code></td>
</tr>
<tr class="even">
<td align="right"><code>## [1] 867</code></td>
</tr>
<tr class="odd">
<td align="right"><code>r mean(ae)</code></td>
</tr>
<tr class="even">
<td align="right"><code>## [1] 1867</code></td>
</tr>
<tr class="odd">
<td align="right"><code>r wilcox.test(ae, mu=800, alternative = 'greater') #change the values of mu and see the results</code></td>
</tr>
<tr class="even">
<td align="right"><code>## ##  Wilcoxon signed rank test with continuity correction ## ## data:  ae ## V = 8990, p-value = 8e-04 ## alternative hypothesis: true location is greater than 800</code></td>
</tr>
<tr class="odd">
<td align="right">- Quick introduction at <a href="https://psychstatsworkshop.wordpress.com/2014/08/06/lesson-9-hypothesis-testing/" class="uri">https://psychstatsworkshop.wordpress.com/2014/08/06/lesson-9-hypothesis-testing/</a></td>
</tr>
<tr class="even">
<td align="right">## p-values
- p-value: the p-value of a statistical test is the probability, computed assuming that <span class="math inline">\(H_0\)</span> is true, that the test statistic would take a value as extreme or more extreme than that actually observed.
- <a href="http://www.nature.com/news/psychology-journal-bans-p-values-1.17001" class="uri">http://www.nature.com/news/psychology-journal-bans-p-values-1.17001</a>
- <a href="https://www.sciencenews.org/blog/context/p-value-ban-small-step-journal-giant-leap-science" class="uri">https://www.sciencenews.org/blog/context/p-value-ban-small-step-journal-giant-leap-science</a></td>
</tr>
<tr class="odd">
<td align="right"><img src="figures/pvalueBan.png" /></td>
</tr>
<tr class="even">
<td align="right"><!--chapter:end:220_classicalHypothesisTesting.Rmd--></td>
</tr>
<tr class="odd">
<td align="right"># (PART) Preprocessing {-}</td>
</tr>
<tr class="even">
<td align="right"># Preprocessing</td>
</tr>
<tr class="odd">
<td align="right">Following the data mining process, we describe what is meant by preprocessing, classical supervised models, unsupervised models and evaluation in the context of software engineering with examples</td>
</tr>
<tr class="even">
<td align="right">This task is probably the hardest and where most of effort is spend in the data mining process. It is quite typical to transform the data, for example, finding inconsistencies, normalising, imputing missing values, transforming input data, merging variables, etc.</td>
</tr>
<tr class="odd">
<td align="right">Typically, pre-processing consist of the following tasks (subprocesses):</td>
</tr>
<tr class="even">
<td align="right">+ Data cleaning (consistency, noise detection, outliers)
+ Data integration
+ Data transformation (normalisation, discretisation) and derivation of new attributes from existing ones (e.g., population density from population and area)
+ Missing data imputation
+ Data reduction (feature selection and instace selection)</td>
</tr>
<tr class="odd">
<td align="right">## Data</td>
</tr>
<tr class="even">
<td align="right"><em>Consistent</em> data are semantically correct based on real-world knowledge of the problem, i.e., no constrains are violated and data that can be used for inducing models and analysis. For example, the LoC or effort is constrained to non-negative values. We can also consider that to multiple attributes are consistent among them, and even datasets (e.g., same metrics but collected by different tools)</td>
</tr>
<tr class="odd">
<td align="right">## Missing values</td>
</tr>
<tr class="even">
<td align="right"><em>Missing values</em> will have a negative effect when analysing the data or learning models. The results can be biased when compared with the models induced from the complete data, the results can be harder to analyse, it may be needed to discard records with missing values depending on the algorithm and this can be an important problems with small datasets such as the effort estimation ones.</td>
</tr>
<tr class="odd">
<td align="right">Missing data is typically classified into:
* MCAR (Missing Completely at Random) or MAR (Missing At Random) where there is no reason for those missing values and we can assume that the distribution could follow the attribute’s distribution.
* MNAR (Missing Not At Random) where there is a pattern for those missing values and it may may be advisable to check the data gathering process to try to understand why such information is missing.</td>
</tr>
<tr class="even">
<td align="right"><em>Imputation</em> consists in replacing missing values for estimates of those missing values. Many algorithms do cannot handle missing values and therefore, imputation methods are needed. We can use simple approaches such as the replacing the missing values with the mean or mode of the attribute. More elaborated approaches include:</td>
</tr>
<tr class="odd">
<td align="right">* EM (Expectation-Maximisation)
* Distance-based
+ <span class="math inline">\(k\)</span>-NN (<span class="math inline">\(k\)</span>-Nearest Neighbours)
+ Clustering</td>
</tr>
<tr class="even">
<td align="right">In R, a missing value is represented with <code>NA</code> and the analyst must decide what to do with missing data. The simplest approach is to leave out instances (ignore missing -IM-) with with missing data. This functionality is supported by many base functions through the <code>na.rm</code> option.</td>
</tr>
<tr class="odd">
<td align="right">The <code>mice</code> R package. MICE (Multivariate Imputation via Chained Equations) assumes that data are missing at random. Other packages include <code>Amelia</code>, <code>missForest</code>, <code>Hmisc</code> and <code>mi</code>.</td>
</tr>
<tr class="even">
<td align="right">## Noise</td>
</tr>
<tr class="odd">
<td align="right">Imperfections of the real-world data that influences negatively in the induced machine learning models. Approaches to deal with noisy data include:
* Robust learners capable of handling noisy data (e.g., C4.5 through pruning strategies)
* Data polishing methods which aim to correct noisy instances prior training
* Noise filters which are used to identify and eliminate noisy instances from the training data.</td>
</tr>
<tr class="even">
<td align="right">Types of noise data:
* Class Noise (aka label noise).
+ There can be contradictory cases (all attributes have the same value except the class)
+ Misclassifications. The class attribute is not labeled with the true label (golden truth)
* Attribute Noise. Values of attributes that are noise, missing or unknown.</td>
</tr>
<tr class="odd">
<td align="right">## Outliers</td>
</tr>
<tr class="even">
<td align="right">There is a large amount of literature related to outlier detection, and furthermore several definitions of outlier exist.</td>
</tr>
<tr class="odd">
<td align="right">```r
library(DMwR2)
library(foreign)</td>
</tr>
<tr class="even">
<td align="right">kc1 &lt;- read.arff(“./datasets/defectPred/D1/KC1.arff”)
```</td>
</tr>
<tr class="odd">
<td align="right">The LOF algorithm (<code>lofactor</code>), given a data set it produces a vector of local outlier factors for each case.</td>
</tr>
<tr class="even">
<td align="right"><code>r kc1num &lt;- kc1[,1:21] outlier.scores &lt;- lofactor(kc1num, k=5) plot(density(na.omit(outlier.scores)))</code></td>
</tr>
<tr class="odd">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-37-1.png" width="672" /></td>
</tr>
<tr class="even">
<td align="right"><code>r outliers &lt;- order(outlier.scores, decreasing=T)[1:5] print(outliers)</code></td>
</tr>
<tr class="odd">
<td align="right"><code>## [1]  1  6 14 31 33</code></td>
</tr>
<tr class="even">
<td align="right">Another simple method of Hiridoglou and Berthelot for positive observations.</td>
</tr>
<tr class="odd">
<td align="right">## Feature selection</td>
</tr>
<tr class="even">
<td align="right">Feature Selection (FS) aims at identifying the most relevant attributes from a dataset. It is important in different ways:</td>
</tr>
<tr class="odd">
<td align="right">* A reduced volume of data allows different data mining or searching techniques to be applied.</td>
</tr>
<tr class="even">
<td align="right">* Irrelevant and redundant attributes can generate less accurate and more complex models. Furthermore, data mining algorithms can be executed faster.</td>
</tr>
<tr class="odd">
<td align="right">* It avoids the collection of data for those irrelevant and redundant attributes in the future.</td>
</tr>
<tr class="even">
<td align="right">The problem of FS received a thorough treatment in pattern recognition and machine learning. Most of the FS algorithms tackle the task as a <em>search</em> problem, where each
state in the search specifies a distinct subset of the possible attributes <span class="citation">(<a href="#ref-BL97" role="doc-biblioref">Blum and Langley 1997</a>)</span>. The search procedure is combined with a criterion to evaluate the merit of each candidate subset of attributes. There are a multiple possible combinations between each procedure search and each attribute measure <span class="citation">(<a href="#ref-LY05" role="doc-biblioref">Liu and Yu 2005</a>)</span>.</td>
</tr>
<tr class="odd">
<td align="right">There are two major approaches in FS from the method’s output point of view:</td>
</tr>
<tr class="even">
<td align="right">* <em>Feature subset selection</em> (FSS)</td>
</tr>
<tr class="odd">
<td align="right">* <em>Feature ranking</em> in which attributes are ranked as a list of features which are ordered according to evaluation measures (a subset of features is often selected from the top of the ranking list).</td>
</tr>
<tr class="even">
<td align="right">FFS algorithms designed with different evaluation criteria broadly fall into two categories:</td>
</tr>
<tr class="odd">
<td align="right">* The <em>filter</em> model relies on general characteristics of the data to evaluate and select feature subsets without involving any data mining algorithm.</td>
</tr>
<tr class="even">
<td align="right">* The <em>wrapper</em> model requires one predetermined mining algorithm and uses its performance as the evaluation criterion. It searches for features better suited to the mining algorithm aiming to improve mining performance, but it also tends to be more computationally expensive than filter model <span class="citation"><a href="#ref-Lan94" role="doc-biblioref">Langley</a> (<a href="#ref-Lan94" role="doc-biblioref">1994</a>)</span>.</td>
</tr>
<tr class="odd">
<td align="right">Feature subset algorithms search through candidate feature subsets guide by a certain evaluation measure <span class="citation">(<a href="#ref-LM98" role="doc-biblioref">Liu and Motoda 1998</a>)</span> which captures the goodness of each subset. An optimal (or near optimal) subset is selected when the search stops.</td>
</tr>
<tr class="even">
<td align="right">Some existing evaluation measures that have been shown effective in removing both irrelevant and redundant features include the consistency measure <span class="citation">(<a href="#ref-DLM00" role="doc-biblioref">Dash, Liu, and Motoda 2000</a>)</span>, the correlation measure <span class="citation">(<a href="#ref-Hal99" role="doc-biblioref">Hall 1999</a>)</span> and the estimated accuracy of a learning algorithm <span class="citation">(<a href="#ref-KJ97" role="doc-biblioref">Kohavi and John 1997</a>)</span>.</td>
</tr>
<tr class="odd">
<td align="right">+ <em>Consistency</em> measure attempts to find a minimum number of features that separate classes as consistently as the full set of features can. An inconsistency is defined as to instances having the same
feature values but different class labels.</td>
</tr>
<tr class="even">
<td align="right">+ <em>Correlation</em> measure evaluates the goodness of feature subsets based on the hypothesis that good feature subsets contain features highly correlated to the class, yet uncorrelated to each other.</td>
</tr>
<tr class="odd">
<td align="right">+ <em>Wrapper-based</em> attribute selection uses the target learning algorithm to estimate the worth of attribute subsets. The feature subset selection algorithm conducts a search for a good subset using
the induction algorithm itself as part of the evaluation function.</td>
</tr>
<tr class="even">
<td align="right">Langley <span class="citation">(<a href="#ref-Lan94" role="doc-biblioref">1994</a>)</span> notes that feature selection algorithms that search through the space of feature subsets must address four main issues: (i) the starting point of the search, (ii) the organization of the search, (iii) the evaluation of features subsets and (iv) the criterion used to terminate the search. Different algorithms address theses issues differently.</td>
</tr>
<tr class="odd">
<td align="right">It is impractical to look at all possible feature subsets, even with a small number of attributes. Feature selection algorithms usually proceed greedily and are be classified into those that add features to an initially empty set (<em>forward selection</em>) and those that remove features from an initially complete set (<em>backwards elimination</em>). Hybrids both add and remove features as the algorithm progresses. Forward selection is much faster than backward elimination and therefore scales better to large data sets. A wide range of search strategies can be used: best-first, branch-and-bound, simulated annealing, genetic algorithms (see Kohavi and John <span class="citation">(<a href="#ref-KJ97" role="doc-biblioref">1997</a>)</span> for a review).</td>
</tr>
<tr class="even">
<td align="right">### FSelector package in R</td>
</tr>
<tr class="odd">
<td align="right">The FSelector package in R implements many algorithms available in Weka</td>
</tr>
<tr class="even">
<td align="right">```r
library(FSelector)
library(foreign)</td>
</tr>
<tr class="odd">
<td align="right">cm1 &lt;- read.arff(“./datasets/defectPred/D1/CM1.arff”)</td>
</tr>
<tr class="even">
<td align="right">cm1RFWeigths &lt;- random.forest.importance(Defective ~ ., cm1)
cutoff.biggest.diff(cm1RFWeigths)
```</td>
</tr>
<tr class="odd">
<td align="right"><code>## [1] "LOC_COMMENTS"         "NUM_UNIQUE_OPERATORS"</code></td>
</tr>
<tr class="even">
<td align="right">Using the Information Gain measure as ranking:</td>
</tr>
<tr class="odd">
<td align="right"><code>r cm1GRWeights &lt;- gain.ratio(Defective ~ ., cm1) cm1GRWeights</code></td>
</tr>
<tr class="even">
<td align="right"><code>##                                 attr_importance ## LOC_BLANK                                0.0000 ## BRANCH_COUNT                             0.0000 ## CALL_PAIRS                               0.0000 ## LOC_CODE_AND_COMMENT                     0.0000 ## LOC_COMMENTS                             0.0754 ## CONDITION_COUNT                          0.0000 ## CYCLOMATIC_COMPLEXITY                    0.0000 ## CYCLOMATIC_DENSITY                       0.0000 ## DECISION_COUNT                           0.0000 ## DECISION_DENSITY                         0.0000 ## DESIGN_COMPLEXITY                        0.0000 ## DESIGN_DENSITY                           0.0000 ## EDGE_COUNT                               0.0000 ## ESSENTIAL_COMPLEXITY                     0.0000 ## ESSENTIAL_DENSITY                        0.0000 ## LOC_EXECUTABLE                           0.0888 ## PARAMETER_COUNT                          0.0000 ## HALSTEAD_CONTENT                         0.0701 ## HALSTEAD_DIFFICULTY                      0.0000 ## HALSTEAD_EFFORT                          0.0375 ## HALSTEAD_ERROR_EST                       0.0448 ## HALSTEAD_LENGTH                          0.0425 ## HALSTEAD_LEVEL                           0.0000 ## HALSTEAD_PROG_TIME                       0.0375 ## HALSTEAD_VOLUME                          0.0471 ## MAINTENANCE_SEVERITY                     0.0000 ## MODIFIED_CONDITION_COUNT                 0.0000 ## MULTIPLE_CONDITION_COUNT                 0.0000 ## NODE_COUNT                               0.0000 ## NORMALIZED_CYLOMATIC_COMPLEXITY          0.0000 ## NUM_OPERANDS                             0.0000 ## NUM_OPERATORS                            0.0471 ## NUM_UNIQUE_OPERANDS                      0.0589 ## NUM_UNIQUE_OPERATORS                     0.0616 ## NUMBER_OF_LINES                          0.0573 ## PERCENT_COMMENTS                         0.0663 ## LOC_TOTAL                                0.0763</code></td>
</tr>
<tr class="odd">
<td align="right"><code>r cutoff.biggest.diff(cm1GRWeights)</code></td>
</tr>
<tr class="even">
<td align="right"><code>##  [1] "LOC_EXECUTABLE"       "LOC_TOTAL"            "LOC_COMMENTS" ##  [4] "HALSTEAD_CONTENT"     "PERCENT_COMMENTS"     "NUM_UNIQUE_OPERATORS" ##  [7] "NUM_UNIQUE_OPERANDS"  "NUMBER_OF_LINES"      "HALSTEAD_VOLUME" ## [10] "NUM_OPERATORS"        "HALSTEAD_ERROR_EST"   "HALSTEAD_LENGTH" ## [13] "HALSTEAD_EFFORT"      "HALSTEAD_PROG_TIME"</code></td>
</tr>
<tr class="odd">
<td align="right"><code>r # After assigning weights, we can select the statistaclly significant ones cm1X2Weights &lt;- chi.squared(Defective ~ ., cm1) cutoff.biggest.diff(cm1X2Weights)</code></td>
</tr>
<tr class="even">
<td align="right"><code>##  [1] "LOC_EXECUTABLE"       "LOC_COMMENTS"         "LOC_TOTAL" ##  [4] "NUM_UNIQUE_OPERATORS" "NUM_UNIQUE_OPERANDS"  "NUMBER_OF_LINES" ##  [7] "HALSTEAD_VOLUME"      "NUM_OPERATORS"        "HALSTEAD_ERROR_EST" ## [10] "HALSTEAD_CONTENT"     "HALSTEAD_EFFORT"      "HALSTEAD_PROG_TIME" ## [13] "HALSTEAD_LENGTH"      "PERCENT_COMMENTS"</code></td>
</tr>
<tr class="odd">
<td align="right">Using CFS attribute selection</td>
</tr>
<tr class="even">
<td align="right">```r
library(FSelector)
library(foreign)</td>
</tr>
<tr class="odd">
<td align="right">cm1 &lt;- read.arff(“./datasets/defectPred/D1/CM1.arff”)</td>
</tr>
<tr class="even">
<td align="right">result &lt;- cfs(Defective ~ ., cm1)
f &lt;- as.simple.formula(result, “Defective”)
f
```</td>
</tr>
<tr class="odd">
<td align="right"><code>## Defective ~ LOC_COMMENTS + LOC_EXECUTABLE + HALSTEAD_CONTENT + ##     NUM_UNIQUE_OPERATORS + PERCENT_COMMENTS ## &lt;environment: 0x562134b4e640&gt;</code></td>
</tr>
<tr class="even">
<td align="right">Other packages for Feature selection in R include <code>FSelectorRccp</code> which re-implments the FSlector without WEKA dependencies.</td>
</tr>
<tr class="odd">
<td align="right">Another popular package is <code>Boruta</code>, which is based on selection based on Random Forest.</td>
</tr>
<tr class="even">
<td align="right">## Instance selection</td>
</tr>
<tr class="odd">
<td align="right">Removal of samples (complementary to the removal of attributes) in order to scale down the dataset prior to learning a model so that there is (almost) no performance loss.</td>
</tr>
<tr class="even">
<td align="right">There are two types of processes:</td>
</tr>
<tr class="odd">
<td align="right">* <em>Prototype Selection</em> (PS) <span class="citation">(<a href="#ref-GDCH12" role="doc-biblioref">Garcia et al. 2012</a>)</span> when the subset is used with a distance based method (kNN)</td>
</tr>
<tr class="even">
<td align="right">* <em>Training Set Selection</em> (TSS) <span class="citation">(<a href="#ref-CanoHL07" role="doc-biblioref">Cano, Herrera, and Lozano 2007</a>)</span> in which an actual model is learned.</td>
</tr>
<tr class="odd">
<td align="right">It is also a search problem as with <em>feature selection</em>. Garcia et al. <span class="citation">(<a href="#ref-GDCH12" role="doc-biblioref">2012</a>)</span> provide a comprehensive overview of the topic.</td>
</tr>
<tr class="even">
<td align="right">## Discretization</td>
</tr>
<tr class="odd">
<td align="right">This process transforms continuous attributes into discrete ones, by associating categorical values to intervals and thus transforming quantitative data into qualitative data.</td>
</tr>
<tr class="even">
<td align="right">## Correlation Coefficient and Covariance for Numeric Data</td>
</tr>
<tr class="odd">
<td align="right">Two random variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are called independent if the probability distribution of one variable is not affected by the presence of another.</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(\tilde{\chi}^2=\frac{1}{d}\sum_{k=1}^{n} \frac{(O_k - E_k)^2}{E_k}\)</span></td>
</tr>
<tr class="odd">
<td align="right"><code>r chisq.test(kc1$LOC_BLANK,kc1$BRANCH_TOTAL)</code></td>
</tr>
<tr class="even">
<td align="right"><code>## ##  Chi-squared test for given probabilities ## ## data:  kc1$LOC_BLANK ## X-squared = 17705, df = 2095, p-value &lt;2e-16</code></td>
</tr>
<tr class="odd">
<td align="right"><code>r chisq.test(kc1$DESIGN_COMPLEXITY,kc1$CYCLOMATIC_COMPLEXITY)</code></td>
</tr>
<tr class="even">
<td align="right"><code>## ##  Pearson's Chi-squared test ## ## data:  kc1$DESIGN_COMPLEXITY and kc1$CYCLOMATIC_COMPLEXITY ## X-squared = 25101, df = 696, p-value &lt;2e-16</code></td>
</tr>
<tr class="odd">
<td align="right">## Normalization</td>
</tr>
<tr class="even">
<td align="right">### Min-Max Normalization</td>
</tr>
<tr class="odd">
<td align="right"><span class="math inline">\(z_i=\frac{x_i-\min(x)}{\max(x)-\min(x)}\)</span></td>
</tr>
<tr class="even">
<td align="right"><code>r library(caret) preObj &lt;- preProcess(kc1[, -22], method=c("center", "scale"))</code></td>
</tr>
<tr class="odd">
<td align="right">### Z-score normalization
TBD</td>
</tr>
<tr class="even">
<td align="right">## Transformations</td>
</tr>
<tr class="odd">
<td align="right">### Linear Transformations and Quadratic Trans formations
TBD</td>
</tr>
<tr class="even">
<td align="right">### Box-cox transformation
TBD</td>
</tr>
<tr class="odd">
<td align="right">### Nominal to Binary tranformations
TBD</td>
</tr>
<tr class="even">
<td align="right">## Preprocessing in R</td>
</tr>
<tr class="odd">
<td align="right">### The <code>dplyr</code> package</td>
</tr>
<tr class="even">
<td align="right">The <em><a href="https://cran.r-project.org/web/packages/dplyr/index.html">dplyr</a></em> package created by Hadley Wickham. Some functions are similar to SQL syntax. key functions in dplyr include:</td>
</tr>
<tr class="odd">
<td align="right">+ select: select columns from a dataframe
+ filter: select rows from a dataframe
+ summarize: allows us to do summary stats based upon the grouped variable
+ group_by: group by a factor variable
+ arrange: order the dataset
+ joins: as in sql left join</td>
</tr>
<tr class="even">
<td align="right">Tutorial:
<a href="https://github.com/justmarkham/dplyr-tutorial">https://github.com/justmarkham/dplyr-tutorial</a></td>
</tr>
<tr class="odd">
<td align="right">Examples</td>
</tr>
<tr class="even">
<td align="right"><code>r library(dplyr)</code></td>
</tr>
<tr class="odd">
<td align="right">Describe the dataframe:</td>
</tr>
<tr class="even">
<td align="right"><code>r str(kc1)</code></td>
</tr>
<tr class="odd">
<td align="right"><code>## 'data.frame':    2096 obs. of  22 variables: ##  $ LOC_BLANK            : num  0 0 0 0 2 0 0 0 0 2 ... ##  $ BRANCH_COUNT         : num  1 1 1 1 1 1 1 1 1 1 ... ##  $ LOC_CODE_AND_COMMENT : num  0 0 0 0 0 0 0 0 0 0 ... ##  $ LOC_COMMENTS         : num  0 0 0 0 0 0 0 0 0 0 ... ##  $ CYCLOMATIC_COMPLEXITY: num  1 1 1 1 1 1 1 1 1 1 ... ##  $ DESIGN_COMPLEXITY    : num  1 1 1 1 1 1 1 1 1 1 ... ##  $ ESSENTIAL_COMPLEXITY : num  1 1 1 1 1 1 1 1 1 1 ... ##  $ LOC_EXECUTABLE       : num  3 1 1 1 8 3 1 1 1 9 ... ##  $ HALSTEAD_CONTENT     : num  11.6 0 0 0 18 ... ##  $ HALSTEAD_DIFFICULTY  : num  2.67 0 0 0 3.5 2.67 0 0 0 3.75 ... ##  $ HALSTEAD_EFFORT      : num  82.3 0 0 0 220.9 ... ##  $ HALSTEAD_ERROR_EST   : num  0.01 0 0 0 0.02 0.01 0 0 0 0.04 ... ##  $ HALSTEAD_LENGTH      : num  11 1 1 1 19 11 1 1 1 29 ... ##  $ HALSTEAD_LEVEL       : num  0.38 0 0 0 0.29 0.38 0 0 0 0.27 ... ##  $ HALSTEAD_PROG_TIME   : num  4.57 0 0 0 12.27 ... ##  $ HALSTEAD_VOLUME      : num  30.9 0 0 0 63.1 ... ##  $ NUM_OPERANDS         : num  4 0 0 0 7 4 0 0 0 10 ... ##  $ NUM_OPERATORS        : num  7 1 1 1 12 7 1 1 1 19 ... ##  $ NUM_UNIQUE_OPERANDS  : num  3 0 0 0 5 3 0 0 0 8 ... ##  $ NUM_UNIQUE_OPERATORS : num  4 1 1 1 5 4 1 1 1 6 ... ##  $ LOC_TOTAL            : num  5 3 3 3 12 5 3 3 3 13 ... ##  $ Defective            : Factor w/ 2 levels "N","Y": 1 1 1 1 1 1 1 1 1 1 ...</code></td>
</tr>
<tr class="even">
<td align="right"><code>tbl_df</code> creates a “local data frame” as a wrapper for better printing</td>
</tr>
<tr class="odd">
<td align="right"><code>r kc1_tbl &lt;- tbl_df(kc1) #deprecated</code></td>
</tr>
<tr class="even">
<td align="right"><code>## Warning: `tbl_df()` was deprecated in dplyr 1.0.0. ## Please use `tibble::as_tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.</code></td>
</tr>
<tr class="odd">
<td align="right"><code>r kc1_tbl &lt;- tibble(kc1)</code></td>
</tr>
<tr class="even">
<td align="right">Filter:</td>
</tr>
<tr class="odd">
<td align="right"><code>r # Filter rows: use comma or &amp; to represent AND condition filter(kc1_tbl, Defective == "Y" &amp; LOC_BLANK != 0)</code></td>
</tr>
<tr class="even">
<td align="right"><code>## # A tibble: 251 × 22 ##    LOC_BLANK BRANCH_COUNT LOC_CODE_AND_COMMENT LOC_COMMENTS CYCLOMATIC_COMPLEXI… ##        &lt;dbl&gt;        &lt;dbl&gt;                &lt;dbl&gt;        &lt;dbl&gt;                &lt;dbl&gt; ##  1         6           21                    0           10                   11 ##  2         5           15                    0            2                    8 ##  3         2            5                    0            0                    3 ##  4         4            5                    0            2                    3 ##  5         2           11                    0            2                    6 ##  6         2           23                    0            3                   12 ##  7         1           11                    0            2                    6 ##  8         1           13                    0            2                    7 ##  9         2           17                    0            2                    9 ## 10         3            1                    0            0                    1 ## # … with 241 more rows, and 17 more variables: DESIGN_COMPLEXITY &lt;dbl&gt;, ## #   ESSENTIAL_COMPLEXITY &lt;dbl&gt;, LOC_EXECUTABLE &lt;dbl&gt;, HALSTEAD_CONTENT &lt;dbl&gt;, ## #   HALSTEAD_DIFFICULTY &lt;dbl&gt;, HALSTEAD_EFFORT &lt;dbl&gt;, HALSTEAD_ERROR_EST &lt;dbl&gt;, ## #   HALSTEAD_LENGTH &lt;dbl&gt;, HALSTEAD_LEVEL &lt;dbl&gt;, HALSTEAD_PROG_TIME &lt;dbl&gt;, ## #   HALSTEAD_VOLUME &lt;dbl&gt;, NUM_OPERANDS &lt;dbl&gt;, NUM_OPERATORS &lt;dbl&gt;, ## #   NUM_UNIQUE_OPERANDS &lt;dbl&gt;, NUM_UNIQUE_OPERATORS &lt;dbl&gt;, LOC_TOTAL &lt;dbl&gt;, ## #   Defective &lt;fct&gt;</code></td>
</tr>
<tr class="odd">
<td align="right">Another operator is <code>%in%</code>.</td>
</tr>
<tr class="even">
<td align="right">Select:</td>
</tr>
<tr class="odd">
<td align="right"><code>r select(kc1_tbl, contains("LOC"), Defective)</code></td>
</tr>
<tr class="even">
<td align="right"><code>## # A tibble: 2,096 × 6 ##    LOC_BLANK LOC_CODE_AND_COMME… LOC_COMMENTS LOC_EXECUTABLE LOC_TOTAL Defective ##        &lt;dbl&gt;               &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt; ##  1         0                   0            0              3         5 N ##  2         0                   0            0              1         3 N ##  3         0                   0            0              1         3 N ##  4         0                   0            0              1         3 N ##  5         2                   0            0              8        12 N ##  6         0                   0            0              3         5 N ##  7         0                   0            0              1         3 N ##  8         0                   0            0              1         3 N ##  9         0                   0            0              1         3 N ## 10         2                   0            0              9        13 N ## # … with 2,086 more rows</code></td>
</tr>
<tr class="odd">
<td align="right">Now, <code>kc1_tbl</code> contains(“LOC”), Defective</td>
</tr>
<tr class="even">
<td align="right">Filter and Select together:</td>
</tr>
<tr class="odd">
<td align="right"><code>r # nesting method filter(select(kc1_tbl, contains("LOC"), Defective), Defective !=0)</code></td>
</tr>
<tr class="even">
<td align="right"><code>## # A tibble: 2,096 × 6 ##    LOC_BLANK LOC_CODE_AND_COMME… LOC_COMMENTS LOC_EXECUTABLE LOC_TOTAL Defective ##        &lt;dbl&gt;               &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt; ##  1         0                   0            0              3         5 N ##  2         0                   0            0              1         3 N ##  3         0                   0            0              1         3 N ##  4         0                   0            0              1         3 N ##  5         2                   0            0              8        12 N ##  6         0                   0            0              3         5 N ##  7         0                   0            0              1         3 N ##  8         0                   0            0              1         3 N ##  9         0                   0            0              1         3 N ## 10         2                   0            0              9        13 N ## # … with 2,086 more rows</code></td>
</tr>
<tr class="odd">
<td align="right">It is easier usign the chaining method:</td>
</tr>
<tr class="even">
<td align="right"><code>r # chaining method kc1_tbl %&gt;% select(contains("LOC"), Defective) %&gt;% filter(Defective !=0)</code></td>
</tr>
<tr class="odd">
<td align="right"><code>## # A tibble: 2,096 × 6 ##    LOC_BLANK LOC_CODE_AND_COMME… LOC_COMMENTS LOC_EXECUTABLE LOC_TOTAL Defective ##        &lt;dbl&gt;               &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt; ##  1         0                   0            0              3         5 N ##  2         0                   0            0              1         3 N ##  3         0                   0            0              1         3 N ##  4         0                   0            0              1         3 N ##  5         2                   0            0              8        12 N ##  6         0                   0            0              3         5 N ##  7         0                   0            0              1         3 N ##  8         0                   0            0              1         3 N ##  9         0                   0            0              1         3 N ## 10         2                   0            0              9        13 N ## # … with 2,086 more rows</code></td>
</tr>
<tr class="even">
<td align="right">Arrange ascending</td>
</tr>
<tr class="odd">
<td align="right"><code>r # kc1_tbl %&gt;% select(LOC_TOTAL, Defective) %&gt;% arrange(LOC_TOTAL)</code></td>
</tr>
<tr class="even">
<td align="right"><code>## # A tibble: 2,096 × 2 ##    LOC_TOTAL Defective ##        &lt;dbl&gt; &lt;fct&gt; ##  1         1 N ##  2         1 N ##  3         1 N ##  4         1 N ##  5         1 N ##  6         1 N ##  7         1 N ##  8         1 N ##  9         1 N ## 10         1 N ## # … with 2,086 more rows</code></td>
</tr>
<tr class="odd">
<td align="right">Arrange descending:</td>
</tr>
<tr class="even">
<td align="right"><code>r kc1_tbl %&gt;% select(LOC_TOTAL, Defective) %&gt;% arrange(desc(LOC_TOTAL))</code></td>
</tr>
<tr class="odd">
<td align="right"><code>## # A tibble: 2,096 × 2 ##    LOC_TOTAL Defective ##        &lt;dbl&gt; &lt;fct&gt; ##  1       288 Y ##  2       286 Y ##  3       283 N ##  4       220 Y ##  5       217 Y ##  6       210 N ##  7       205 Y ##  8       184 Y ##  9       179 Y ## 10       176 Y ## # … with 2,086 more rows</code></td>
</tr>
<tr class="even">
<td align="right">Mutate:</td>
</tr>
<tr class="odd">
<td align="right"><code>r kc1_tbl %&gt;% filter(Defective == "Y") %&gt;% select(NUM_OPERANDS, NUM_OPERATORS, Defective) %&gt;% mutate(HalsteadLength = NUM_OPERANDS + NUM_OPERATORS)</code></td>
</tr>
<tr class="even">
<td align="right"><code>## # A tibble: 325 × 4 ##    NUM_OPERANDS NUM_OPERATORS Defective HalsteadLength ##           &lt;dbl&gt;         &lt;dbl&gt; &lt;fct&gt;              &lt;dbl&gt; ##  1           64           107 Y                    171 ##  2           52            89 Y                    141 ##  3           17            41 Y                     58 ##  4           41            74 Y                    115 ##  5           54            95 Y                    149 ##  6           75           156 Y                    231 ##  7           54            95 Y                    149 ##  8           56            99 Y                    155 ##  9           69           124 Y                    193 ## 10           44            60 Y                    104 ## # … with 315 more rows</code></td>
</tr>
<tr class="odd">
<td align="right"><code>summarise</code>: Reduce variables to values</td>
</tr>
<tr class="even">
<td align="right"><code>r # Create a table grouped by Defective, and then summarise each group by taking the mean of loc kc1_tbl %&gt;% group_by(Defective) %&gt;% summarise(avg_loc = mean(LOC_TOTAL, na.rm=TRUE))</code></td>
</tr>
<tr class="odd">
<td align="right"><code>## # A tibble: 2 × 2 ##   Defective avg_loc ##   &lt;fct&gt;       &lt;dbl&gt; ## 1 N            15.9 ## 2 Y            44.7</code></td>
</tr>
<tr class="even">
<td align="right"><code>r # Create a table grouped by Defective, and then summarise each group by taking the mean of loc kc1_tbl %&gt;% group_by(Defective) %&gt;% summarise_each(funs(mean, min, max), BRANCH_COUNT, LOC_TOTAL)</code></td>
</tr>
<tr class="odd">
<td align="right"><code>## Warning: `summarise_each_()` was deprecated in dplyr 0.7.0. ## Please use `across()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.</code></td>
</tr>
<tr class="even">
<td align="right"><code>## Warning: `funs()` was deprecated in dplyr 0.8.0. ## Please use a list of either functions or lambdas: ## ##   # Simple named list: ##   list(mean = mean, median = median) ## ##   # Auto named with `tibble::lst()`: ##   tibble::lst(mean, median) ## ##   # Using lambdas ##   list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.</code></td>
</tr>
<tr class="odd">
<td align="right"><code>## # A tibble: 2 × 7 ##   Defective BRANCH_COUNT_mean LOC_TOTAL_mean BRANCH_COUNT_min LOC_TOTAL_min ##   &lt;fct&gt;                 &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt; ## 1 N                      3.68           15.9                1             1 ## 2 Y                     10.1            44.7                1             2 ## # … with 2 more variables: BRANCH_COUNT_max &lt;dbl&gt;, LOC_TOTAL_max &lt;dbl&gt;</code></td>
</tr>
<tr class="even">
<td align="right">It seems than the number of <em>Defective</em> modules is larger than the <em>Non-Defective</em> ones. We can count them with:</td>
</tr>
<tr class="odd">
<td align="right"><code>r # n() or tally kc1_tbl %&gt;% group_by(Defective) %&gt;% tally()</code></td>
</tr>
<tr class="even">
<td align="right"><code>## # A tibble: 2 × 2 ##   Defective     n ##   &lt;fct&gt;     &lt;int&gt; ## 1 N          1771 ## 2 Y           325</code></td>
</tr>
<tr class="odd">
<td align="right">It seems that it’s an imbalanced dataset…</td>
</tr>
<tr class="even">
<td align="right"><code>r # randomly sample a fixed number of rows, without replacement kc1_tbl %&gt;% sample_n(2)</code></td>
</tr>
<tr class="odd">
<td align="right"><code>## # A tibble: 2 × 22 ##   LOC_BLANK BRANCH_COUNT LOC_CODE_AND_COMMENT LOC_COMMENTS CYCLOMATIC_COMPLEXITY ##       &lt;dbl&gt;        &lt;dbl&gt;                &lt;dbl&gt;        &lt;dbl&gt;                 &lt;dbl&gt; ## 1         0            3                    0            0                     2 ## 2         0            1                    0            0                     1 ## # … with 17 more variables: DESIGN_COMPLEXITY &lt;dbl&gt;, ## #   ESSENTIAL_COMPLEXITY &lt;dbl&gt;, LOC_EXECUTABLE &lt;dbl&gt;, HALSTEAD_CONTENT &lt;dbl&gt;, ## #   HALSTEAD_DIFFICULTY &lt;dbl&gt;, HALSTEAD_EFFORT &lt;dbl&gt;, HALSTEAD_ERROR_EST &lt;dbl&gt;, ## #   HALSTEAD_LENGTH &lt;dbl&gt;, HALSTEAD_LEVEL &lt;dbl&gt;, HALSTEAD_PROG_TIME &lt;dbl&gt;, ## #   HALSTEAD_VOLUME &lt;dbl&gt;, NUM_OPERANDS &lt;dbl&gt;, NUM_OPERATORS &lt;dbl&gt;, ## #   NUM_UNIQUE_OPERANDS &lt;dbl&gt;, NUM_UNIQUE_OPERATORS &lt;dbl&gt;, LOC_TOTAL &lt;dbl&gt;, ## #   Defective &lt;fct&gt;</code></td>
</tr>
<tr class="even">
<td align="right"><code>r # randomly sample a fraction of rows, with replacement kc1_tbl %&gt;% sample_frac(0.05, replace=TRUE)</code></td>
</tr>
<tr class="odd">
<td align="right"><code>## # A tibble: 105 × 22 ##    LOC_BLANK BRANCH_COUNT LOC_CODE_AND_COMMENT LOC_COMMENTS CYCLOMATIC_COMPLEXI… ##        &lt;dbl&gt;        &lt;dbl&gt;                &lt;dbl&gt;        &lt;dbl&gt;                &lt;dbl&gt; ##  1         1            3                    0            0                    2 ##  2         0            1                    0            0                    1 ##  3         0            1                    0            0                    1 ##  4         2            5                    0            0                    3 ##  5         2            7                    0            0                    4 ##  6         0            1                    0            0                    1 ##  7         0            1                    0            0                    1 ##  8         0            1                    0            0                    1 ##  9         0            1                    0            0                    1 ## 10         0            1                    0            1                    1 ## # … with 95 more rows, and 17 more variables: DESIGN_COMPLEXITY &lt;dbl&gt;, ## #   ESSENTIAL_COMPLEXITY &lt;dbl&gt;, LOC_EXECUTABLE &lt;dbl&gt;, HALSTEAD_CONTENT &lt;dbl&gt;, ## #   HALSTEAD_DIFFICULTY &lt;dbl&gt;, HALSTEAD_EFFORT &lt;dbl&gt;, HALSTEAD_ERROR_EST &lt;dbl&gt;, ## #   HALSTEAD_LENGTH &lt;dbl&gt;, HALSTEAD_LEVEL &lt;dbl&gt;, HALSTEAD_PROG_TIME &lt;dbl&gt;, ## #   HALSTEAD_VOLUME &lt;dbl&gt;, NUM_OPERANDS &lt;dbl&gt;, NUM_OPERATORS &lt;dbl&gt;, ## #   NUM_UNIQUE_OPERANDS &lt;dbl&gt;, NUM_UNIQUE_OPERATORS &lt;dbl&gt;, LOC_TOTAL &lt;dbl&gt;, ## #   Defective &lt;fct&gt;</code></td>
</tr>
<tr class="even">
<td align="right"><code>r # Better formatting adapted to the screen width glimpse(kc1_tbl)</code></td>
</tr>
<tr class="odd">
<td align="right"><code>## Rows: 2,096 ## Columns: 22 ## $ LOC_BLANK             &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 2, 1, 2, 2, … ## $ BRANCH_COUNT          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, … ## $ LOC_CODE_AND_COMMENT  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ LOC_COMMENTS          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ CYCLOMATIC_COMPLEXITY &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, … ## $ DESIGN_COMPLEXITY     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, … ## $ ESSENTIAL_COMPLEXITY  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, … ## $ LOC_EXECUTABLE        &lt;dbl&gt; 3, 1, 1, 1, 8, 3, 1, 1, 1, 9, 8, 1, 8, 1, 8, 12,… ## $ HALSTEAD_CONTENT      &lt;dbl&gt; 11.6, 0.0, 0.0, 0.0, 18.0, 11.6, 0.0, 0.0, 0.0, … ## $ HALSTEAD_DIFFICULTY   &lt;dbl&gt; 2.67, 0.00, 0.00, 0.00, 3.50, 2.67, 0.00, 0.00, … ## $ HALSTEAD_EFFORT       &lt;dbl&gt; 82.3, 0.0, 0.0, 0.0, 220.9, 82.3, 0.0, 0.0, 0.0,… ## $ HALSTEAD_ERROR_EST    &lt;dbl&gt; 0.01, 0.00, 0.00, 0.00, 0.02, 0.01, 0.00, 0.00, … ## $ HALSTEAD_LENGTH       &lt;dbl&gt; 11, 1, 1, 1, 19, 11, 1, 1, 1, 29, 19, 1, 19, 1, … ## $ HALSTEAD_LEVEL        &lt;dbl&gt; 0.38, 0.00, 0.00, 0.00, 0.29, 0.38, 0.00, 0.00, … ## $ HALSTEAD_PROG_TIME    &lt;dbl&gt; 4.57, 0.00, 0.00, 0.00, 12.27, 4.57, 0.00, 0.00,… ## $ HALSTEAD_VOLUME       &lt;dbl&gt; 30.9, 0.0, 0.0, 0.0, 63.1, 30.9, 0.0, 0.0, 0.0, … ## $ NUM_OPERANDS          &lt;dbl&gt; 4, 0, 0, 0, 7, 4, 0, 0, 0, 10, 7, 0, 7, 0, 11, 1… ## $ NUM_OPERATORS         &lt;dbl&gt; 7, 1, 1, 1, 12, 7, 1, 1, 1, 19, 12, 1, 12, 1, 16… ## $ NUM_UNIQUE_OPERANDS   &lt;dbl&gt; 3, 0, 0, 0, 5, 3, 0, 0, 0, 8, 5, 0, 5, 0, 6, 9, … ## $ NUM_UNIQUE_OPERATORS  &lt;dbl&gt; 4, 1, 1, 1, 5, 4, 1, 1, 1, 6, 5, 1, 5, 1, 8, 12,… ## $ LOC_TOTAL             &lt;dbl&gt; 5, 3, 3, 3, 12, 5, 3, 3, 3, 13, 12, 3, 12, 4, 13… ## $ Defective             &lt;fct&gt; N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, …</code></td>
</tr>
<tr class="even">
<td align="right">## Other libraries and tricks</td>
</tr>
<tr class="odd">
<td align="right">The <code>lubridate</code> package contains a number of functions facilitating the conversion of text to
POSIX dates. As an example, consider the following code. We may use this, for example, with time series.</td>
</tr>
<tr class="even">
<td align="right">For example <a href="https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf">https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf</a></td>
</tr>
<tr class="odd">
<td align="right"><code>r library(lubridate) dates &lt;- c("15/02/2013", "15 Feb 13", "It happened on 15 02 '13") dmy(dates)</code></td>
</tr>
<tr class="even">
<td align="right"><code>## [1] "2013-02-15" "2013-02-15" "2013-02-15"</code></td>
</tr>
<tr class="odd">
<td align="right"><!--chapter:end:300_basicPreprocessing.Rmd--></td>
</tr>
<tr class="even">
<td align="right"># (PART) Supervised Models {-}</td>
</tr>
<tr class="odd">
<td align="right"># Supervised Classification</td>
</tr>
<tr class="even">
<td align="right">A classification problem can be defined as the induction, from a dataset <span class="math inline">\(\cal D\)</span>, of a classification function <span class="math inline">\(\psi\)</span> that, given the attribute vector of an instance/example, returns a class <span class="math inline">\({c}\)</span>. A regression problem, on the other hand, returns an numeric value.</td>
</tr>
<tr class="odd">
<td align="right">Dataset, <span class="math inline">\(\cal D\)</span>, is typically composed of <span class="math inline">\(n\)</span> attributes and a class attribute <span class="math inline">\(C\)</span>.</td>
</tr>
<tr class="even">
<td align="right">| <span class="math inline">\(Att_1\)</span> | … | <span class="math inline">\(Att_n\)</span> | <span class="math inline">\(Class\)</span> |
|———-|—–| ———|———|
| <span class="math inline">\(a_{11}\)</span> | … | <span class="math inline">\(a_{1n}\)</span> | <span class="math inline">\(c_1\)</span> |
| <span class="math inline">\(a_{21}\)</span> | … | <span class="math inline">\(a_{2n}\)</span> | <span class="math inline">\(c_2\)</span> |
| … | … | … | … |
| <span class="math inline">\(a_{m1}\)</span> | … | <span class="math inline">\(a_{mn}\)</span> | <span class="math inline">\(c_m\)</span> |</td>
</tr>
<tr class="odd">
<td align="right">Columns are usually called <em>attributes</em> or <em>features</em>. Typically, there is a <em>class</em> attribute, which can be numeric or discrete. When the class is numeric, it is a regression problem. With discrete values, we can talk about binary classification or multiclass (multinomial classification) when we have more than three values. There are variants such <em>multi-label</em> classification (we will cover these in the advanced models section).</td>
</tr>
<tr class="even">
<td align="right">Once we learn a model, new instances are classified. As shown in the next figure.</td>
</tr>
<tr class="odd">
<td align="right"><img src="figures/supervClass1.png" alt="Supervised Classification" /></td>
</tr>
<tr class="even">
<td align="right">We have multiple types of models such as <em>classification trees</em>, <em>rules</em>, <em>neural networks</em>, and <em>probabilistic classifiers</em> that can be used to classify instances.</td>
</tr>
<tr class="odd">
<td align="right">Fernandez et al provide an extensive comparison of 176 classifiers using the UCI dataset <span class="citation">(<a href="#ref-FernandezCBA14" role="doc-biblioref">Fernández-Delgado et al. 2014</a>)</span>.</td>
</tr>
<tr class="even">
<td align="right">We will show the use of different classification techniques in the problem of defect prediction as running example. In this example,the different datasets are composed of classical metrics (<em>Halstead</em> or <em>McCabe</em> metrics) based on counts of operators/operands and like or object-oriented metrics (e.g. Chidamber and Kemerer) and the class attribute indicating whether the module or class was defective.</td>
</tr>
<tr class="odd">
<td align="right">## Classification Trees</td>
</tr>
<tr class="even">
<td align="right">There are several packages for inducing classification trees, for example with the <a href="https://cran.r-project.org/web/packages/party/index.html">party package</a> (recursive partitioning):</td>
</tr>
<tr class="odd">
<td align="right">```r
library(foreign) # To load arff file
library(party) # Build a decision tree
library(caret)</td>
</tr>
<tr class="even">
<td align="right">jm1 &lt;- read.arff(“./datasets/defectPred/D1/JM1.arff”)
str(jm1)
```</td>
</tr>
<tr class="odd">
<td align="right"><code>## 'data.frame':    9593 obs. of  22 variables: ##  $ LOC_BLANK            : num  447 37 11 106 101 67 105 18 39 143 ... ##  $ BRANCH_COUNT         : num  826 29 405 240 464 187 344 47 163 67 ... ##  $ LOC_CODE_AND_COMMENT : num  12 8 0 7 11 4 9 0 1 7 ... ##  $ LOC_COMMENTS         : num  157 42 17 344 75 1 40 10 6 49 ... ##  $ CYCLOMATIC_COMPLEXITY: num  470 19 404 127 263 94 207 24 94 34 ... ##  $ DESIGN_COMPLEXITY    : num  385 19 2 105 256 63 171 13 67 25 ... ##  $ ESSENTIAL_COMPLEXITY : num  113 6 1 33 140 27 58 1 3 1 ... ##  $ LOC_EXECUTABLE       : num  2824 133 814 952 1339 ... ##  $ HALSTEAD_CONTENT     : num  210 108 101 218 106 ... ##  $ HALSTEAD_DIFFICULTY  : num  384.4 46.3 206 215.2 337.4 ... ##  $ HALSTEAD_EFFORT      : num  31079782 232044 4294926 10100867 12120796 ... ##  $ HALSTEAD_ERROR_EST   : num  26.95 1.67 6.95 15.65 11.98 ... ##  $ HALSTEAD_LENGTH      : num  8441 685 2033 5669 4308 ... ##  $ HALSTEAD_LEVEL       : num  0 0.02 0 0 0 0.02 0 0.03 0.01 0.02 ... ##  $ HALSTEAD_PROG_TIME   : num  1726655 12891 238607 561159 673378 ... ##  $ HALSTEAD_VOLUME      : num  80843 5009 20848 46944 35928 ... ##  $ NUM_OPERANDS         : num  3021 295 813 2301 1556 ... ##  $ NUM_OPERATORS        : num  5420 390 1220 3368 2752 ... ##  $ NUM_UNIQUE_OPERANDS  : num  609 121 811 262 226 167 279 47 117 355 ... ##  $ NUM_UNIQUE_OPERATORS : num  155 38 411 49 98 27 105 18 52 23 ... ##  $ LOC_TOTAL            : num  3442 222 844 1411 1532 ... ##  $ Defective            : Factor w/ 2 levels "N","Y": 2 2 2 2 2 2 2 2 1 2 ...</code></td>
</tr>
<tr class="even">
<td align="right">```r
# Stratified partition (training and test sets)
set.seed(1234)
inTrain &lt;- createDataPartition(y=jm1$Defective,p=.60,list=FALSE)
jm1.train &lt;- jm1[inTrain,]
jm1.test &lt;- jm1[-inTrain,]</td>
</tr>
<tr class="odd">
<td align="right">jm1.formula &lt;- jm1$Defective ~ . # formula approach: defect as dependent variable and the rest as independent variables
jm1.ctree &lt;- ctree(jm1.formula, data = jm1.train)</td>
</tr>
<tr class="even">
<td align="right"># predict on test data
pred &lt;- predict(jm1.ctree, newdata = jm1.test)
# check prediction result
table(pred, jm1.test$Defective)
```</td>
</tr>
<tr class="odd">
<td align="right"><code>## ## pred    N    Y ##    N  168   11 ##    Y 2965  692</code></td>
</tr>
<tr class="even">
<td align="right"><code>r plot(jm1.ctree)</code></td>
</tr>
<tr class="odd">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-58-1.png" width="672" /></td>
</tr>
<tr class="even">
<td align="right">Using the C50 package, there are two ways, specifying train and testing</td>
</tr>
<tr class="odd">
<td align="right"><code>r library(C50) require(utils) # c50t &lt;- C5.0(jm1.train[,-ncol(jm1.train)], jm1.train[,ncol(jm1.train)]) c50t &lt;- C5.0(Defective ~ ., jm1.train) summary(c50t) plot(c50t) c50tPred &lt;- predict(c50t, jm1.train) # table(c50tPred, jm1.train$Defective)</code></td>
</tr>
<tr class="even">
<td align="right">Using the <a href="https://cran.r-project.org/web/packages/rpart/index.html">‘rpart’</a> package</td>
</tr>
<tr class="odd">
<td align="right"><code>r # Using the 'rpart' package library(rpart) jm1.rpart &lt;- rpart(Defective ~ ., data=jm1.train, parms = list(prior = c(.65,.35), split = "information")) # par(mfrow = c(1,2), xpd = NA) plot(jm1.rpart) text(jm1.rpart, use.n = TRUE)</code></td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-60-1.png" width="672" /></td>
</tr>
<tr class="odd">
<td align="right"><code>r jm1.rpart</code></td>
</tr>
<tr class="even">
<td align="right"><code>## n= 5757 ## ## node), split, n, loss, yval, (yprob) ##       * denotes terminal node ## ##  1) root 5757 2010.0 N (0.650 0.350) ##    2) LOC_TOTAL&lt; 38.5 4172  969.0 N (0.751 0.249) * ##    3) LOC_TOTAL&gt;=38.5 1585  825.0 Y (0.441 0.559) ##      6) LOC_TOTAL&lt; 87.5 1027  540.0 N (0.523 0.477) ##       12) LOC_BLANK&lt; 7.5 580  263.0 N (0.572 0.428) * ##       13) LOC_BLANK&gt;=7.5 447  240.0 Y (0.465 0.535) ##         26) HALSTEAD_DIFFICULTY&gt;=34.9 62   15.3 N (0.738 0.262) * ##         27) HALSTEAD_DIFFICULTY&lt; 34.9 385  197.0 Y (0.430 0.570) * ##      7) LOC_TOTAL&gt;=87.5 558  233.0 Y (0.316 0.684) *</code></td>
</tr>
<tr class="odd">
<td align="right"><code>r library(rpart.plot) # asRules(jm1.rpart) # fancyRpartPlot(jm1.rpart)</code></td>
</tr>
<tr class="even">
<td align="right">## Rules</td>
</tr>
<tr class="odd">
<td align="right">C5 Rules</td>
</tr>
<tr class="even">
<td align="right"><code>r library(C50) c50r &lt;- C5.0(jm1.train[,-ncol(jm1.train)], jm1.train[,ncol(jm1.train)], rules = TRUE) summary(c50r)</code></td>
</tr>
<tr class="odd">
<td align="right"><code>## ## Call: ## C5.0.default(x = jm1.train[, -ncol(jm1.train)], y = ##  jm1.train[, ncol(jm1.train)], rules = TRUE) ## ## ## C5.0 [Release 2.07 GPL Edition]      Sun Oct 10 13:28:46 2021 ## ------------------------------- ## ## Class specified by attribute `outcome' ## ## Read 5757 cases (22 attributes) from undefined.data ## ## Rules: ## ## Rule 1: (5682/1005, lift 1.0) ##  NUM_OPERANDS &lt;= 376 ##  -&gt;  class N  [0.823] ## ## Rule 2: (75/24, lift 3.7) ##  NUM_OPERANDS &gt; 376 ##  -&gt;  class Y  [0.675] ## ## Default class: N ## ## ## Evaluation on training data (5757 cases): ## ##          Rules ##    ---------------- ##      No      Errors ## ##       2 1029(17.9%)   &lt;&lt; ## ## ##     (a)   (b)    &lt;-classified as ##    ----  ---- ##    4677    24    (a): class N ##    1005    51    (b): class Y ## ## ##  Attribute usage: ## ##  100.00% NUM_OPERANDS ## ## ## Time: 0.1 secs</code></td>
</tr>
<tr class="even">
<td align="right"><code>r # c50rPred &lt;- predict(c50r, jm1.train) # table(c50rPred, jm1.train$Defective)</code></td>
</tr>
<tr class="odd">
<td align="right">## Distanced-based Methods</td>
</tr>
<tr class="even">
<td align="right">In this case, there is no model as such. Given a new instance to classify, this approach finds the closest <span class="math inline">\(k\)</span>-neighbours to the given instance.</td>
</tr>
<tr class="odd">
<td align="right"><img src="figures/279px-KnnClassification.svg.png" alt="k-NN Classification" />
(Source: Wikipedia - <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" class="uri">https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm</a>)</td>
</tr>
<tr class="even">
<td align="right">```r
library(class)
m1 &lt;- knn(train=jm1.train[,-22], test=jm1.test[,-22], cl=jm1.train[,22], k=3)</td>
</tr>
<tr class="odd">
<td align="right">table(jm1.test[,22],m1)
```</td>
</tr>
<tr class="even">
<td align="right"><code>##    m1 ##        N    Y ##   N 2851  282 ##   Y  554  149</code></td>
</tr>
<tr class="odd">
<td align="right">## Neural Networks</td>
</tr>
<tr class="even">
<td align="right"><img src="figures/neuralnet.png" alt="Neural Networks" /></td>
</tr>
<tr class="odd">
<td align="right"><img src="figures/neuralnet2.png" alt="Neural Networks" /></td>
</tr>
<tr class="even">
<td align="right">## Support Vector Machine</td>
</tr>
<tr class="odd">
<td align="right"><img src="figures/Kernel_Machine.svg.png" alt="SVM" />
(Source: wikipedia <a href="https://en.wikipedia.org/wiki/Support_vector_machine" class="uri">https://en.wikipedia.org/wiki/Support_vector_machine</a>)</td>
</tr>
<tr class="even">
<td align="right">## Probabilistic Methods</td>
</tr>
<tr class="odd">
<td align="right">### Naive Bayes</td>
</tr>
<tr class="even">
<td align="right">Probabilistic graphical model assigning a probability to each possible outcome <span class="math inline">\(p(C_k, x_1,\ldots,x_n)\)</span></td>
</tr>
<tr class="odd">
<td align="right"><img src="figures/classifier_NB.png" alt="Naive Bayes" /></td>
</tr>
<tr class="even">
<td align="right">Using the <code>klaR</code> package with <code>caret</code>:</td>
</tr>
<tr class="odd">
<td align="right"><code>r library(caret) library(klaR)</code></td>
</tr>
<tr class="even">
<td align="right"><code>## Loading required package: MASS</code></td>
</tr>
<tr class="odd">
<td align="right"><code>## ## Attaching package: 'MASS'</code></td>
</tr>
<tr class="even">
<td align="right"><code>## The following object is masked from 'package:dplyr': ## ##     select</code></td>
</tr>
<tr class="odd">
<td align="right"><code>## The following object is masked from 'package:sm': ## ##     muscle</code></td>
</tr>
<tr class="even">
<td align="right"><code>r model &lt;- NaiveBayes(Defective ~ ., data = jm1.train) predictions &lt;- predict(model, jm1.test[,-22]) confusionMatrix(predictions$class, jm1.test$Defective)</code></td>
</tr>
<tr class="odd">
<td align="right"><code>## Confusion Matrix and Statistics ## ##           Reference ## Prediction    N    Y ##          N 2963  554 ##          Y  170  149 ## ##                Accuracy : 0.811 ##                  95% CI : (0.799, 0.824) ##     No Information Rate : 0.817 ##     P-Value [Acc &gt; NIR] : 0.815 ## ##                   Kappa : 0.2 ## ##  Mcnemar's Test P-Value : &lt;2e-16 ## ##             Sensitivity : 0.946 ##             Specificity : 0.212 ##          Pos Pred Value : 0.842 ##          Neg Pred Value : 0.467 ##              Prevalence : 0.817 ##          Detection Rate : 0.772 ##    Detection Prevalence : 0.917 ##       Balanced Accuracy : 0.579 ## ##        'Positive' Class : N ##</code></td>
</tr>
<tr class="even">
<td align="right">Using the <code>e1071</code> package:</td>
</tr>
<tr class="odd">
<td align="right">```r
library (e1071)
n1 &lt;-naiveBayes(jm1.train$Defective ~ ., data=jm1.train)</td>
</tr>
<tr class="even">
<td align="right"># Show first 3 results using ‘class’
head(predict(n1,jm1.test, type = c(“class”)),3) # class by default
```</td>
</tr>
<tr class="odd">
<td align="right"><code>## [1] Y Y Y ## Levels: N Y</code></td>
</tr>
<tr class="even">
<td align="right"><code>r # Show first 3 results using 'raw' head(predict(n1,jm1.test, type = c("raw")),3)</code></td>
</tr>
<tr class="odd">
<td align="right"><code>##            N Y ## [1,] 8.6e-50 1 ## [2,] 0.0e+00 1 ## [3,] 0.0e+00 1</code></td>
</tr>
<tr class="even">
<td align="right">There are other variants such as TAN and KDB that do not assume the independece condition allowin us more complex structures.</td>
</tr>
<tr class="odd">
<td align="right"><img src="figures/classifier_TAN.png" alt="Naive Bayes" /></td>
</tr>
<tr class="even">
<td align="right"><img src="figures/classifier_KDB.png" alt="Naive Bayes" /></td>
</tr>
<tr class="odd">
<td align="right">A comprehensice comparison of</td>
</tr>
<tr class="even">
<td align="right">## Linear Discriminant Analysis (LDA)</td>
</tr>
<tr class="odd">
<td align="right">One classical approach to classification is Linear Discriminant Analysis (LDA), a generalization of Fisher’s linear discriminant, as a method used to find a linear combination of features to separate two or more classes.</td>
</tr>
<tr class="even">
<td align="right"><code>r ldaModel &lt;- train (Defective ~ ., data=jm1.train, method="lda", preProc=c("center","scale")) ldaModel</code></td>
</tr>
<tr class="odd">
<td align="right"><code>## Linear Discriminant Analysis ## ## 5757 samples ##   21 predictor ##    2 classes: 'N', 'Y' ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 5757, 5757, 5757, 5757, 5757, 5757, ... ## Resampling results: ## ##   Accuracy  Kappa ##   0.82      0.164</code></td>
</tr>
<tr class="even">
<td align="right">We can observe that we are training our model using <code>Defective ~ .</code> as a formula were <code>Defective</code> is the class variable separed by <code>~</code> and the ´.´ means the rest of the variables. Also, we are using a filter for the training data to (preProc) to center and scale.</td>
</tr>
<tr class="odd">
<td align="right">Also, as stated in the documentation about the <code>train</code> method :
&gt; <a href="http://topepo.github.io/caret/training.html" class="uri">http://topepo.github.io/caret/training.html</a></td>
</tr>
<tr class="even">
<td align="right">```r
ctrl &lt;- trainControl(method = “repeatedcv,”repeats=3)
ldaModel &lt;- train (Defective ~ ., data=jm1.train, method=“lda,” trControl=ctrl, preProc=c(“center,”“scale”))</td>
</tr>
<tr class="odd">
<td align="right">ldaModel
```</td>
</tr>
<tr class="even">
<td align="right"><code>## Linear Discriminant Analysis ## ## 5757 samples ##   21 predictor ##    2 classes: 'N', 'Y' ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 5181, 5182, 5181, 5182, 5180, 5181, ... ## Resampling results: ## ##   Accuracy  Kappa ##   0.82      0.159</code></td>
</tr>
<tr class="odd">
<td align="right">Instead of accuracy we can activate other metrics using <code>summaryFunction=twoClassSummary</code> such as <code>ROC</code>, <code>sensitivity</code> and <code>specificity</code>. To do so, we also need to speficy <code>classProbs=TRUE</code>.</td>
</tr>
<tr class="even">
<td align="right">```r
ctrl &lt;- trainControl(method = “repeatedcv,”repeats=3, classProbs=TRUE, summaryFunction=twoClassSummary)
ldaModel3xcv10 &lt;- train (Defective ~ ., data=jm1.train, method=“lda,” trControl=ctrl, preProc=c(“center,”“scale”))</td>
</tr>
<tr class="odd">
<td align="right">ldaModel3xcv10
```</td>
</tr>
<tr class="even">
<td align="right"><code>## Linear Discriminant Analysis ## ## 5757 samples ##   21 predictor ##    2 classes: 'N', 'Y' ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 5181, 5181, 5181, 5182, 5182, 5181, ... ## Resampling results: ## ##   ROC    Sens   Spec ##   0.708  0.971  0.143</code></td>
</tr>
<tr class="odd">
<td align="right">Most methods have parameters that need to be optimised and that is one of the</td>
</tr>
<tr class="even">
<td align="right">```r
plsFit3x10cv &lt;- train (Defective ~ ., data=jm1.train, method=“pls,” trControl=trainControl(classProbs=TRUE), metric=“ROC,” preProc=c(“center,”“scale”))</td>
</tr>
<tr class="odd">
<td align="right">plsFit3x10cv
```</td>
</tr>
<tr class="even">
<td align="right"><code>## Partial Least Squares ## ## 5757 samples ##   21 predictor ##    2 classes: 'N', 'Y' ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 5757, 5757, 5757, 5757, 5757, 5757, ... ## Resampling results across tuning parameters: ## ##   ncomp  Accuracy  Kappa ##   1      0.821     0.0620 ##   2      0.821     0.0978 ##   3      0.821     0.0992 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was ncomp = 3.</code></td>
</tr>
<tr class="odd">
<td align="right"><code>r plot(plsFit3x10cv)</code></td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-68-1.png" width="672" /></td>
</tr>
<tr class="odd">
<td align="right">The parameter <code>tuneLength</code> allow us to specify the number values per parameter to consider.</td>
</tr>
<tr class="even">
<td align="right">```r
plsFit3x10cv &lt;- train (Defective ~ ., data=jm1.train, method=“pls,” trControl=ctrl, metric=“ROC,” tuneLength=5, preProc=c(“center,”“scale”))</td>
</tr>
<tr class="odd">
<td align="right">plsFit3x10cv
```</td>
</tr>
<tr class="even">
<td align="right"><code>## Partial Least Squares ## ## 5757 samples ##   21 predictor ##    2 classes: 'N', 'Y' ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 5181, 5182, 5181, 5182, 5181, 5182, ... ## Resampling results across tuning parameters: ## ##   ncomp  ROC    Sens   Spec ##   1      0.700  0.996  0.0429 ##   2      0.703  0.989  0.0710 ##   3      0.706  0.990  0.0720 ##   4      0.708  0.990  0.0808 ##   5      0.708  0.990  0.0808 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was ncomp = 5.</code></td>
</tr>
<tr class="odd">
<td align="right"><code>r plot(plsFit3x10cv)</code></td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-69-1.png" width="672" /></td>
</tr>
<tr class="odd">
<td align="right">Finally to predict new cases, <code>caret</code> will use the best classfier obtained for prediction.</td>
</tr>
<tr class="even">
<td align="right"><code>r plsProbs &lt;- predict(plsFit3x10cv, newdata = jm1.test, type = "prob")</code></td>
</tr>
<tr class="odd">
<td align="right"><code>r plsClasses &lt;- predict(plsFit3x10cv, newdata = jm1.test, type = "raw") confusionMatrix(data=plsClasses,jm1.test$Defective)</code></td>
</tr>
<tr class="even">
<td align="right"><code>## Confusion Matrix and Statistics ## ##           Reference ## Prediction    N    Y ##          N 3094  652 ##          Y   39   51 ## ##                Accuracy : 0.82 ##                  95% CI : (0.807, 0.832) ##     No Information Rate : 0.817 ##     P-Value [Acc &gt; NIR] : 0.317 ## ##                   Kappa : 0.091 ## ##  Mcnemar's Test P-Value : &lt;2e-16 ## ##             Sensitivity : 0.9876 ##             Specificity : 0.0725 ##          Pos Pred Value : 0.8259 ##          Neg Pred Value : 0.5667 ##              Prevalence : 0.8167 ##          Detection Rate : 0.8066 ##    Detection Prevalence : 0.9765 ##       Balanced Accuracy : 0.5300 ## ##        'Positive' Class : N ##</code></td>
</tr>
<tr class="odd">
<td align="right">### Predicting the number of defects (numerical class)</td>
</tr>
<tr class="even">
<td align="right">From the Bug Prediction Repository (BPR) <a href="http://bug.inf.usi.ch/download.php">http://bug.inf.usi.ch/download.php</a></td>
</tr>
<tr class="odd">
<td align="right">Some datasets contain CK and other 11 object-oriented metrics for the last version of the system plus categorized (with severity and priority) post-release defects. Using such dataset:</td>
</tr>
<tr class="even">
<td align="right">```r
jdt &lt;- read.csv(“./datasets/defectPred/BPD/single-version-ck-oo-EclipseJDTCore.csv,” sep=“;”)</td>
</tr>
<tr class="odd">
<td align="right"># We just use the number of bugs, so we removed others
jdt<span class="math inline">\(classname &lt;- NULL jdt\)</span>nonTrivialBugs &lt;- NULL
jdt<span class="math inline">\(majorBugs &lt;- NULL jdt\)</span>minorBugs &lt;- NULL
jdt<span class="math inline">\(criticalBugs &lt;- NULL jdt\)</span>highPriorityBugs &lt;- NULL
jdt$X &lt;- NULL</td>
</tr>
<tr class="even">
<td align="right"># Caret
library(caret)</td>
</tr>
<tr class="odd">
<td align="right"># Split data into training and test datasets
set.seed(1)
inTrain &lt;- createDataPartition(y=jdt$bugs,p=.8,list=FALSE)
jdt.train &lt;- jdt[inTrain,]
jdt.test &lt;- jdt[-inTrain,]
```</td>
</tr>
<tr class="even">
<td align="right"><code>r ctrl &lt;- trainControl(method = "repeatedcv",repeats=3) glmModel &lt;- train (bugs ~ ., data=jdt.train, method="glm", trControl=ctrl, preProc=c("center","scale")) glmModel</code></td>
</tr>
<tr class="odd">
<td align="right"><code>## Generalized Linear Model ## ## 798 samples ##  17 predictor ## ## Pre-processing: centered (17), scaled (17) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 719, 718, 718, 718, 718, 718, ... ## Resampling results: ## ##   RMSE   Rsquared  MAE ##   0.936  0.273     0.442</code></td>
</tr>
<tr class="even">
<td align="right">Others such as Elasticnet:</td>
</tr>
<tr class="odd">
<td align="right"><code>r glmnetModel &lt;- train (bugs ~ ., data=jdt.train, method="glmnet", trControl=ctrl, preProc=c("center","scale")) glmnetModel</code></td>
</tr>
<tr class="even">
<td align="right"><code>## glmnet ## ## 798 samples ##  17 predictor ## ## Pre-processing: centered (17), scaled (17) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 718, 718, 718, 718, 718, 718, ... ## Resampling results across tuning parameters: ## ##   alpha  lambda   RMSE   Rsquared  MAE ##   0.10   0.00112  1.004  0.249     0.458 ##   0.10   0.01120  0.938  0.247     0.447 ##   0.10   0.11195  0.840  0.272     0.430 ##   0.55   0.00112  1.006  0.249     0.458 ##   0.55   0.01120  0.918  0.249     0.444 ##   0.55   0.11195  0.825  0.286     0.433 ##   1.00   0.00112  1.008  0.248     0.458 ##   1.00   0.01120  0.902  0.252     0.442 ##   1.00   0.11195  0.831  0.282     0.446 ## ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were alpha = 0.55 and lambda = 0.112.</code></td>
</tr>
<tr class="odd">
<td align="right">## Binary Logistic Regression (BLR)</td>
</tr>
<tr class="even">
<td align="right">Binary Logistic Regression (BLR) can models fault-proneness as follows</td>
</tr>
<tr class="odd">
<td align="right"><span class="math display">\[fp(X) = \frac{e^{logit()}}{1 + e^{logit(X)}}\]</span></td>
</tr>
<tr class="even">
<td align="right">where the simplest form for logit is:</td>
</tr>
<tr class="odd">
<td align="right"><span class="math inline">\(logit(X) = c_{0} + c_{1}X\)</span></td>
</tr>
<tr class="even">
<td align="right">```r
jdt &lt;- read.csv(“./datasets/defectPred/BPD/single-version-ck-oo-EclipseJDTCore.csv,” sep=“;”)</td>
</tr>
<tr class="odd">
<td align="right"># Caret
library(caret)</td>
</tr>
<tr class="even">
<td align="right"># Convert the response variable into a boolean variable (0/1)
jdt<span class="math inline">\(bugs[jdt\)</span>bugs&gt;=1]&lt;-1</td>
</tr>
<tr class="odd">
<td align="right">cbo &lt;- jdt<span class="math inline">\(cbo bugs &lt;- jdt\)</span>bugs</td>
</tr>
<tr class="even">
<td align="right"># Split data into training and test datasets
jdt2 = data.frame(cbo, bugs)
inTrain &lt;- createDataPartition(y=jdt2$bugs,p=.8,list=FALSE)
jdtTrain &lt;- jdt2[inTrain,]
jdtTest &lt;- jdt2[-inTrain,]
```</td>
</tr>
<tr class="odd">
<td align="right">BLR models fault-proneness are as follows <span class="math inline">\(fp(X) = \frac{e^{logit()}}{1 + e^{logit(X)}}\)</span></td>
</tr>
<tr class="even">
<td align="right">where the simplest form for logit is <span class="math inline">\(logit(X) = c_{0} + c_{1}X\)</span></td>
</tr>
<tr class="odd">
<td align="right">```r
# logit regression
# glmLogit &lt;- train (bugs ~ ., data=jdt.train, method=“glm,” family=binomial(link = logit))</td>
</tr>
<tr class="even">
<td align="right">glmLogit &lt;- glm (bugs ~ ., data=jdtTrain, family=binomial(link = logit))
summary(glmLogit)
```</td>
</tr>
<tr class="odd">
<td align="right"><code>## ## Call: ## glm(formula = bugs ~ ., family = binomial(link = logit), data = jdtTrain) ## ## Deviance Residuals: ##    Min      1Q  Median      3Q     Max ## -3.654  -0.591  -0.515  -0.471   2.150 ## ## Coefficients: ##             Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.20649    0.13900  -15.87   &lt;2e-16 *** ## cbo          0.06298    0.00765    8.23   &lt;2e-16 *** ## --- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ##     Null deviance: 807.98  on 797  degrees of freedom ## Residual deviance: 691.80  on 796  degrees of freedom ## AIC: 695.8 ## ## Number of Fisher Scoring iterations: 5</code></td>
</tr>
<tr class="even">
<td align="right">Predict a single point:</td>
</tr>
<tr class="odd">
<td align="right"><code>r newData = data.frame(cbo = 3) predict(glmLogit, newData, type = "response")</code></td>
</tr>
<tr class="even">
<td align="right"><code>##     1 ## 0.117</code></td>
</tr>
<tr class="odd">
<td align="right">Draw the results, modified from:
<a href="http://www.shizukalab.com/toolkits/plotting-logistic-regression-in-r" class="uri">http://www.shizukalab.com/toolkits/plotting-logistic-regression-in-r</a></td>
</tr>
<tr class="even">
<td align="right">```r
results &lt;- predict(glmLogit, jdtTest, type = “response”)</td>
</tr>
<tr class="odd">
<td align="right">range(jdtTrain$cbo)
```</td>
</tr>
<tr class="even">
<td align="right"><code>## [1]   0 156</code></td>
</tr>
<tr class="odd">
<td align="right"><code>r range(results)</code></td>
</tr>
<tr class="even">
<td align="right"><code>## [1] 0.0992 0.9993</code></td>
</tr>
<tr class="odd">
<td align="right"><code>r plot(jdt2$cbo,jdt2$bugs) curve(predict(glmLogit, data.frame(cbo=x), type = "response"),add=TRUE)</code></td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-78-1.png" width="672" /></td>
</tr>
<tr class="odd">
<td align="right"><code>r # points(jdtTrain$cbo,fitted(glmLogit))</code></td>
</tr>
<tr class="even">
<td align="right">Another type of graph:</td>
</tr>
<tr class="odd">
<td align="right"><code>r library(popbio)</code></td>
</tr>
<tr class="even">
<td align="right"><code>## ## Attaching package: 'popbio'</code></td>
</tr>
<tr class="odd">
<td align="right"><code>## The following object is masked from 'package:caret': ## ##     sensitivity</code></td>
</tr>
<tr class="even">
<td align="right"><code>r logi.hist.plot(jdt2$cbo,jdt2$bugs,boxp=FALSE,type="hist",col="gray")</code></td>
</tr>
<tr class="odd">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-79-1.png" width="672" /></td>
</tr>
<tr class="even">
<td align="right">## The caret package</td>
</tr>
<tr class="odd">
<td align="right">There are hundreds of packages to perform classification task in R, but many of those can be used throught the ‘caret’ package which helps with many of the data mining process task as described next.</td>
</tr>
<tr class="even">
<td align="right">The caret package<a href="http://topepo.github.io/caret/">http://topepo.github.io/caret/</a> provides a unified interface for modeling and prediction with around 150 different models with tools for:</td>
</tr>
<tr class="odd">
<td align="right">+ data splitting</td>
</tr>
<tr class="even">
<td align="right">+ pre-processing</td>
</tr>
<tr class="odd">
<td align="right">+ feature selection</td>
</tr>
<tr class="even">
<td align="right">+ model tuning using resampling</td>
</tr>
<tr class="odd">
<td align="right">+ variable importance estimation, etc.</td>
</tr>
<tr class="even">
<td align="right">Website: <a href="http://caret.r-forge.r-project.org">http://caret.r-forge.r-project.org</a></td>
</tr>
<tr class="odd">
<td align="right">JSS Paper: <a href="www.jstatsoft.org/v28/i05/paper">www.jstatsoft.org/v28/i05/paper</a></td>
</tr>
<tr class="even">
<td align="right">Book: <a href="http://AppliedPredictiveModeling.com/">Applied Predictive Modeling</a></td>
</tr>
<tr class="odd">
<td align="right"><!--chapter:end:400_basicModelBuildingSupervised.Rmd--></td>
</tr>
<tr class="even">
<td align="right"># Regression {#regression}</td>
</tr>
<tr class="odd">
<td align="right">## Linear Regression modeling</td>
</tr>
<tr class="even">
<td align="right">- <em>Linear Regression</em> is one of the oldest and most known predictive methods. As its name says, the idea is to try to fit a linear equation between a dependent variable and an independent, or explanatory, variable. The idea is that the independent variable <span class="math inline">\(x\)</span> is something the experimenter controls and the dependent variable <span class="math inline">\(y\)</span> is something that the experimenter measures. The line is used to predict the value of <span class="math inline">\(y\)</span> for a known value of <span class="math inline">\(x\)</span>. The variable <span class="math inline">\(x\)</span> is the predictor variable and <span class="math inline">\(y\)</span> the response variable.</td>
</tr>
<tr class="odd">
<td align="right">- <em>Multiple linear regression</em> uses 2 or more independent variables for building a model. See <a href="https://www.wikipedia.org/wiki/Linear_regression" class="uri">https://www.wikipedia.org/wiki/Linear_regression</a>.</td>
</tr>
<tr class="even">
<td align="right">- First proposed many years ago but still very useful…</td>
</tr>
<tr class="odd">
<td align="right"><img src="figures/galton.png" alt="Galton Data" /></td>
</tr>
<tr class="even">
<td align="right">- The equation takes the form <span class="math inline">\(\hat{y}=b_0+b_1 * x\)</span>
- The method used to choose the values <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> is to minimize the sum of the squares of the residual errors.</td>
</tr>
<tr class="odd">
<td align="right">### Regression: Galton Data</td>
</tr>
<tr class="even">
<td align="right">Not related to Software Engineering but …</td>
</tr>
<tr class="odd">
<td align="right"><code>r library(UsingR) data(galton) par(mfrow=c(1,2)) hist(galton$child,col="blue",breaks=100) hist(galton$parent,col="blue",breaks=100)</code></td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-80-1.png" width="672" /></td>
</tr>
<tr class="odd">
<td align="right"><code>r plot(galton$parent,galton$child,pch=1,col="blue", cex=0.4) lm1 &lt;- lm(galton$child ~ galton$parent) lines(galton$parent,lm1$fitted,col="red",lwd=3) plot(galton$parent,lm1$residuals,col="blue",pch=1, cex=0.4) abline(c(0,0),col="red",lwd=3)</code></td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-80-2.png" width="672" /></td>
</tr>
<tr class="odd">
<td align="right"><code>r qqnorm(galton$child)</code></td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-80-3.png" width="672" /></td>
</tr>
<tr class="odd">
<td align="right">### Simple Linear Regression</td>
</tr>
<tr class="even">
<td align="right">- Given two variables <span class="math inline">\(Y\)</span> (response) and <span class="math inline">\(X\)</span> (predictor), the assumption is that there is an approximate (<span class="math inline">\(\approx\)</span>) <em>linear</em> relation between those variables.
- The mathematical model of the observed data is described as (for the case of simple linear regression):
<span class="math display">\[ Y \approx \beta_0 + \beta_1 X\]</span></td>
</tr>
<tr class="odd">
<td align="right">- the parameter <span class="math inline">\(\beta_0\)</span> is named the <em>intercept</em> and <span class="math inline">\(\beta_1\)</span> is the <em>slope</em>
- Each observation can be modeled as</td>
</tr>
<tr class="even">
<td align="right"><span class="math display">\[y_i = \beta_0 + \beta_1 x_i + \epsilon_i;
\epsilon_i \sim N(0,\sigma^2)\]</span>
- <span class="math inline">\(\epsilon_i\)</span> is the <em>error</em>
- This means that the variable <span class="math inline">\(y\)</span> is normally distributed:
<span class="math display">\[ y_i \sim N( \beta_0 + \beta_1 x_i, \sigma^2) \]</span></td>
</tr>
<tr class="odd">
<td align="right">- The <em>predictions</em> or <em>estimations</em> of this model are obtained by a linear equation of the form <span class="math inline">\(\hat{Y}=\hat{\beta_0} + \hat{\beta}_1X\)</span>, that is, each new prediction is computed with
<span class="math display">\[\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_i \]</span>.
- The actual parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are unknown
- The parameters <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> of the linear equation can be estimated with different methods.</td>
</tr>
<tr class="even">
<td align="right">### Least Squares</td>
</tr>
<tr class="odd">
<td align="right">- One of the most used methods for computing <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> is the criterion of “least squares” minimization.
- The data is composed of <span class="math inline">\(n\)</span> pairs of observations <span class="math inline">\((x_i, y_i)\)</span>
- Given an observation <span class="math inline">\(y_i\)</span> and its corresponding estimation <span class="math inline">\(\hat{y_i})\)</span> the <em>residual</em> <span class="math inline">\(e_i\)</span> is defined as <span class="math display">\[e_i= y_i - \hat{y_i}\]</span>
- the Residual Sum of Squares is defined as <span class="math display">\[RSS=e_1^2+\dots + e_i^2+\dots+e_n^2\]</span>
- the Least Squares Approach minimizes the RSS
- as result of that minimizitation, it can be obtained, by means of calculus, the estimation of <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> as <span class="math display">\[\hat{\beta}_1=\frac{\sum_{i=1}^{n}{(x_i-\bar{x})(y_i-\bar{y})}}{\sum_{i=1}^{n}(x_i-\bar{x})^2}\]</span> and <span class="math display">\[\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x} \]</span> where <span class="math inline">\(\bar{y}\)</span> and <span class="math inline">\(\bar{x}\)</span> are the sample means.
- the variance <span class="math inline">\(\sigma^2\)</span> is estimated by
<span class="math display">\[\hat\sigma^2 = {RSS}/{(n-2)}\]</span> where n is the number of observations
- The <em>Residual Standard Error</em> is defined as <span class="math display">\[RSE = \sqrt{{RSS}/{(n-2)}}\]</span>
- The equation <span class="math display">\[ Y = \beta_0 + \beta_1 X + \epsilon\]</span> defines the linear model, i.e., the <em>population regression line</em>
- The <em>least squares line</em> is <span class="math inline">\(\hat{Y}=\hat{\beta_0} + \hat{\beta}_1X\)</span>
- <em>Confidence intervals</em> are computed using the <em>standard errors</em> of the intercept and the slope.
- The <span class="math inline">\(95\%\)</span> confidence interval for the slope is computed as <span class="math display">\[[\hat{\beta}_1 - 2 \cdot SE(\hat{\beta}_1), \hat{\beta}_1+SE(\hat{\beta}_1)]\]</span>
- where <span class="math display">\[ SE(\hat{\beta}_1) = \sqrt{\frac{\sigma^2}{\sum_{i=1}^{n}(x_i-\bar{x})^2}}\]</span></td>
</tr>
<tr class="even">
<td align="right">### Linear regression in R</td>
</tr>
<tr class="odd">
<td align="right">The following are the basic commands in R:</td>
</tr>
<tr class="even">
<td align="right">- The basic function is <code>lm()</code>, that returns an object with the model.
- Other commands: <code>summary</code> prints out information about the regression, <code>coef</code> gives the coefficients for the linear model, <code>fitted</code> gives the predictd value of <span class="math inline">\(y\)</span> for each value of <span class="math inline">\(x\)</span>, <code>residuals</code> contains the differences between observed and fitted values.
- <a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/predict.lm.html"><code>predict</code></a> will generate predicted values of the response for the values of the explanatory variable.</td>
</tr>
<tr class="odd">
<td align="right">## Linear Regression Diagnostics</td>
</tr>
<tr class="even">
<td align="right">- Several plots help to evaluate the suitability of the linear regression
+ <em>Residuals vs fitted</em>: The residuals should be randomly distributed around the horizontal line representing a residual error of zero; that is, there should not be a distinct trend in the distribution of points.
+ <em>Standard Q-Q plot</em>: residual errors are normally distributed
+ <em>Square root of the standardized residuals vs the fitted values</em>: there should be no obvious trend. This plot is similar to the residuals versus fitted values plot, but it uses the square root of the standardized residuals.
+ <em>Leverage</em>: measures the importance of each point in determining the regression result. Smaller values means that removing the observation has little effect on the regression result.</td>
</tr>
<tr class="odd">
<td align="right">### Simulation example</td>
</tr>
<tr class="even">
<td align="right">#### Simulate a dataset</td>
</tr>
<tr class="odd">
<td align="right">```r
set.seed(3456)
# equation is y = -6.6 + 0.13 x +e
# range x 100,400
a &lt;- -6.6
b &lt;- 0.13
num_obs &lt;- 60
xmin &lt;- 100
xmax &lt;- 400
x &lt;- sample(seq(from=xmin, to = xmax, by =1), size= num_obs, replace=FALSE)</td>
</tr>
<tr class="even">
<td align="right">sderror &lt;- 9 # sigma for the error term in the model
e &lt;- rnorm(num_obs, 0, sderror)</td>
</tr>
<tr class="odd">
<td align="right">y &lt;- a + b * x + e</td>
</tr>
<tr class="even">
<td align="right">newlm &lt;- lm(y~x)
summary(newlm)
```</td>
</tr>
<tr class="odd">
<td align="right"><code>## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ##     Min      1Q  Median      3Q     Max ## -26.518  -5.645   0.363   5.695  18.392 ## ## Coefficients: ##             Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept)  -7.9060     3.3922   -2.33    0.023 * ## x             0.1331     0.0132   10.05  2.6e-14 *** ## --- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 8.48 on 58 degrees of freedom ## Multiple R-squared:  0.635,  Adjusted R-squared:  0.629 ## F-statistic:  101 on 1 and 58 DF,  p-value: 2.57e-14</code></td>
</tr>
<tr class="even">
<td align="right">```r
cfa1 &lt;- coef(newlm)[1]
cfb2 &lt;- coef(newlm)[2]
plot(x,y, xlab=“x axis,” ylab= “y axis,” xlim = c(xmin, xmax), ylim = c(0,60), sub = “Line in black is the actual model”)
title(main = paste(“Line in blue is the Regression Line for,” num_obs, ” points.”))</td>
</tr>
<tr class="odd">
<td align="right">abline(a = cfa1, b = cfb2, col= “blue,” lwd=3)
abline(a = a, b = b, col= “black,” lwd=1) #original line
```</td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-81-1.png" width="672" /></td>
</tr>
<tr class="odd">
<td align="right">##### Subset a set of points from the same sample</td>
</tr>
<tr class="even">
<td align="right">```r
# sample from the same x to compare least squares lines
# change the denominator in newsample to see how the least square lines changes accordingly.
newsample &lt;- as.integer(num_obs/8) # number of pairs x,y</td>
</tr>
<tr class="odd">
<td align="right">idxs_x1 &lt;- sample(1:num_obs, size = newsample, replace = FALSE) #sample indexes
x1 &lt;- x[idxs_x1]
e1 &lt;- e[idxs_x1]
y1 &lt;- a + b * x1 + e1
xy_obs &lt;- data.frame(x1, y1)
names(xy_obs) &lt;- c(“x_obs,” “y_obs”)</td>
</tr>
<tr class="even">
<td align="right">newlm1 &lt;- lm(y1~x1)
summary(newlm1)
```</td>
</tr>
<tr class="odd">
<td align="right"><code>## ## Call: ## lm(formula = y1 ~ x1) ## ## Residuals: ##      1      2      3      4      5      6      7 ##  3.968 -8.537  3.141 -8.723  7.294 -0.235  3.092 ## ## Coefficients: ##             Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept)   2.9107     7.7166    0.38    0.722 ## x1            0.0913     0.0328    2.79    0.039 * ## --- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 6.89 on 5 degrees of freedom ## Multiple R-squared:  0.609,  Adjusted R-squared:  0.53 ## F-statistic: 7.77 on 1 and 5 DF,  p-value: 0.0385</code></td>
</tr>
<tr class="even">
<td align="right">```r
cfa21 &lt;- coef(newlm1)[1]
cfb22 &lt;- coef(newlm1)[2]</td>
</tr>
<tr class="odd">
<td align="right">plot(x1,y1, xlab=“x axis,” ylab= “y axis,” xlim = c(xmin, xmax), ylim = c(0,60))
title(main = paste(“New line in red with,” newsample, ” points in sample”))</td>
</tr>
<tr class="even">
<td align="right">abline(a = a, b = b, col= “black,” lwd=1) # True line
abline(a = cfa1, b = cfb2, col= “blue,” lwd=1) #sample
abline(a = cfa21, b = cfb22, col= “red,” lwd=2) #new line
```</td>
</tr>
<tr class="odd">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-82-1.png" width="672" /></td>
</tr>
<tr class="even">
<td align="right">##### Compute a confidence interval on the original sample regression line</td>
</tr>
<tr class="odd">
<td align="right">```r
newx &lt;- seq(xmin, xmax)
ypredicted &lt;- predict(newlm, newdata=data.frame(x=newx), interval= “confidence,” level= 0.90, se = TRUE)</td>
</tr>
<tr class="even">
<td align="right">plot(x,y, xlab=“x axis,” ylab= “y axis,” xlim = c(xmin, xmax), ylim = c(0,60))
# points(x1, fitted(newlm1))
abline(newlm)</td>
</tr>
<tr class="odd">
<td align="right">lines(newx,ypredicted<span class="math inline">\(fit[,2],col=&quot;red&quot;,lty=2) lines(newx,ypredicted\)</span>fit[,3],col=“red,”lty=2)
```</td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-83-1.png" width="672" /></td>
</tr>
<tr class="odd">
<td align="right"><code>r # Plot the residuals or errors ypredicted_x &lt;- predict(newlm, newdata=data.frame(x=x)) plot(x,y, xlab="x axis", ylab= "y axis", xlim = c(xmin, xmax), ylim = c(0,60), sub = "", pch=19, cex=0.75) title(main = paste("Residuals or errors", num_obs, " points.")) abline(newlm) segments(x, y, x, ypredicted_x)</code></td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-83-2.png" width="672" /></td>
</tr>
<tr class="odd">
<td align="right">##### Take another sample from the model and explore</td>
</tr>
<tr class="even">
<td align="right">```r
# equation is y = -6.6 + 0.13 x +e
# range x 100,400
num_obs &lt;- 35
xmin &lt;- 100
xmax &lt;- 400
x3 &lt;- sample(seq(from=xmin, to = xmax, by =1), size= num_obs, replace=FALSE)
sderror &lt;- 14 # sigma for the error term in the model
e3 &lt;- rnorm(num_obs, 0, sderror)</td>
</tr>
<tr class="odd">
<td align="right">y3 &lt;- a + b * x3 + e3</td>
</tr>
<tr class="even">
<td align="right">newlm3 &lt;- lm(y3~x3)
summary(newlm3)
```</td>
</tr>
<tr class="odd">
<td align="right"><code>## ## Call: ## lm(formula = y3 ~ x3) ## ## Residuals: ##    Min     1Q Median     3Q    Max ## -40.87  -9.20  -2.28  12.08  47.17 ## ## Coefficients: ##             Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept)  -0.9284     8.7458   -0.11   0.9161 ## x3            0.1193     0.0345    3.45   0.0015 ** ## --- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 17.2 on 33 degrees of freedom ## Multiple R-squared:  0.266,  Adjusted R-squared:  0.243 ## F-statistic: 11.9 on 1 and 33 DF,  p-value: 0.00153</code></td>
</tr>
<tr class="even">
<td align="right">```r
cfa31 &lt;- coef(newlm3)[1]
cfb32 &lt;- coef(newlm3)[2]
plot(x3,y3, xlab=“x axis,” ylab= “y axis,” xlim = c(xmin, xmax), ylim = c(0,60))
title(main = paste(“Line in red is the Regression Line for,” num_obs, ” points.”))
abline(a = cfa31, b = cfb32, col= “red,” lwd=3)
abline(a = a, b = b, col= “black,” lwd=2) #original line
abline(a = cfa1, b = cfb2, col= “blue,” lwd=1) #first sample</td>
</tr>
<tr class="odd">
<td align="right"># confidence intervals for the new sample</td>
</tr>
<tr class="even">
<td align="right">newx &lt;- seq(xmin, xmax)
ypredicted &lt;- predict(newlm3, newdata=data.frame(x3=newx), interval= “confidence,” level= 0.90, se = TRUE)</td>
</tr>
<tr class="odd">
<td align="right">lines(newx,ypredicted<span class="math inline">\(fit[,2],col=&quot;red&quot;,lty=2, lwd=2) lines(newx,ypredicted\)</span>fit[,3],col=“red,”lty=2, lwd=2)
```</td>
</tr>
<tr class="even">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-84-1.png" width="672" /></td>
</tr>
<tr class="odd">
<td align="right">### Diagnostics fro assessing the regression line</td>
</tr>
<tr class="even">
<td align="right">#### Residual Standard Error
- It gives us an idea of the typical or average error of the model. It is the estimated standard deviation of the residuals.</td>
</tr>
<tr class="odd">
<td align="right">#### <span class="math inline">\(R^2\)</span> statistic
- This is the proportion of variability in the data that is explained by the model. Best values are those close to 1.</td>
</tr>
<tr class="even">
<td align="right">## Multiple Linear Regression</td>
</tr>
<tr class="odd">
<td align="right">### Partial Least Squares
- If several predictors are highly correlated, the least squares approach has high variability.
- PLS finds linear combinations of the predictors, that are called <em>components</em> or <em>latent</em> variables.</td>
</tr>
<tr class="even">
<td align="right">## Linear regression in Software Effort estimation</td>
</tr>
<tr class="odd">
<td align="right">Fitting a linear model to log-log
- the predictive power equation is <span class="math inline">\(y= e^{b_0}*x^{b_1}\)</span>, ignoring the bias corrections. Note: depending how the error term behaves we could try another general linear model (GLM) or other model that does not rely on the normality of the residuals (quantile regression, etc.)
- First, we are fitting the model to the whole dataset. But it is not the right way to do it, because of overfitting.</td>
</tr>
<tr class="even">
<td align="right"><code>r library(foreign) china &lt;- read.arff("./datasets/effortEstimation/china.arff") china_size &lt;- china$AFP summary(china_size)</code></td>
</tr>
<tr class="odd">
<td align="right"><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. ##       9     100     215     487     438   17518</code></td>
</tr>
<tr class="even">
<td align="right"><code>r china_effort &lt;- china$Effort summary(china_effort)</code></td>
</tr>
<tr class="odd">
<td align="right"><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. ##      26     704    1829    3921    3826   54620</code></td>
</tr>
<tr class="even">
<td align="right"><code>r par(mfrow=c(1,2)) hist(china_size, col="blue", xlab="Adjusted Function Points", main="Distribution of AFP") hist(china_effort, col="blue",xlab="Effort", main="Distribution of Effort")</code></td>
</tr>
<tr class="odd">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-85-1.png" width="672" /></td>
</tr>
<tr class="even">
<td align="right"><code>r boxplot(china_size) boxplot(china_effort)</code></td>
</tr>
<tr class="odd">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-85-2.png" width="672" /></td>
</tr>
<tr class="even">
<td align="right"><code>r qqnorm(china_size) qqline(china_size) qqnorm(china_effort) qqline(china_effort)</code></td>
</tr>
<tr class="odd">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-85-3.png" width="672" /></td>
</tr>
<tr class="even">
<td align="right">Applying the <code>log</code> function (it computes natural logarithms, base <span class="math inline">\(e\)</span>)</td>
</tr>
<tr class="odd">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-86-1.png" width="672" /><img src="DASE_files/figure-html/unnamed-chunk-86-2.png" width="672" /></td>
</tr>
<tr class="even">
<td align="right"><code>r linmodel_logchina &lt;- lm(logchina_effort ~ logchina_size) par(mfrow=c(1,1)) plot(logchina_size, logchina_effort) abline(linmodel_logchina, lwd=3, col=3)</code></td>
</tr>
<tr class="odd">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-87-1.png" width="672" /></td>
</tr>
<tr class="even">
<td align="right"><code>r par(mfrow=c(1,2)) plot(linmodel_logchina, ask = FALSE)</code></td>
</tr>
<tr class="odd">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-87-2.png" width="672" /><img src="DASE_files/figure-html/unnamed-chunk-87-3.png" width="672" /></td>
</tr>
<tr class="even">
<td align="right"><code>r linmodel_logchina</code></td>
</tr>
<tr class="odd">
<td align="right"><code>## ## Call: ## lm(formula = logchina_effort ~ logchina_size) ## ## Coefficients: ##   (Intercept)  logchina_size ##         3.301          0.768</code></td>
</tr>
<tr class="even">
<td align="right">## References</td>
</tr>
<tr class="odd">
<td align="right">- The New Statistics with R, Andy Hector, 2015
- An Introduction to R, W.N. Venables and D.M. Smith and the R Development Core Team
- Practical Data Science with R, Nina Zumel and John Mount
- G. James et al, An Introduction to Statistical Learning with Applications in R, Springer, 2013</td>
</tr>
<tr class="even">
<td align="right"><!--chapter:end:417_regression.Rmd--></td>
</tr>
<tr class="odd">
<td align="right"># (PART) Unsupervised Models {-}</td>
</tr>
<tr class="even">
<td align="right"># Unsupervised or Descriptive modeling</td>
</tr>
<tr class="odd">
<td align="right">From the descriptive (unsupervised) point of view, patterns are found to predict future behaviour or estimate. This include association rules, clustering, or tree clustering which aim at grouping together objects (e.g., animals) into successively larger clusters, using some measure of similarity or distance. The dataset will be as the previous table without the <span class="math inline">\(C\)</span> class attribute</td>
</tr>
<tr class="even">
<td align="right">| Att<sub>1</sub>| | Att<sub>n</sub> |
|——-|—–| ——-|
| a<sub>11</sub> | … | a<sub>1n</sub> |
| a<sub>21</sub> | … | a<sub>2n</sub> |
| … | … | … |
| a<sub>m1</sub> | … | a<sub>mn</sub> |</td>
</tr>
<tr class="odd">
<td align="right">## Clustering</td>
</tr>
<tr class="even">
<td align="right">```r
library(foreign)
library(fpc)</td>
</tr>
<tr class="odd">
<td align="right">kc1 &lt;- read.arff(“./datasets/defectPred/D1/KC1.arff”)</td>
</tr>
<tr class="even">
<td align="right"># Split into training and test datasets
set.seed(1)
ind &lt;- sample(2, nrow(kc1), replace = TRUE, prob = c(0.7, 0.3))
kc1.train &lt;- kc1[ind==1, ]
kc1.test &lt;- kc1[ind==2, ]</td>
</tr>
<tr class="odd">
<td align="right"># No class
kc1.train$Defective &lt;- NULL</td>
</tr>
<tr class="even">
<td align="right">ds &lt;- dbscan(kc1.train, eps = 0.42, MinPts = 5)</td>
</tr>
<tr class="odd">
<td align="right">kc1.kmeans &lt;- kmeans(kc1.train, 2)
```</td>
</tr>
<tr class="even">
<td align="right">### k-Means</td>
</tr>
<tr class="odd">
<td align="right"><code>r library(reshape, quietly=TRUE) library(graphics) kc1kmeans &lt;- kmeans(sapply(na.omit(kc1.train), rescaler, "range"), 10) #plot(kc1kmeans, col = kc1kmeans$cluster) #points(kc1kmeans$centers, col = 1:5, pch = 8)</code></td>
</tr>
<tr class="even">
<td align="right">## Association rules</td>
</tr>
<tr class="odd">
<td align="right">```r
library(arules)</td>
</tr>
<tr class="even">
<td align="right"># x &lt;- as.numeric(kc1$LOC_TOTAL)
# str(x)
# summary(x)
# hist(x, breaks=30, main=“LoC Total”)
# xDisc &lt;- discretize(x, categories=5)
# table(xDisc)</td>
</tr>
<tr class="odd">
<td align="right">for(i in 1:21) kc1[,i] &lt;- discretize(kc1[,i], method = “interval,” breaks = 5)</td>
</tr>
<tr class="even">
<td align="right">rules &lt;- apriori(kc1,
parameter = list(minlen=3, supp=0.05, conf=0.35),
appearance = list(rhs=c(“Defective=Y”),
default=“lhs”),
control = list(verbose=F))</td>
</tr>
<tr class="odd">
<td align="right">#rules &lt;- apriori(kc1,
# parameter = list(minlen=2, supp=0.05, conf=0.3),
# appearance = list(rhs=c(“Defective=Y,” “Defective=N”),
# default=“lhs”))</td>
</tr>
<tr class="even">
<td align="right">inspect(rules)
```</td>
</tr>
<tr class="odd">
<td align="right"><code>##     lhs                               rhs           support confidence coverage lift count ## [1] {HALSTEAD_CONTENT=[38.6,77.2), ##      HALSTEAD_LEVEL=[0,0.4)}       =&gt; {Defective=Y}  0.0539      0.370    0.146 2.39   113 ## [2] {LOC_CODE_AND_COMMENT=[0,2.4), ##      HALSTEAD_CONTENT=[38.6,77.2)} =&gt; {Defective=Y}  0.0525      0.377    0.139 2.43   110 ## [3] {LOC_CODE_AND_COMMENT=[0,2.4), ##      HALSTEAD_CONTENT=[38.6,77.2), ##      HALSTEAD_LEVEL=[0,0.4)}       =&gt; {Defective=Y}  0.0515      0.374    0.138 2.41   108</code></td>
</tr>
<tr class="even">
<td align="right"><code>r library(arulesViz) plot(rules)</code></td>
</tr>
<tr class="odd">
<td align="right"><img src="DASE_files/figure-html/unnamed-chunk-90-1.png" width="672" /></td>
</tr>
<tr class="even">
<td align="right"><!--chapter:end:420_basicModelBuildingUnsupervised.Rmd--></td>
</tr>
<tr class="odd">
<td align="right"># (PART) Evaluation {-}</td>
</tr>
<tr class="even">
<td align="right"># Evaluation of Models</td>
</tr>
<tr class="odd">
<td align="right">Once we obtain the model with the training data, we need to evaluate it with some new data (testing data).</td>
</tr>
<tr class="even">
<td align="right">&gt; <strong>No Free Lunch theorem</strong>
&gt; In the absence of any knowledge about the prediction problem, no model
&gt; can be said to be uniformly better than any other</td>
</tr>
<tr class="odd">
<td align="right">## Building and Validating a Model</td>
</tr>
<tr class="even">
<td align="right">We cannot use the the same data for training and testing (it is like evaluating a student with the exercises previously solved in class, the student’s marks will be “optimistic” and we do not know about student capability to generalise the learned concepts).</td>
</tr>
<tr class="odd">
<td align="right">Therefore, we should, at a minimum, divide the dataset into <em>training</em> and <em>testing</em>, learn the model with the training data and test it with the rest of data as explained next.</td>
</tr>
<tr class="even">
<td align="right">### Holdout approach</td>
</tr>
<tr class="odd">
<td align="right"><strong>Holdout approach</strong> consists of dividing the dataset into <em>training</em> (typically approx. 2/3 of the data) and <em>testing</em> (approx 1/3 of the data).
+ Problems: Data can be skewed, missing classes, etc. if randomly divided. Stratification ensures that each class is represented with approximately equal proportions (e.g., if data contains approximately 45% of positive cases, the training and testing datasets should maintain similar proportion of positive cases).</td>
</tr>
<tr class="even">
<td align="right">Holdout estimate can be made more reliable by repeating the process with different subsamples (repeated holdout method).</td>
</tr>
<tr class="odd">
<td align="right">The error rates on the different iterations are averaged (overall error rate).</td>
</tr>
<tr class="even">
<td align="right">- Usually, part of the data points are used for building the model and the remaining points are used for validating the model. There are several approaches to this process.
- <em>Validation Set approach</em>: it is the simplest method. It consists of randomly dividing the available set of observations into two parts, a <em>training set</em> and a <em>validation set</em> or hold-out
set. Usually 2/3 of the data points are used for training and 1/3 is used for testing purposes.</td>
</tr>
<tr class="odd">
<td align="right"><img src="figures/validation.png" alt="Hold out validation" /></td>
</tr>
<tr class="even">
<td align="right">### Cross Validation (CV)</td>
</tr>
<tr class="odd">
<td align="right"><em>k-fold Cross-Validation</em> involves randomly dividing the set of observations into <span class="math inline">\(k\)</span> groups, or folds, of approximately equal size. One fold is treated as a validation set and the method is trained on the remaining <span class="math inline">\(k-1\)</span> folds. This procedure is repeated <span class="math inline">\(k\)</span> times. If <span class="math inline">\(k\)</span> is equal to <span class="math inline">\(n\)</span> we are in the previous method.</td>
</tr>
<tr class="even">
<td align="right">+ 1st step: split dataset (<span class="math inline">\(\cal D\)</span>) into <span class="math inline">\(k\)</span> subsets of approximately equal size <span class="math inline">\(C_1, \dots, C_k\)</span></td>
</tr>
<tr class="odd">
<td align="right">+ 2nd step: we construct a dataset <span class="math inline">\(D_i = D-C_i\)</span> used for training and test the accuracy of the classifier <span class="math inline">\(D_i\)</span> on <span class="math inline">\(C_i\)</span> subset for testing</td>
</tr>
<tr class="even">
<td align="right">Having done this for all <span class="math inline">\(k\)</span> we estimate the accuracy of the method by averaging the accuracy over the <span class="math inline">\(k\)</span> cross-validation trials</td>
</tr>
<tr class="odd">
<td align="right"><img src="figures/kfold.png" alt="k-fold" /></td>
</tr>
<tr class="even">
<td align="right">### Leave-One-Out Cross-Validation (LOO-CV)</td>
</tr>
<tr class="odd">
<td align="right">- <em>Leave-One-Out Cross-Validation</em> (LOO-CV): This is a special case of CV. Instead of creating two subsets for training and testing, a single observation is used for the validation set, and the remaining observations make up the training set. This approach is repeated <span class="math inline">\(n\)</span> times (the total number of observations) and the estimate for the test mean squared error is the average of the <span class="math inline">\(n\)</span> test estimates.</td>
</tr>
<tr class="even">
<td align="right"><img src="figures/leaveone.png" alt="Leave One Out" /></td>
</tr>
<tr class="odd">
<td align="right"><!--chapter:end:430_evaluation.Rmd--></td>
</tr>
</tbody>
</table>
<p>output:
html_document: default
pdf_document: default
—
## Evaluation of Classification Models</p>
<p>The confusion matrix (which can be extended to multiclass problems) is a table that presents the results of a classification algorithm. The following table shows the possible outcomes for binary classification problems:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th><span class="math inline">\(Act Pos\)</span></th>
<th><span class="math inline">\(Act Neg\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(Pred Pos\)</span></td>
<td><span class="math inline">\(TP\)</span></td>
<td><span class="math inline">\(FP\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(Pred Neg\)</span></td>
<td><span class="math inline">\(FN\)</span></td>
<td><span class="math inline">\(TN\)</span></td>
</tr>
</tbody>
</table>
<p>where <em>True Positives</em> (<span class="math inline">\(TP\)</span>) and <em>True Negatives</em> (<span class="math inline">\(TN\)</span>) are respectively the number of positive and negative instances correctly classified, <em>False Positives</em> (<span class="math inline">\(FP\)</span>) is the number of negative instances misclassified as positive (also called Type I errors), and <em>False Negatives</em> (<span class="math inline">\(FN\)</span>) is the number of positive instances misclassified as negative (Type II errors).</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix in Wikipedia</a></li>
</ul>
<p>From the confusion matrix, we can calculate:</p>
<ul>
<li><p><em>True positive rate</em>, or <em>recall </em> (<span class="math inline">\(TP_r = recall = TP/TP+FN\)</span>) is the proportion of positive cases correctly classified as belonging to the positive class.</p></li>
<li><p><em>False negative rate</em> (<span class="math inline">\(FN_r=FN/TP+FN\)</span>) is the proportion of positive cases misclassified as belonging to the negative class.</p></li>
<li><p><em>False positive rate</em> (<span class="math inline">\(FP_r=FP/FP+TN\)</span>) is the proportion of negative cases misclassified as belonging to the positive class.</p></li>
<li><p><em>True negative rate</em> (<span class="math inline">\(TN_r=TN/FP+TN\)</span>) is the proportion of negative cases correctly classified as belonging to the negative class.</p></li>
</ul>
<p>There is a trade-off between <span class="math inline">\(FP_r\)</span> and <span class="math inline">\(FN_r\)</span> as the objective is minimize both metrics (or conversely, maximize the true negative and positive rates). It is possible to combine both metrics into a single figure, predictive <span class="math inline">\(accuracy\)</span>:</p>
<p><span class="math display">\[accuracy = \frac{TP + TN}{TP + TN + FP + FN}\]</span></p>
<p>to measure performance of classifiers (or the complementary value, the <em>error rate</em> which is defined as <span class="math inline">\(1-accuracy\)</span>)</p>
<ul>
<li><p>Precision, fraction of relevant instances among the retrieved instances, <span class="math display">\[\frac{TP}{TP+FP}\]</span></p></li>
<li><p>Recall$ (<span class="math inline">\(sensitivity\)</span> probability of detection, <span class="math inline">\(PD\)</span>) is the fraction of relevant instances that have been retrieved over total relevant instances, <span class="math inline">\(\frac{TP}{TP+FN}\)</span></p></li>
<li><p><em>f-measure</em> is the harmonic mean of precision and recall,
<span class="math inline">\(2 \cdot \frac{precision \cdot recall}{precision + recall}\)</span></p></li>
<li><p>G-mean: <span class="math inline">\(\sqrt{PD \times Precision}\)</span></p></li>
<li><p>G-mean2: <span class="math inline">\(\sqrt{PD \times Specificity}\)</span></p></li>
<li><p>J coefficient, <span class="math inline">\(j-coeff = sensitivity + specificity - 1 = PD-PF\)</span></p></li>
<li><p>A suitable and interesting performance metric for binary classification when data are imbalanced is the Matthew’s Correlation Coefficient (<span class="math inline">\(MCC\)</span>)~:</p></li>
</ul>
<p><span class="math display">\[MCC=\frac{TP\times TN - FP\times FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\]</span></p>
<p><span class="math inline">\(MCC\)</span> can also be calculated from the confusion matrix. Its range goes from -1 to +1; the closer to one the better as it indicates perfect prediction whereas a value of 0 means that classification is not better than random prediction and negative values mean that predictions are worst than random.</p>
<div id="prediction-in-probabilistic-classifiers" class="section level3" number="2.8.1">
<h3><span class="header-section-number">2.8.1</span> Prediction in probabilistic classifiers</h3>
<p>A probabilistic classifier estimates the probability of each of the posible class values given the attribute values of the instance <span class="math inline">\(P(c|{x})\)</span>. Then, given a new instance, <span class="math inline">\({x}\)</span>, the class value with the highest a posteriori probability will be assigned to that new instance (the <em>winner takes all</em> approach):</p>
<p><span class="math inline">\(\psi({x}) = argmax_c (P(c|{x}))\)</span></p>
</div>
</div>
<div id="other-metrics-used-in-software-engineering-with-classification" class="section level2" number="2.9">
<h2><span class="header-section-number">2.9</span> Other Metrics used in Software Engineering with Classification</h2>
<p>In the domain of defect prediction and when two classes are considered, it is also customary to refer to the <em>probability of detection</em>, (<span class="math inline">\(pd\)</span>) which corresponds to the True Positive rate (<span class="math inline">\(TP_{rate}\)</span> or ) as a measure of the goodness of the model, and <em>probability of false alarm</em> (<span class="math inline">\(pf\)</span>) as performance measures~.</p>
<p>The objective is to find which techniques that maximise <span class="math inline">\(pd\)</span> and minimise <span class="math inline">\(pf\)</span>. As stated by Menzies et al., the balance between these two measures depends on the project characteristics (e.g. real-time systems vs. information management systems) it is formulated as the Euclidean distance from the sweet spot <span class="math inline">\(pf=0\)</span> and <span class="math inline">\(pd=1\)</span> to a pair of <span class="math inline">\((pf,pd)\)</span>.</p>
<p><span class="math display">\[balance=1-\frac{\sqrt{(0-pf^2)+(1-pd^2)}}{\sqrt{2}}\]</span></p>
<p>It is normalized by the maximum possible distance across the ROC square (<span class="math inline">\(\sqrt{2}, 2\)</span>), subtracted this value from 1, and expressed it as a percentage.</p>

</div>
<div id="graphical-evaluation" class="section level2" number="2.10">
<h2><span class="header-section-number">2.10</span> Graphical Evaluation</h2>
<div id="receiver-operating-characteristic-roc" class="section level3" number="2.10.1">
<h3><span class="header-section-number">2.10.1</span> Receiver Operating Characteristic (ROC)</h3>
<p>The <em>Receiver Operating Characteristic</em> (<span class="math inline">\(ROC\)</span>)<span class="citation">(<a href="#ref-Fawcett2006" role="doc-biblioref">Fawcett 2006</a>)</span> curve which provides a graphical visualisation of the results.</p>
<div class="figure">
<img src="figures/roc.png" alt="" />
<p class="caption">Receiver Operating Characteristic</p>
</div>
<p>The Area Under the ROC Curve (AUC) also provides a quality measure between positive and negative rates with a single value.</p>
<p>A simple way to approximate the AUC is with the following equation:
<span class="math inline">\(AUC=\frac{1+TP_{r}-FP_{r}}{2}\)</span></p>
</div>
<div id="precision-recall-curve-prc" class="section level3" number="2.10.2">
<h3><span class="header-section-number">2.10.2</span> Precision-Recall Curve (PRC)</h3>
<p>Similarly to ROC, another widely used evaluation technique is the Precision-Recall Curve (PRC), which depicts a trade off between precision and recall and can also be summarised into a single value as the Area Under the Precision-Recall Curve (AUPRC)~.</p>
<p>%AUPCR is more accurate than the ROC for testing performances when dealing with imbalanced datasets as well as optimising ROC values does not necessarily optimises AUPR values, i.e., a good classifier in AUC space may not be so good in PRC space.
%The weighted average uses weights proportional to class frequencies in the data.
%The weighted average is computed by weighting the measure of class (TP rate, precision, recall …) by the proportion of instances there are in that class. Computing the average can be sometimes be misleading. For instance, if class 1 has 100 instances and you achieve a recall of 30%, and class 2 has 1 instance and you achieve recall of 100% (you predicted the only instance correctly), then when taking the average (65%) you will inflate the recall score because of the one instance you predicted correctly. Taking the weighted average will give you 30.7%, which is much more realistic measure of the performance of the classifier.</p>

</div>
</div>
<div id="numeric-prediction-evaluation" class="section level2" number="2.11">
<h2><span class="header-section-number">2.11</span> Numeric Prediction Evaluation</h2>
<p>In the case of defect prediction, it matters the difference between the predicted value and the actual value. Common performance metrics used for numeric prediction are as follows, where <span class="math inline">\(\hat{y_n}\)</span> represents the predicted value and <span class="math inline">\(y_n\)</span> the actual one.</p>
<p>Mean Square Error (<span class="math inline">\(MSE\)</span>)</p>
<p><span class="math inline">\(MSE = \frac{(\hat{y_1} - y_1)^2 + \ldots +(\hat{y_n} - y_n)^2}{n} = \frac{1}{n}\sum_{i=1}^n(\hat{y_i} - y_i)^2\)</span></p>
<p>Root mean-squared error (<span class="math inline">\(RMSE\)</span>)</p>
<p><span class="math inline">\({RMSE} = \sqrt{\frac{\sum_{t=1}^n (\hat y_t - y)^2}{n}}\)</span></p>
<p>Mean Absolute Error (<span class="math inline">\(MAE\)</span>)</p>
<p><span class="math inline">\(MAE = \frac{|\hat{y_1} - y_1| + \ldots +|\hat{y_n} - y_n|}{n} = \sqrt{\frac{\sum_{t=1}^n |\hat y_t - y|}{n}}\)</span></p>
<p>Relative Absolute Error (<span class="math inline">\(RAE\)</span>)</p>
<p><span class="math inline">\(RAE = \frac{ \sum^N_{i=1} | \hat{\theta}_i - \theta_i | } { \sum^N_{i=1} | \overline{\theta} - \theta_i | }\)</span></p>
<p>Root Relative-Squared Error (<span class="math inline">\(RRSE\)</span>)</p>
<p><span class="math inline">\(RRSE = \sqrt{ \frac{ \sum^N_{i=1} | \hat{\theta}_i - \theta_i | } { \sum^N_{i=1} | \overline{\theta} - \theta_i | } }\)</span></p>
<p>where <span class="math inline">\(\hat{\theta}\)</span> is a mean value of <span class="math inline">\(\theta\)</span>.</p>
<p>Relative-Squared r (<span class="math inline">\(RSE\)</span>)</p>
<p><span class="math inline">\(\frac{(p_1-a_1)^2 + \ldots +(p_n-a_n)^2}{(a_1-\hat{a})^2 + \ldots + (a_n-\hat{a})^2}\)</span></p>
<p>where (<span class="math inline">\(\hat{a}\)</span> is the mean value over the training data)</p>
<p>Relative Absolute Error (<span class="math inline">\(RAE\)</span>)</p>
<p>Correlation Coefficient</p>
<p><em>Correlation coefficient</em> between two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is defined as <span class="math inline">\(\rho(X,Y) = \frac{{\bf Cov}(X,Y)}{\sqrt{{\bf Var}(X){\bf Var}(Y)}}\)</span>. The sample correlation coefficient} <span class="math inline">\(r\)</span> between two samples <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_j\)</span> is vvdefined as <span class="math inline">\(r = S_{xy}/\sqrt{S_{xx}S_{yy}}\)</span></p>
<p>Example: Is there any linear relationship between the effort estimates (<span class="math inline">\(p_i\)</span>) and actual effort (<span class="math inline">\(a_i\)</span>)?</p>
<p><span class="math inline">\(a\|39,43,21,64,57,47,28,75,34,52\)</span></p>
<p><span class="math inline">\(p\|65,78,52,82,92,89,73,98,56,75\)</span></p>
<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb396-1"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#cb396-1" aria-hidden="true" tabindex="-1"></a>p<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">39</span>,<span class="dv">43</span>,<span class="dv">21</span>,<span class="dv">64</span>,<span class="dv">57</span>,<span class="dv">47</span>,<span class="dv">28</span>,<span class="dv">75</span>,<span class="dv">34</span>,<span class="dv">52</span>)</span>
<span id="cb396-2"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#cb396-2" aria-hidden="true" tabindex="-1"></a>a<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">65</span>,<span class="dv">78</span>,<span class="dv">52</span>,<span class="dv">82</span>,<span class="dv">92</span>,<span class="dv">89</span>,<span class="dv">73</span>,<span class="dv">98</span>,<span class="dv">56</span>,<span class="dv">75</span>)</span>
<span id="cb396-3"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#cb396-3" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb396-4"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#cb396-4" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(p,a)</span></code></pre></div>
<pre><code>## [1] 0.84</code></pre>
<p><span class="math inline">\(R^2\)</span></p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Abran_TSE96_FP" class="csl-entry">
Abran, A., and P. N. Robillard. 1996. <span>“Function Points Analysis: An Empirical Study of Its Measurement Processes.”</span> <em>Software Engineering, IEEE Transactions on</em> 22 (12): 895–910. <a href="https://doi.org/10.1109/32.553638">https://doi.org/10.1109/32.553638</a>.
</div>
<div id="ref-AlbrechtG83" class="csl-entry">
Albrecht, A. J., and Jr. Gaffney J. E. 1983. <span>“Software Function, Source Lines of Code, and Development Effort Prediction: A Software Science Validation.”</span> <em>IEEE Transactions on Software Engineering</em> 9 (6): 639–48. <a href="https://doi.org/10.1109/TSE.1983.235271">https://doi.org/10.1109/TSE.1983.235271</a>.
</div>
<div id="ref-Bailey81" class="csl-entry">
Bailey, John W., and Victor R. Basili. 1981. <span>“A Meta-Model for Software Development Resource Expenditures.”</span> In <em>Proceedings of the 5th International Conference on Software Engineering (ICSE’81)</em>, 107–16. ICSE’81. Piscataway, NJ, USA: IEEE Press. <a href="http://dl.acm.org/citation.cfm?id=800078.802522">http://dl.acm.org/citation.cfm?id=800078.802522</a>.
</div>
<div id="ref-Belady79" class="csl-entry">
Belady, L. A., and M. M. Lehman. 1979. <span>“Research Directions in Software Technology.”</span> In. Cambridge, MA: MIT Press.
</div>
<div id="ref-BL97" class="csl-entry">
Blum, A. L., and P. Langley. 1997. <span>“Selection of Relevant Features and Examples in Machine Learning.”</span> Edited by R. Greiner and D. Subramanian. <em>Artificial Intelligence</em> 97 (1-2): 245–71.
</div>
<div id="ref-Boehm81" class="csl-entry">
Boehm, Barry W. 1981. <em>Software Engineering Economics</em>. 1st ed. Upper Saddle River, NJ, USA: Prentice Hall PTR.
</div>
<div id="ref-CanoHL07" class="csl-entry">
Cano, José Ramón, Francisco Herrera, and Manuel Lozano. 2007. <span>“Evolutionary Stratified Training Set Selection for Extracting Classification Rules with Trade Off Precision-Interpretability.”</span> <em>Data &amp; Knowledge Engineering</em> 60 (1): 90–108. <a href="https://doi.org/10.1016/j.datak.2006.01.008">https://doi.org/10.1016/j.datak.2006.01.008</a>.
</div>
<div id="ref-ALR11" class="csl-entry">
———. 2011. <span>“Evaluating Defect Prediction Approaches: A Benchmark and an Extensive Comparison.”</span> <em>Empirical Software Engineering</em>, 1–47. <a href="https://doi.org/10.1007/s10664-011-9173-9">https://doi.org/10.1007/s10664-011-9173-9</a>.
</div>
<div id="ref-DLM00" class="csl-entry">
Dash, M., H. Liu, and H. Motoda. 2000. <span>“Consistency Based Feature Selection.”</span> In <em>Pacific-Asia Conf. On Knowledge Discovery and Data Mining</em>, 98–109.
</div>
<div id="ref-Dejaeger_TSE12_EffEst" class="csl-entry">
Dejaeger, K., W. Verbeke, D. Martens, and B. Baesens. 2012. <span>“Data Mining Techniques for Software Effort Estimation: A Comparative Study.”</span> <em>Software Engineering, IEEE Transactions on</em> 38 (2): 375–97. <a href="https://doi.org/10.1109/TSE.2011.55">https://doi.org/10.1109/TSE.2011.55</a>.
</div>
<div id="ref-Desharnais88" class="csl-entry">
Desharnais, J. M. 1988. <span>“Analyse Statistique de La Productivite Des Projects de Development En Informatique a Partir de La Technique Des Points de Fonction.”</span> MSc Thesis, Univ. du Quebec a Montreal.
</div>
<div id="ref-Dolado97" class="csl-entry">
Dolado, J. J. 1997. <span>“A Study of the Relationships Among Albrecht and Mark II Function Points, Lines of Code 4gl and Effort.”</span> <em>Journal of Systems and Software</em> 37 (2): 161–73. <a href="https://doi.org/10.1016/S0164-1212(96)00111-2">https://doi.org/10.1016/S0164-1212(96)00111-2</a>.
</div>
<div id="ref-Fawcett2006" class="csl-entry">
Fawcett, Tom. 2006. <span>“An Introduction to ROC Analysis.”</span> <em>Pattern Recognition Letters</em> 27 (8): 861–74. https://doi.org/<a href="http://dx.doi.org/10.1016/j.patrec.2005.10.010">http://dx.doi.org/10.1016/j.patrec.2005.10.010</a>.
</div>
<div id="ref-FayyadPS1996" class="csl-entry">
Fayyad, Usama, Gregory Piatetsky-Shapiro, and Padhraic Smyth. 1996. <span>“The KDD Process for Extracting Useful Knowledge from Volumes of Data.”</span> <em>Commun. ACM</em> 39 (11): 27–34. <a href="https://doi.org/10.1145/240455.240464">https://doi.org/10.1145/240455.240464</a>.
</div>
<div id="ref-FernandezCBA14" class="csl-entry">
Fernández-Delgado, Manuel, Eva Cernadas, Senén Barro, and Dinani Amorim. 2014. <span>“Do We Need Hundreds of Classifiers to Solve Real World Classification Problems?”</span> <em>Journal of Machine Learning Research</em> 15: 3133–81. <a href="http://jmlr.org/papers/v15/delgado14a.html">http://jmlr.org/papers/v15/delgado14a.html</a>.
</div>
<div id="ref-GDCH12" class="csl-entry">
Garcia, S., J. Derrac, J. Cano, and F. Herrera. 2012. <span>“Prototype Selection for Nearest Neighbor Classification: Taxonomy and Empirical Study.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 34 (3): 417–35. <a href="https://doi.org/10.1109/TPAMI.2011.142">https://doi.org/10.1109/TPAMI.2011.142</a>.
</div>
<div id="ref-Gray2011" class="csl-entry">
Gray, D., D. Bowes, N. Davey, Y. Sun, and B. Christianson. 2011. <span>“The Misuse of the NASA Metrics Data Program Data Sets for Automated Software Defect Prediction.”</span> In <em>15th Annual Conference on Evaluation Assessment in Software Engineering (EASE 2011)</em>, 96–103. <a href="https://doi.org/10.1049/ic.2011.0012">https://doi.org/10.1049/ic.2011.0012</a>.
</div>
<div id="ref-Hal99" class="csl-entry">
Hall, M. A. 1999. <span>“Correlation-Based Feature Selection for Machine Learning.”</span> PhD thesis, Hamilton, New Zealand: University of Waikato, Department of Computer Science.
</div>
<div id="ref-Hastings01" class="csl-entry">
Hastings, T. E., and A. S. M. Sajeev. 2001. <span>“A Vector-Based Approach to Software Size Measurement and Effort Estimation.”</span> <em>IEEE Transactions on Software Engineering</em> 27 (4): 337–50. <a href="https://doi.org/10.1109/32.917523">https://doi.org/10.1109/32.917523</a>.
</div>
<div id="ref-Heiat97" class="csl-entry">
Heiat, Abbas, and Nafisseh Heiat. 1997. <span>“A Model for Estimating Efforts Required for Developing Small-Scale Business Applications.”</span> <em>Journal of Systems and Software</em> 39 (1): 7–14. <a href="https://doi.org/10.1016/S0164-1212(96)00159-8">https://doi.org/10.1016/S0164-1212(96)00159-8</a>.
</div>
<div id="ref-herraiz2009flossmetrics" class="csl-entry">
Herraiz, Israel, Daniel Izquierdo-Cortazar, Francisco Rivas-Hernandez, Jesus M. Gonzalez-Barahona, Gregorio Robles, Santiago Dueñas Dominguez, Carlos Garcia-Campos, Juan Francisco Gato, and Liliana Tovar. 2009. <span>“<span>FLOSSMetrics</span>: Free / Libre / Open Source Software Metrics.”</span> In <em>Proceedings of the 13th European Conference on Software Maintenance and Reengineering (CSMR)</em>. Kaiserlauten, Germany: IEEE Computer Society.
</div>
<div id="ref-Hoekstra2014" class="csl-entry">
Hoekstra, Rink, Richard D. Morey, Jeffrey N. Rouder, and Eric-Jan Wagenmakers. 2014. <span>“Robust Misinterpretation of Confidence Intervals.”</span> <em>Psychonomic Bulletin <span>&amp;</span> Review</em> 21 (5): 1157–64. <a href="https://doi.org/10.3758/s13423-013-0572-3">https://doi.org/10.3758/s13423-013-0572-3</a>.
</div>
<div id="ref-HCC06" class="csl-entry">
Howison, James, Megan Conklin, and Kevin Crowston. 2006. <span>“<span>FLOSSmole</span>: A Collaborative Repository for <span>FLOSS</span> Research Data and Analyses.”</span> <em>International Journal of Information Technology and Web Engineering</em> 1 (3). <a href="https://doi.org/10.4018/jitwe.2006070102">https://doi.org/10.4018/jitwe.2006070102</a>.
</div>
<div id="ref-Jeffery_ESE96" class="csl-entry">
Jeffery, Ross, and John Stathis. 1996. <span>“Function Point Sizing: Structure, Validity and Applicability.”</span> <em>Empirical Software Engineering</em> 1: 11–30. <a href="http://dx.doi.org/10.1007/BF00125809">http://dx.doi.org/10.1007/BF00125809</a>.
</div>
<div id="ref-Jorgensen04" class="csl-entry">
Jorgensen, M. 2004. <span>“Realism in Assessment of Effort Estimation Uncertainty: It Matters How You Ask.”</span> <em>IEEE Transactions on Software Engineering</em> 30 (4): 209–17. <a href="https://doi.org/10.1109/TSE.2004.1274041">https://doi.org/10.1109/TSE.2004.1274041</a>.
</div>
<div id="ref-Jorgensen2003" class="csl-entry">
Jørgensen, Magne, Ulf Indahl, and Dag Sjøberg. 2003. <span>“Software Effort Estimation by Analogy and ’Regression Toward Themean’.”</span> <em>Journal of Systems and Software</em> 68 (3): 253–62. <a href="https://doi.org/10.1016/S0164-1212(03)00066-9">https://doi.org/10.1016/S0164-1212(03)00066-9</a>.
</div>
<div id="ref-Jorgensen07" class="csl-entry">
Jørgensen, M., and M. Shepperd. 2007. <span>“A Systematic Review of Software Development Cost Estimation Studies.”</span> <em>IEEE Transactions on Software Engineering</em> 33 (1): 33–53. <a href="https://doi.org/10.1109/TSE.2007.256943">https://doi.org/10.1109/TSE.2007.256943</a>.
</div>
<div id="ref-Kemerer87" class="csl-entry">
Kemerer, Chris F. 1987. <span>“An Empirical Validation of Software Cost Estimation Models.”</span> <em>Communications of the ACM</em> 30 (5): 416–29. <a href="https://doi.org/10.1145/22899.22906">https://doi.org/10.1145/22899.22906</a>.
</div>
<div id="ref-Kitchenham2002" class="csl-entry">
Kitchenham, Barbara A. 2002. <span>“The Question of Scale Economies in Software — Why Cannot Researchers Agree?”</span> <em>Information and Software Technology</em> 44 (1): 13–24. <a href="https://doi.org/10.1016/S0950-5849(01)00204-X">https://doi.org/10.1016/S0950-5849(01)00204-X</a>.
</div>
<div id="ref-Kitchenham85" class="csl-entry">
Kitchenham, Barbara A., and N. R. Taylor. 1985. <span>“Software Project Development Cost Estimation.”</span> <em>Journal of Systems and Software</em> 5 (4): 267–78. <a href="https://doi.org/10.1016/0164-1212(85)90026-3">https://doi.org/10.1016/0164-1212(85)90026-3</a>.
</div>
<div id="ref-Kitchenham02_CSC" class="csl-entry">
Kitchenham, Barbara, Shari Lawrence Pfleeger, Beth McColl, and Suzanne Eagan. 2002. <span>“An Empirical Study of Maintenance and Development Estimation Accuracy.”</span> <em>Journal of Systems and Software</em> 64 (1): 57–77. <a href="https://doi.org/10.1016/S0164-1212(02)00021-3">https://doi.org/10.1016/S0164-1212(02)00021-3</a>.
</div>
<div id="ref-KJ97" class="csl-entry">
Kohavi, R., and G. H. John. 1997. <span>“Wrappers for Feature Subset Selection.”</span> <em>Artificial Intelligence</em> 1-2: 273–324.
</div>
<div id="ref-Lan94" class="csl-entry">
Langley, P. 1994. <span>“Selection of Relevant Features in Machine Learning.”</span> In <em>Procs. Of the AAAI Fall Symposium on Relevance</em>, 140–44.
</div>
<div id="ref-LiRAR07" class="csl-entry">
Li, Jingzhou, Guenther Ruhe, Ahmed Al-Emran, and Michael M. Richter. 2007. <span>“A Flexible Method for Software Effort Estimation by Analogy.”</span> <em>Empirical Software Engineering</em> 12 (1): 65–106. <a href="https://doi.org/10.1007/s10664-006-7552-4">https://doi.org/10.1007/s10664-006-7552-4</a>.
</div>
<div id="ref-Lincke2008" class="csl-entry">
Lincke, Rüdiger, Jonas Lundberg, and Welf Löwe. 2008. <span>“Comparing Software Metrics Tools.”</span> In <em>Proceedings of the 2008 International Symposium on Software Testing and Analysis (ISSTA’08)</em>, 131–42. ISSTA’08. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/1390630.1390648">https://doi.org/10.1145/1390630.1390648</a>.
</div>
<div id="ref-LBNRB09" class="csl-entry">
Linstead, Erik, Sushil Bajracharya, Trung Ngo, Paul Rigor, Cristina Lopes, and Pierre Baldi. 2009. <span>“Sourcerer: Mining and Searching Internet-Scale Software Repositories.”</span> <em>Data Mining and Knowledge Discovery</em> 18: 300–336. <a href="https://doi.org/10.1007/s10618-008-0118-x">https://doi.org/10.1007/s10618-008-0118-x</a>.
</div>
<div id="ref-LM98" class="csl-entry">
Liu, H., and H. Motoda. 1998. <em>Feature Selection for Knowlegde Discovery and Data Mining</em>. London, UK: Kluwer Academic Publishers.
</div>
<div id="ref-LY05" class="csl-entry">
Liu, H., and L. Yu. 2005. <span>“Toward Integrating Feature Selection Algorithms for Classification and Clustering.”</span> <em>IEEE Trans. On Knowledge and Data Eng.</em> 17 (3): 1–12.
</div>
<div id="ref-MairSJ05" class="csl-entry">
Mair, Carolyn, Martin Shepperd, and Magne Jørgensen. 2005. <span>“An Analysis of Data Sets Used to Train and Validate Cost Prediction Systems.”</span> <em>SIGSOFT Software Engineering Notes</em> 30 (4): 1–6. <a href="https://doi.org/10.1145/1082983.1083166">https://doi.org/10.1145/1082983.1083166</a>.
</div>
<div id="ref-Maxwell02" class="csl-entry">
Maxwell, Katrina. 2002. <em>Applied Statistics for Software Managers</em>. Prentice Hall.
</div>
<div id="ref-Misic19981" class="csl-entry">
Mišić, Vojislav B, and Dejan N Tevsić. 1998. <span>“Estimation of Effort and Complexity: An Object-Oriented Case Study.”</span> <em>Journal of Systems and Software</em> 41 (2): 133–43. <a href="https://doi.org/10.1016/S0164-1212(97)10014-0">https://doi.org/10.1016/S0164-1212(97)10014-0</a>.
</div>
<div id="ref-Miyazaki94" class="csl-entry">
Miyazaki, Y., M. Terakado, K. Ozaki, and H. Nozaki. 1994. <span>“Robust Regression for Developing Software Estimation Models.”</span> <em>Journal of Systems and Software</em> 27 (1): 3–16. <a href="https://doi.org/10.1016/0164-1212(94)90110-4">https://doi.org/10.1016/0164-1212(94)90110-4</a>.
</div>
<div id="ref-Moser1999" class="csl-entry">
Moser, Simon, Brian Henderson-Sellers, and Vojislav B Mišić. 1999. <span>“Cost Estimation Based on Business Models.”</span> <em>Journal of Systems and Software</em> 49 (1): 33–42. <a href="https://doi.org/10.1016/S0164-1212(99)00064-3">https://doi.org/10.1016/S0164-1212(99)00064-3</a>.
</div>
<div id="ref-NZZH12" class="csl-entry">
Nagappan, Nachiappan, Andreas Zeller, Thomas Zimmermann, Kim Herzig, and Brendan Murphy. 2012. <span>“Change Bursts as Defect Predictors.”</span> In <em>21st IEEE International Symposium on Software Reliability Engineering (ISSRE 2012)</em>. San Jose, California, USA.
</div>
<div id="ref-NZ10" class="csl-entry">
Nussbaum, L., and S. Zacchiroli. 2010. <span>“The Ultimate Debian Database: Consolidating Bazaar Metadata for Quality Assurance and Data Mining.”</span> In <em>7th IEEE Working Conference on Mining Software Repositories (MSR 2010)</em>, 52–61. <a href="https://doi.org/10.1109/MSR.2010.5463277">https://doi.org/10.1109/MSR.2010.5463277</a>.
</div>
<div id="ref-Schofield98PhD" class="csl-entry">
Schofield, C. 1998. <span>“An Empirical Investigation into Software Effort Estimation by Analogy.”</span> PhD thesis, Bournemouth University.
</div>
<div id="ref-shearer00crisp" class="csl-entry">
Shearer, Colin. 2000. <span>“The CRISP-DM Model: The New Blueprint for Data Mining.”</span> <em>Journal of Data Warehousing</em> 5 (4).
</div>
<div id="ref-Shepperd_TSE01" class="csl-entry">
Shepperd, M., and M. Cartwright. 2001. <span>“Predicting with Sparse Data.”</span> <em>Software Engineering, IEEE Transactions on</em> 27 (11): 987–98. <a href="https://doi.org/10.1109/32.965339">https://doi.org/10.1109/32.965339</a>.
</div>
<div id="ref-Shepperd97_Analogy" class="csl-entry">
Shepperd, M., and C. Schofield. 1997. <span>“Estimating Software Project Effort Using Analogies.”</span> <em>IEEE Transactions on Software Engineering</em> 23 (11): 736–43. <a href="https://doi.org/10.1109/32.637387">https://doi.org/10.1109/32.637387</a>.
</div>
<div id="ref-Shepperd2013" class="csl-entry">
Shepperd, M., Qinbao Song, Zhongbin Sun, and C. Mair. 2013. <span>“Data Quality: Some Comments on the NASA Software Defect Datasets.”</span> <em>IEEE Transactions on Software Engineering</em> 39 (9): 1208–15. <a href="https://doi.org/10.1109/TSE.2013.11">https://doi.org/10.1109/TSE.2013.11</a>.
</div>
<div id="ref-QualitasCorpus2010" class="csl-entry">
Tempero, Ewan, Craig Anslow, Jens Dietrich, Ted Han, Jing Li, Markus Lumpe, Hayden Melton, and James Noble. 2010. <span>“Qualitas Corpus: A Curated Collection of Java Code for Empirical Studies.”</span> In <em>2010 Asia Pacific Software Engineering Conference (Apsec2010)</em>.
</div>
<div id="ref-ugarte2015probability" class="csl-entry">
Ugarte, M. D., A. F. Militino, and A. T. Arnholt. 2015. <em>Probability and Statistics with r, Second Edition</em>. Taylor &amp; Francis. <a href="https://books.google.es/books?id=4aB0pwAACAAJ">https://books.google.es/books?id=4aB0pwAACAAJ</a>.
</div>
<div id="ref-VanAntwerpM2008" class="csl-entry">
Van Antwerp, M., and G. Madey. 2008. <span>“Advances in the SourceForge Research Data Archive (SRDA).”</span> Milan, Italy.
</div>
<div id="ref-Vasa2010" class="csl-entry">
Vasa, Rajesh. 2010. <span>“Growth and Change Dynamics in Open Source Software Systems.”</span> PhD thesis, Faculty of Information; Communication Technologies Swinburne University of Technology Melbourne, Australia. <a href="http://hdl.handle.net/1959.3/95058">http://hdl.handle.net/1959.3/95058</a>.
</div>
<div id="ref-Woodfield81" class="csl-entry">
Woodfield, S. N., V. Y. Shen, and H. E. Dunsmore. 1981. <span>“A Study of Several Metrics for Programming Effort.”</span> <em>Journal of Systems and Software</em> 2 (2): 97–103. <a href="https://doi.org/10.1016/0164-1212(81)90029-7">https://doi.org/10.1016/0164-1212(81)90029-7</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="r-intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="evaluationSE.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/danrodgar/dasedown/edit/master/100_intro.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DASE.pdf", "DASE.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
